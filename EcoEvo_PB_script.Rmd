---
title: "Untitled"
author: "Yefeng Yang"
date: "2022/4/4"
output: html_document
---

# Load packages and custom functions

```{r setup, echo = FALSE}
# Tidy
 # rm(list=ls())
 # graphics.off()

# Preparing workspace
knitr::opts_chunk$set(echo = TRUE, include = TRUE)

# Loading packages
pacman::p_load(knitr, # knit markdown
               readxl, 
               readr, 
               metafor, 
               dplyr, 
               tidyverse, 
               janitor, # generate 1-, 2-way table
               patchwork, # layout of plots
               cowplot, 
               ggpubr,
               gridExtra,
               orchaRd, # forest-like plot
               gridGraphics, # Redraw Base Graphics Using 'grid' Graphics. `gridGraphics` is required to handle base-R plots.
               dabestr,
               here,
               retrodesign,
               lme4,
               car, # logit transformation, car::logit()
               boot, # Bootstrap Resampling
               lmerTest, # get p-valus from lme4 model, but need to re-fit the model lmerTest::lme4
               ggthemes,
               modi, # weighted variance
               qqman
               )


# Function to calculate power (two-tail) for meta-analysis
power.ma_Shinichi <- function(mu, SE, alpha = 0.05) {
  2-pnorm(qnorm(1-alpha/2)-abs(mu)/SE)-pnorm(qnorm(1-alpha/2)+abs(mu)/SE)
  } # or power.ma_Shinichi1 <- function(mu,SE){1 - pnorm(qnorm(1-0.05/2)-abs(mu)/SE) + pnorm(-qnorm(1-0.05/2)-abs(mu)/SE)}


# Function for power analysis for empirical data point
power.individual_Shinichi <- function(mu, se, alpha = 0.05) {
  2-pnorm(qnorm(1-alpha/2)-abs(mu)/se)-pnorm(qnorm(1-alpha/2)+abs(mu)/se)} # two-tailed power


# Function for Type S error for empirical data point
error_S <- function(mu, se, alpha = 0.05){
  #z <- qnorm(1 - alpha/2) # Z-score or quantile
  p.u <- 1 - pnorm(qnorm(1 - alpha/2) - abs(mu)/se) # upper-tail probability
  p.l <- pnorm(-qnorm(1 - alpha/2) - abs(mu)/se) # lower-tail probability
  power <- p.u + p.l # upper + lower
  errorS <- p.l/power # percentage of the opposite direction
  return(errorS)
} 

# Function for Type M error for empirical data point
error_M <- function(mu, se, alpha = 0.05, N = 10000) {
    est.random <- rnorm(n=N, mean = mu, sd = se)
    # est.random <- mu + se*rnorm(n=N, mean=0, sd=1)
    sig.index <- abs(est.random) > se*qnorm(1 - alpha/2)
    overestimate <- mean(abs(est.random)[sig.index])/abs(mu) # ratio is regardnesss of sign, so we need absolute value
    absolute_error <- overestimate*abs(mu) - abs(mu)
    relative_error <- absolute_error/(overestimate*abs(mu))
  return(abs(overestimate) %>% round(3))
}


error_M2 <- function(mu, se, alpha = 0.05, N = 10000) {
    est.random <- rnorm(n=N, mean = mu, sd = se)
    # est.random <- mu + se*rnorm(n=N, mean=0, sd=1)
    sig.index <- abs(est.random) > se*qnorm(1 - alpha/2)
    overestimate <- mean(abs(est.random)[sig.index])/abs(mu) # ratio is regardnesss of sign, so we need absolute value
    absolute_error <- overestimate*abs(mu) - abs(mu)
    relative_error <- absolute_error/(overestimate*abs(mu))
  return(abs(relative_error) %>% round(3))
} # relative error: (M - 1) / M



# meta-analysis of magnitude
## folded effect size
folded_es <-function(mean, variance){ # the sampling variance of magnitude   
  mu <- mean
  sigma <- sqrt(variance)
  fold_mu <- sigma*sqrt(2/pi)*exp((-mu^2)/(2*sigma^2)) + mu*(1 - 2*pnorm(-mu/sigma))
  fold_mu
}
## folded error
folded_error <- function(mean, variance){ # the sampling variance of magnitude   
  mu <- mean
  sigma <- sqrt(variance)
  fold_mu <- sigma*sqrt(2/pi)*exp((-mu^2)/(2*sigma^2)) + mu*(1 - 2*pnorm(-mu/sigma))
  fold_se <- sqrt(mu^2 + sigma^2 - fold_mu^2)
  # adding se to make bigger mean
  fold_v <- fold_se^2
  fold_v
}


# custom function for extracting mean and CI from each metafor model
estimates.CI <- function(model){
  db.mf <- data.frame(model$b,row.names = 1:nrow(model$b))
  db.mf <- cbind(db.mf,model$ci.lb,model$ci.ub,row.names(model$b))
  names(db.mf) <- c("mean","lower","upper","estimate")
  return(db.mf[,c("estimate","mean","lower","upper")])
}
```


# 1. Import data and pre-process

Import lnRR, SMD, and Zr datasets

```{r}
#*************************************************************************#
#           import datasets with calculated effect sizes                    
#*************************************************************************#
## read lnRR, SMD and Zr datasets, which contain the calculated effect size (es) and sampling variance (var)
### lnRR
lnRR_csv <- list.files(path = "./dataset/lnRR", pattern = "*.csv", full.names = TRUE) %>% lapply(read_csv) ## need to use full name of each dataset, otherwise read_csv is not able to read it
### SMD
SMD_csv <- list.files(path = "./dataset/SMD", pattern = "*.csv", full.names = TRUE) %>% lapply(read_csv)
### Zr
Zr_csv <- list.files(path = "./dataset/Zr", pattern = "*.csv", full.names = TRUE) %>% lapply(read_csv)

### get names of each .csv file
lnRR_filenames <- list.files(path = "./dataset/lnRR", pattern = "*.csv", full.names = FALSE)
SMD_filenames <- list.files(path = "./dataset/SMD", pattern = "*.csv", full.names = FALSE)
Zr_filenames <- list.files(path = "./dataset/Zr", pattern = "*.csv", full.names = FALSE)
### rename the elements of the list
names(lnRR_csv) <- lnRR_filenames
names(SMD_csv) <- SMD_filenames
names(Zr_csv) <- Zr_filenames


#*************************************************************************#
#                      import datasets with raw data                    
#*************************************************************************#

## read another sets of lnRR, SMD and Zr datasets, which contain descriptive statistics (mean, sd and sample size)
### lnRR
lnRR_des_csv <- list.files(path = "./dataset/lnRR/des_stat", pattern = "*.csv", full.names = TRUE) %>% lapply(read_csv) 
lnRR_des_filenames <- list.files(path = "./dataset/lnRR/des_stat", pattern = "*.csv", full.names = FALSE) # extract file names, which will be used later


### SMD
SMD_des_csv <- list.files(path = "./dataset/SMD/des_stat", pattern = "*.csv", full.names = TRUE) %>% lapply(read_csv)
SMD_des_filenames <- list.files(path = "./dataset/SMD/des_stat", pattern = "*.csv", full.names = FALSE) # extract file names, which will be used later

## for datasets with descriptive statistics, we also need to create 'effective sample size'-related variables for each of them 
## these variables make the examination of the small-study effect more statistically sound (see below or the main text for explanations)

### function to calculate effective sample size
ess_cal <- function(dat){(4*dat$C_n*dat$T_n) / (dat$C_n + dat$T_n)}
### calculate effective sample size for lnRR with descriptive statistics
ess <- NA
for (i in 1:length(lnRR_des_csv)) {
  ess[i] <- ess_cal(lnRR_des_csv[[i]]) %>% list()}
### allocate each set of effective sample size into corresponding dataset
for (i in 1:length(lnRR_des_csv)) {
  lnRR_des_csv[[i]]$ess <- ess[[i]]
}

### calculate effective sample size for SMD with descriptive statistics
ess <- NA
for (i in 1:length(SMD_des_csv)) {
  ess[i] <- ess_cal(SMD_des_csv[[i]]) %>% list()}
### allocate each set of effective sample size into corresponding dataset
for (i in 1:length(SMD_des_csv)) {
  SMD_des_csv[[i]]$ess <- ess[[i]]
}
 
#### create inverse of effective sample size - "effective sample size" based "sampling variance" 
#### function to calculate inverse of effective sample size 
ess.var_cal <- function(dat){1/dat$C_n + 1/dat$T_n}
#### calculations for lnRR
ess.var <- NA
for (i in 1:length(lnRR_des_csv)) {
  ess.var[i] <- ess.var_cal(lnRR_des_csv[[i]]) %>% list()}
#### allocate each set of effective sample size into corresponding dataset
for (i in 1:length(lnRR_des_csv)) {
  lnRR_des_csv[[i]]$ess.var <- ess.var[[i]]
}

#### calculate inverse sqrt of effective sample size - "effective sample size" based "sampling error" 
for (i in 1:length(lnRR_des_csv)) {
  lnRR_des_csv[[i]]$ess.sei <- sqrt(lnRR_des_csv[[i]]$ess.var)
}


#### calculations for SMD
ess.var <- NA
for (i in 1:length(SMD_des_csv)) {
  ess.var[i] <- ess.var_cal(SMD_des_csv[[i]]) %>% list()}
#### allocate each set of effective sample size into corresponding dataset
for (i in 1:length(SMD_des_csv)) {
  SMD_des_csv[[i]]$ess.var <- ess.var[[i]]
}
#### calculate inverse sqrt of effective sample size - "effective sample size" based "sampling error" 
for (i in 1:length(SMD_des_csv)) {
  SMD_des_csv[[i]]$ess.sei <- sqrt(SMD_des_csv[[i]]$ess.var)
}


## recalculate effect size for datasets with descriptive statistics
### lnRR
lnRR_es <- NA
for (i in 1:length(lnRR_des_csv)) {
  lnRR_es[i] <- escalc(measure = "ROM",
                    m1i = T_mean,
                    m2i = C_mean,
                    sd1i = T_sd,
                    sd2i = C_sd,
                    n1i = T_n,
                    n2i = C_n,
                    data = lnRR_des_csv[[i]]) %>% list()
}
### rename the elements of the list
names(lnRR_es) <- lnRR_des_filenames


### SMD
SMD_es <- NA
for (i in 1:length(SMD_des_csv)) {
  SMD_es[i] <- escalc(measure = "SMD",
                    m1i = T_mean,
                    m2i = C_mean,
                    sd1i = T_sd,
                    sd2i = C_sd,
                    n1i = T_n,
                    n2i = C_n,
                    data = SMD_des_csv[[i]]) %>% list()
}
### rename the elements of the list
names(SMD_es) <- SMD_des_filenames

## to keep consistant, rename effect size and sampling variance
### lnRR
for (i in 1:length(lnRR_es)) {
  names(lnRR_es[[i]])[names(lnRR_es[[i]]) == "yi"] <- "es"
  names(lnRR_es[[i]])[names(lnRR_es[[i]]) == "vi"] <- "var"
}
### SMD
for (i in 1:length(SMD_es)) {
  names(SMD_es[[i]])[names(SMD_es[[i]]) == "yi"] <- "es"
  names(SMD_es[[i]])[names(SMD_es[[i]]) == "vi"] <- "var"
}

## combine two sets of dataset for lnRR and SMD (Zr only has one set dataset, so no need to combine)
lnRR <- append(lnRR_csv, lnRR_es) # or c(lnRR_csv, lnRR_es)
SMD <- append(SMD_csv, SMD_es)
Zr <- Zr_csv # for consistence, create Zr to instead of Zr_csv

## remove NAs, zero variance, and +-Inf
### lnRR
#### delete NAs, zero variance
for (i in 1:length(lnRR)) {
  lnRR[[i]] <- lnRR[[i]][!is.na(lnRR[[i]]$es) & !is.na(lnRR[[i]]$var) & lnRR[[i]]$var != 0 & !is.na(lnRR[[i]]$year_pub), ]
}
#### delete +-Inf
for (i in 1:length(lnRR)) {
  lnRR[[i]] <- lnRR[[i]] %>% na.omit()
}

### SMD
#### delete NAs, zero variance
for (i in 1:length(SMD)) {
  SMD[[i]] <- SMD[[i]][!is.na(SMD[[i]]$es) & !is.na(SMD[[i]]$var) & SMD[[i]]$var != 0 & !is.na(SMD[[i]]$year_pub), ]
}
#### delete +-Inf
for (i in 1:length(SMD)) {
  SMD[[i]] <- SMD[[i]] %>% na.omit()
}


### Zr
#### delete NAs, zero variance
for (i in 1:length(Zr)) {
  Zr[[i]] <- Zr[[i]][!is.na(Zr[[i]]$es) & !is.na(Zr[[i]]$var) & Zr[[i]]$var != 0 & !is.na(Zr[[i]]$year_pub), ]
}
#### delete +-Inf
for (i in 1:length(Zr)) {
  Zr[[i]] <- Zr[[i]] %>% na.omit()
}

# create the variable of latest-year-centring publication year, which was used as a predictor to test time-lag bias (decline effect). The reason why creating this variable is to set the intercept conditional on the latest year rather than zero year (details see the main text)

## lnRR
for (i in 1:length(lnRR)) {
  lnRR[[i]]$year_pub.l <- as.vector(lnRR[[i]]$year_pub - max(lnRR[[i]]$year_pub))
}
## SMD
for (i in 1:length(SMD)) {
  SMD[[i]]$year_pub.l <- as.vector(SMD[[i]]$year_pub - max(SMD[[i]]$year_pub))
}

## Zr
for (i in 1:length(Zr)) {
  Zr[[i]]$year_pub.l <- as.vector(Zr[[i]]$year_pub - max(Zr[[i]]$year_pub))
}


# create the variable of sampling error, which was used as a predictor to test small-study effect

## lnRR
for (i in 1:length(lnRR)) {
  lnRR[[i]]$sei <- sqrt(lnRR[[i]]$var)
}
## SMD
for (i in 1:length(SMD)) {
  SMD[[i]]$sei <- sqrt(SMD[[i]]$var)
}
## Zr
for (i in 1:length(Zr)) {
  Zr[[i]]$sei <- sqrt(Zr[[i]]$var)
}




## transform effect size, sei, and year_pub.l prior to model fitting. This  is to eliminate scale-dependency and to allow for aggregations of model coefficients over different effect size metrics in subsequent second-order meta-analysis

## scale data using respective standard deviation
### lnRR
for (i in 1:length(lnRR)) {
  lnRR[[i]]$es_zscore <- scale(lnRR[[i]]$es, center = F, scale = TRUE) # without centering, which is used to estimate intercept
  lnRR[[i]]$var_zscore <- scale(lnRR[[i]]$var, scale = TRUE) - ( (0-mean(lnRR[[i]]$var))/sd(lnRR[[i]]$var) ) # see Equation 7 for explanations
  lnRR[[i]]$sei_zscore <- scale(lnRR[[i]]$sei, scale = TRUE) - ( (0-mean(lnRR[[i]]$sei))/sd(lnRR[[i]]$sei) ) # see Equation 7 for explanations
  lnRR[[i]]$year_pub.l_zscore <- scale(lnRR[[i]]$year_pub.l, scale = TRUE)
}

## also need to scale effective sample size related variables
for (i in 1:length(lnRR[lnRR_des_filenames])) {
  lnRR[lnRR_des_filenames][[i]]$ess.var_zscore <- scale(lnRR[lnRR_des_filenames][[i]]$ess.var, scale = TRUE) - ( (0-mean(lnRR[lnRR_des_filenames][[i]]$ess.var))/sd(lnRR[lnRR_des_filenames][[i]]$ess.var) ) # see Equation 7 for explanations
  lnRR[lnRR_des_filenames][[i]]$ess.sei_zscore <- scale(lnRR[lnRR_des_filenames][[i]]$ess.sei, scale = TRUE) - ( (0-mean(lnRR[lnRR_des_filenames][[i]]$ess.sei))/sd(lnRR[lnRR_des_filenames][[i]]$ess.sei) )
}

### SMD
for (i in 1:length(SMD)) {
  SMD[[i]]$es_zscore <- scale(SMD[[i]]$es, center = F, scale = TRUE) # without centering, which is used to estimate intercept
  SMD[[i]]$var_zscore <- scale(SMD[[i]]$var, scale = TRUE) - ( (0-mean(SMD[[i]]$var))/sd(SMD[[i]]$var) ) # see Equation 7 for explanations
  SMD[[i]]$sei_zscore <- scale(SMD[[i]]$sei, scale = TRUE) - ( (0-mean(SMD[[i]]$sei))/sd(SMD[[i]]$sei) ) # see Equation 7 for explanations
  SMD[[i]]$year_pub.l_zscore <- scale(SMD[[i]]$year_pub.l, scale = TRUE)
}

## also need to scale effective sample size related variables
for (i in 1:length(SMD[SMD_des_filenames])) {
  SMD[SMD_des_filenames][[i]]$ess.var_zscore <- scale(SMD[SMD_des_filenames][[i]]$ess.var, scale = TRUE) - ( (0-mean(SMD[SMD_des_filenames][[i]]$ess.var))/sd(SMD[SMD_des_filenames][[i]]$ess.var) ) # see Equation 7 for explanations
  SMD[SMD_des_filenames][[i]]$ess.sei_zscore <- scale(SMD[SMD_des_filenames][[i]]$ess.sei, scale = TRUE) - ( (0-mean(SMD[SMD_des_filenames][[i]]$ess.sei))/sd(SMD[SMD_des_filenames][[i]]$ess.sei) )
}


### Zr
for (i in 1:length(Zr)) {
  Zr[[i]]$es_zscore <- scale(Zr[[i]]$es, center = F, scale = TRUE) # without centering, which is used to estimate intercept
  Zr[[i]]$var_zscore <- scale(Zr[[i]]$var, scale = TRUE) - ( (0-mean(Zr[[i]]$var))/sd(Zr[[i]]$var) ) # see Equation 7 for explanations
  Zr[[i]]$sei_zscore <- scale(Zr[[i]]$sei, scale = TRUE) - ( (0-mean(Zr[[i]]$sei))/sd(Zr[[i]]$sei) ) # see Equation 7 for explanations
  Zr[[i]]$year_pub.l_zscore <- scale(Zr[[i]]$year_pub.l, scale = TRUE)
}


## scale data using respective weighted standard deviation - this can alleviate the negative impact of some extreme effect sizes

### lnRR
### calculate weighted sd for each lnRR dataset
# w.sd_es_lnRR <- NA
# for (i in 1:length(lnRR)) {
#   w.sd_es_lnRR[i] <- sqrt(weighted.var(lnRR[[i]]$es, w = 1/lnRR[[i]]$var)) %>% list()
# }
# for (i in 1:length(lnRR)) {
#   lnRR[[i]]$es_zscore2 <- lnRR[[i]]$es/w.sd_es_lnRR[[i]] # using weighted sd to scale data
# }

### SMD
### calculate weighted sd for each SMD dataset
# w.sd_es_SMD <- NA
# for (i in 1:length(SMD)) {
#   w.sd_es_SMD[i] <- sqrt(weighted.var(SMD[[i]]$es, w = 1/SMD[[i]]$var)) %>% list()
# }
### 6th value in w.sd_es_SMD is very unusual, so we use unweighted sd to replace it
# w.sd_es_SMD[[6]] <- sd(SMD[[6]]$es)
# for (i in 1:length(SMD)) {
#   SMD[[i]]$es_zscore2 <- SMD[[i]]$es/w.sd_es_SMD[[i]] # using weighted sd to scale data
# }

### Zr
### calculate weighted sd for each Zr dataset
# w.sd_es_Zr <- NA
#for (i in 1:length(Zr)) {
#  w.sd_es_Zr[i] <- sqrt(weighted.var(Zr[[i]]$es, w = 1/Zr[[i]]$var)) %>% list()
# }
# for (i in 1:length(Zr)) {
#  Zr[[i]]$es_zscore2 <- Zr[[i]]$es/w.sd_es_Zr[[i]] # using weighted sd to scale data
# }

```


# 2. Multilevel meta-analytic modelling  

we will use multilevel meta-analytic model to fit two types data:
2.1 original scale data
2.2 scaled data

For each type of data, we:
(i) estimate the meta-analytic overall mean, model intercept (beta0)
(ii) detect potential publication bias - test for small-study (sign & significance of beta1) and decline effects (sign & significance of beta1)
(iii) correct for publication bias and estimate bias-corrected overall mean (beta0_c)

# 2.1 original scale data

## (i) estimates of beta0
fit intercept-only multilevel model to each dataset

```{r}

#*************************************************************************#
#                        meta-analytic overall mean                    
#*************************************************************************#

## lnRR
## 8th meta-analysis (names(lnRR)[8]:"ft127.csv") can not achieve convergence (when fitting scaled effect sizes) although we used different numerical optimizer, adjusted different step length. So we deleted this dataset here
# lnRR <- lnRR[names(lnRR) != "ft127.csv"]
model_lnRR <- NA
for (i in 1:length(lnRR)) {
  model_lnRR[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), method = "REML", test = "t", data = lnRR[[i]], sparse = TRUE, control = list(optimizer = "optim")) %>% list()
}

## SMD
### 23rd meta-analysis (names(SMD)[23]:"ft040.csv") can not achieve convergence although we used different numerical optimizer, adjusted different step length.. So we deleted this dataset
# SMD <- SMD[names(SMD) != "ft040.csv"]
model_SMD <- NA
for (i in 1:length(SMD)) {
  model_SMD[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), method = "REML", test = "t", data = SMD[[i]], sparse = TRUE, control = list(optimizer = "optim")) %>% list()
}

## Zr
model_Zr <- NA
for (i in 1:length(Zr)) {
  model_Zr[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), method = "REML", test = "t", data = Zr[[i]], sparse=TRUE, control=list(optimizer="optim")) %>% list()
}

```


## (ii) detect publication bias

we aim for detecting two forms of publication bias: small-study effect  and decline effect.

we use a full model with sampling error (sei) and publication year (year_pub.l) as moderators to detect publication bias.

of relevance, sei's slope (beta1) and year_pub.l's slope (beta2) can be used to indicate the occurrence of small-study effect and decline effect, respectively.

note that for SMD we need to model 'effective sample size' based sampling error analogue (ees.sei), where possible

### (a) fit full models with samping error and year as predictors

```{r}
## the point estimate of SMD and lnRR are inherently correlated with their sampling variances. To avoid such ‘artefactual’ correlation between effect size and sampling error, we need to use "effective sample size" based sampling error to let its estimate get rid of point estimate


#*************************************************************************#
#       Full model with error and latest year as predictors
#*************************************************************************#

## use sampling error (sei) and latest year (year_pub.l) as predictors for those with calculated effect sizes and sampling variance

## lnRR
## as mentioned above, "ft127.csv" has the issue of convergence, so we need to excluded it
# lnRR_filenames <- lnRR_filenames[lnRR_filenames != "ft127.csv"]
model_lnRR_sei.year <- NA
for (i in 1:length(lnRR[lnRR_filenames])) {
  model_lnRR_sei.year[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ sei + year_pub.l, method = "REML", test = "t", data = lnRR[lnRR_filenames][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
} # note that here only fit the subset of lnRR[lnRR_filenames] - lnRR with calculated effect sizes and sampling variance

## use effective-sample-size based sampling error (ess.sei) where possible (lnRR with descriptive statistics)
model_lnRR_ess.sei.year <- NA
for (i in 1:length(lnRR[lnRR_des_filenames])) {
  model_lnRR_ess.sei.year[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ ess.sei + year_pub.l, method = "REML", test = "t", data = lnRR[lnRR_des_filenames][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
} 

## SMD
model_SMD_sei.year <- NA
for (i in 1:length(SMD[SMD_filenames])) {
  model_SMD_sei.year[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ sei + year_pub.l, method = "REML", test = "t", data = SMD[SMD_filenames][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
} # note that here only fit the subset of SMD[SMD_filenames] - SMD with calculated effect sizes and sampling variance

## use effective-sample-size based sampling error (ess.sei) where possible (SMD with descriptive statistics)

# as mentioned above, "ft040.csv" has the issue of convergence, so we need to excluded it
# SMD_des_filenames <- SMD_des_filenames[SMD_des_filenames != "ft040.csv"]
model_SMD_ess.sei.year <- NA
for (i in 1:length(SMD[SMD_des_filenames])) {
  model_SMD_ess.sei.year[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ ess.sei + year_pub.l, method = "REML", test = "t", data = SMD[SMD_des_filenames][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
} 

## Zr - Zr does not have the concern of ‘artefactual’ correlation between effect size and sampling error (because the formula to Zr's estimate sampling error has no component of point estimate: 1/(n-3)). So we only need to fit the regression model with sampling error (sei) as a predictor
model_Zr_sei.year <- NA
for (i in 1:length(Zr)) {
  model_Zr_sei.year[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ sei + year_pub.l, method = "REML", test = "t", data = Zr[[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}
```


#### bubble plot - publication bias
```{r}
## example of small-study effect
sse_SMD$case
which(model_est_SMD$case == "ft117.csv") # 10

pred.sse_SMD10 <- predict.rma(model_SMD_pb[[10]], newmods = cbind(seq(min(SMD[[10]]$sei), max(SMD[[10]]$sei), length.out=nrow(SMD[[10]])), c(0)))

newdat <- data.frame(sei= seq(min(SMD[[10]]$sei), max(SMD[[10]]$sei), length.out=nrow(SMD[[10]])),
                     fit=pred.sse_SMD10$pred,
                     upper=pred.sse_SMD10$ci.ub,
                     lower=pred.sse_SMD10$ci.lb,
                     stringsAsFactors=FALSE)

xaxis <- SMD[[10]]$sei
yaxis <- SMD[[10]]$es

png(filename = "./sse_SMD10.jpg", width = 5, height = 5, units = "in", res = 400, type = "windows")
plot(xaxis,yaxis, 
     type="n", 
     main="", ylab="", xlab="",
     # xlim = c(0, 4), ylim = c(-1, 9)
     # xaxt="n", yaxt="n"
     )
abline(a=0,b=0, lwd=1, lty=1)
title(main = "Small-study effect (ft117)",
      xlab = "Sampling error (SE)", 
      ylab = "SMD",
      line = 2.75, cex.lab=1.4)
points(xaxis,yaxis,
       bg=rgb(0,0,0, 0.1),
       col=rgb(0,0,0, 0.2),
       pch=21,
       cex=1)
lines(newdat$sei, newdat$fit, lwd=2.75,col="darkorchid4") 
polygon(c(newdat$sei,rev(newdat$sei)),
        c(newdat$lower,rev(newdat$upper)),
        border=NA,col=rgb(104/255,34/255,139/255, 0.5))
dev.off()


## example of decline effect
de_SMD$case
which(model_est_SMD$case == "ft167.csv") # 17

pred.de_SMD17 <- predict.rma(model_SMD_pb[[17]], newmods = cbind(mean(SMD[[17]]$sei), seq(min(SMD[[17]]$year_pub.l), max(SMD[[17]]$year_pub.l), length.out=nrow(SMD[[17]]))))


newdat <- data.frame(year= seq(min(SMD[[17]]$year_pub), max(SMD[[17]]$year_pub), length.out=nrow(SMD[[17]])),
                     fit=pred.de_SMD17$pred,
                     upper=pred.de_SMD17$ci.ub,
                     lower=pred.de_SMD17$ci.lb,
                     stringsAsFactors=FALSE)

xaxis <- SMD[[17]]$year_pub
yaxis <- SMD[[17]]$es

png(filename = "./de_SMD17.jpg", width = 5, height = 5, units = "in", res = 400, type = "windows")
plot(xaxis,yaxis, 
     type="n", 
     main="", ylab="", xlab="",
     # xlim = c(0, 4), ylim = c(-1, 9)
     # xaxt="n", yaxt="n"
     )
abline(a=0,b=0, lwd=1, lty=1)
title(main = "Decline effect (ft167)",
      xlab = "Publication year", 
      ylab = "SMD",
      line = 2.75, cex.lab=1.4)
points(xaxis,yaxis,
       bg=rgb(0,0,0, 0.1),
       col=rgb(0,0,0, 0.2),
       pch=21,
       cex=1)
lines(newdat$year, newdat$fit, lwd=2.75,col="darkorchid4") 
polygon(c(newdat$year,rev(newdat$year)),
        c(newdat$lower,rev(newdat$upper)),
        border=NA,col=rgb(104/255,34/255,139/255, 0.5))
dev.off()







lnRR[[6]] %>% mutate(ymin = pred_model1$ci.lb, 
                    ymax = pred_model1$ci.ub, 
                    ymin2 = pred_model1$cr.lb, 
                    ymax2 = pred_model1$cr.ub, 
                    pred = pred_model1$pred) %>% ggplot(aes(x = sei, y = es, size = 1/sqrt(sei))) + 
  geom_point(shape = 21, fill = 'grey95') + 
  #geom_smooth(aes(y = ymin2), method = "lm", se = FALSE, lty = "dashed", lwd = 0.6, colour = "#0072B2") + 
  #geom_smooth(aes(y = ymax2), method = "lm", se = FALSE, lty = "dashed", lwd = 0.6, colour = "#0072B2") + 
  geom_smooth(aes(y = ymin), method = "lm", se = FALSE, lty = "dashed", lwd = 0.6, colour = "#D55E00") + 
  geom_smooth(aes(y = ymax), method = "lm", se = FALSE, lty = "dashed", lwd = 0.6, colour = "#D55E00") + 
  geom_smooth(aes(y = pred), method = "lm", se = FALSE, lty = 1, lwd = 0.6, colour = "red") + 
  labs(x = "Sample error (SE) ", y = 'lnRR', size = expression(paste('Precision (1/SE)')), title = " ") + 
  scale_x_continuous(labels = scales::number_format(accuracy = 1)) + # need "scales" package
  guides(fill = "none", colour = "none") + 
  theme_bw() + 
  theme(legend.position = c(0, 1), legend.justification = c(0, 1)) + 
  theme(legend.direction = "horizontal") + 
  theme(legend.background = element_blank()) + 
  theme(axis.title = element_text(size = 16, colour = "black"),
        axis.text.x =  element_text(size = 16, colour = "black"),
        axis.text.y =  element_text(size = 16, colour = "black"),
        legend.text = element_text(size = 14, colour = "black"),
        legend.title = element_text(size = 14, colour = "black"),
        panel.grid = element_blank(),
        axis.ticks = element_line(size = 1, colour = "black"),
        axis.ticks.length = unit(0.15, "cm"),
        panel.border = element_rect(size = 1.2, colour = "black", fill = NA))




which(model_est_lnRR$case == "ft089.csv")

pred.sse_lnRR6 <- predict.rma(model_lnRR_pb[[6]], newmods = cbind(seq(min(lnRR[[6]]$sei), max(lnRR[[6]]$sei), length.out=177), c(mean(lnRR[[6]]$year_pub))))

# method = "loess"
lnRR[[6]] %>% mutate(ymin = pred.sse_lnRR6$ci.lb, 
                    ymax = pred.sse_lnRR6$ci.ub, 
                    ymin2 = pred.sse_lnRR6$cr.lb, 
                    ymax2 = pred.sse_lnRR6$cr.ub, 
                    pred = pred.sse_lnRR6$pred) %>% ggplot(aes(x = sei, y = es, size = 1/sqrt(sei))) + 
  geom_point(shape = 21, fill = 'grey95') + 
  geom_smooth(aes(y = ymin2), method = "loess", se = FALSE, lty = "dashed", lwd = 0.6, colour = "#0072B2") + 
  geom_smooth(aes(y = ymax2), method = "loess", se = FALSE, lty = "dashed", lwd = 0.6, colour = "#0072B2") + 
  geom_smooth(aes(y = ymin), method = "loess", se = FALSE, lty = "dashed", lwd = 0.6, colour = "#D55E00") + 
  geom_smooth(aes(y = ymax), method = "loess", se = FALSE, lty = "dashed", lwd = 0.6, colour = "#D55E00") + 
  geom_smooth(aes(y = pred), method = "loess", se = FALSE, lty = 1, lwd = 0.6, colour = "red") + 
  labs(x = "Sample error (SE) ", y = 'lnRR', size = expression(paste('Precision (1/SE)')), title = " ") + 
  scale_x_continuous(labels = scales::number_format(accuracy = 1)) + # need "scales" package
  guides(fill = "none", colour = "none") + 
  theme_bw() + 
  theme(legend.position = c(0, 1), legend.justification = c(0, 1)) + 
  theme(legend.direction = "horizontal") + 
  theme(legend.background = element_blank()) + 
  theme(axis.title = element_text(size = 16, colour = "black"),
        axis.text.x =  element_text(size = 16, colour = "black"),
        axis.text.y =  element_text(size = 16, colour = "black"),
        legend.text = element_text(size = 14, colour = "black"),
        legend.title = element_text(size = 14, colour = "black"),
        panel.grid = element_blank(),
        axis.ticks = element_line(size = 1, colour = "black"),
        axis.ticks.length = unit(0.15, "cm"),
        panel.border = element_rect(size = 1.2, colour = "black", fill = NA))


# method = "lm" - smoothing method (function)
lnRR[[6]] %>% mutate(ymin = pred.sse_lnRR6$ci.lb, 
                    ymax = pred.sse_lnRR6$ci.ub, 
                    ymin2 = pred.sse_lnRR6$cr.lb, 
                    ymax2 = pred.sse_lnRR6$cr.ub, 
                    pred = pred.sse_lnRR6$pred) %>% ggplot(aes(x = sei, y = es, size = 1/sqrt(sei))) + 
  geom_point(shape = 21, fill = 'grey95') + 
  geom_smooth(aes(y = ymin2), method = "lm", se = FALSE, lty = "dashed", lwd = 0.6, colour = "#0072B2") + 
  geom_smooth(aes(y = ymax2), method = "lm", se = FALSE, lty = "dashed", lwd = 0.6, colour = "#0072B2") + 
  geom_smooth(aes(y = ymin), method = "lm", se = FALSE, lty = "dashed", lwd = 0.6, colour = "#D55E00") + 
  geom_smooth(aes(y = ymax), method = "lm", se = FALSE, lty = "dashed", lwd = 0.6, colour = "#D55E00") + 
  geom_smooth(aes(y = pred), method = "lm", se = FALSE, lty = 1, lwd = 0.6, colour = "red") + 
  labs(x = "Sample error (SE) ", y = 'lnRR', size = expression(paste('Precision (1/SE)')), title = " ") + 
  scale_x_continuous(labels = scales::number_format(accuracy = 1)) + # need "scales" package
  guides(fill = "none", colour = "none") + 
  theme_bw() + 
  theme(legend.position = c(0, 1), legend.justification = c(0, 1)) + 
  theme(legend.direction = "horizontal") + 
  theme(legend.background = element_blank()) + 
  theme(axis.title = element_text(size = 16, colour = "black"),
        axis.text.x =  element_text(size = 16, colour = "black"),
        axis.text.y =  element_text(size = 16, colour = "black"),
        legend.text = element_text(size = 14, colour = "black"),
        legend.title = element_text(size = 14, colour = "black"),
        panel.grid = element_blank(),
        axis.ticks = element_line(size = 1, colour = "black"),
        axis.ticks.length = unit(0.15, "cm"),
        panel.border = element_rect(size = 1.2, colour = "black", fill = NA))



jpeg(filename = "./time_trend_RR.p.jpg", width = 5, height = 5, units = "in", type = "windows", res = 400)
time_trend_RR.p
dev.off()

```


### (b) identify the presence of publication bias

we next aim for identify the presence of the small-study effect and decline effect for each meta-analysis. 

our rational is: for an effect that is expected to be positive, a small study effect and decline effect would be expressed in a positive value of beta1 and negative value of beta2, respectively. In this respect, a slope (beta1 or beta2)) with opposing direction (unexpected sign) indicates no detectable publication bias and subsequently does not require correction for such a bias.

we use the product of beta0 and beta1 (i.e., beta×beta1) as the signal, that is, if beta0×beta1 is positive, it indicates the examined meta-analysis has a small-study effect (beta1 is in a correct direction).

similarly, when the product of beta0×beta2 is negative, the examined meta-analysis has a decline effect (beta 2 is in a correct direction)

##### lnRR
```{r}
#*************************************************************************#
#             Identify the presence of publication bias 
#*************************************************************************#

## check the significance and direction of model regressions from each meta-analysis to identify whether it presents a small-study effect or decline effect

## first to create a dataframe containing full-model's parameter estimates
### combine two types full-models (with sei and ess.sei as a predictor, respectively)
### lnRR
model_lnRR_pb <- append(model_lnRR_sei.year, model_lnRR_ess.sei.year)
### extract model model coefficients and their significance test results
model_est_lnRR <- data.frame(case = names(lnRR),
                             es_type = rep("lnRR", length(lnRR)),
                             beta0 = sapply(model_lnRR, function(x) x$beta), # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = sapply(model_lnRR, function(x) x$se), # standard error of beta0
                             pval_beta0 = sapply(model_lnRR, function(x) x$pval), # p value of beta0
                             beta0_c = sapply(model_lnRR_pb, function(x) x$beta[1]), # beta0_c in equation 2 -  bias corrected overall mean
                             se_beta0_c = sapply(model_lnRR_pb, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_lnRR_pb, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_lnRR_pb, function(x) x$beta[2]), # beta1 in Equation 2 - slope of sampling error 
                             se_beta1 = sapply(model_lnRR_pb, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_lnRR_pb, function(x) x$pval[2]), # p value of beta1
                             beta2 = sapply(model_lnRR_pb, function(x) x$beta[3]), # beta2 in Equation 2 - slope of year 
                             se_beta2 = sapply(model_lnRR_pb, function(x) x$se[3]), # standard error of beta2
                             pval_beta2 = sapply(model_lnRR_pb, function(x) x$pval[3]) # p value of beta2
                            )






## we next aim to identify the presence of the small-study effect and decline effect for each meta-analysis. We also figure out the wrong directions of slope, which will be used to inform the parameterization of reduced models

## our rational is: for an effect that is expected to be positive, a small study effect and decline effect would be expressed in a positive value of beta1 and negative value of beta2, respectively. In such a case, a slope (beta1] or beta2)) with opposing direction (unexpected sign) indicates no detectable publication bias and subsequently does not require correction for such a bias

## we use the product of beta0 and beta1 (i.e., beta*beta1) as the signal, that is, if beta0*beta1 is positive, it indicates the examined meta-analysis has a small-study effect (beta1 is in a correct direction)

## of relevance, when the value of beta0*beta2 is negative, the examined meta-analysis has a decline effect (beta 2 is in a correct direction)

## so we first to create two new columns to contain the two products:  beta0*beta1 and beta0*beta2

## lnRR
model_est_lnRR[15:16] <- data.frame(beta0Tbeta1 = model_est_lnRR$beta0 * model_est_lnRR$beta1, beta0Tbeta2 = model_est_lnRR$beta0 * model_est_lnRR$beta2) # model_est_lnRR has 14 column (ncol(model_est_lnRR)), so we add columns 15 and 16

## visual check
model_est_lnRR

## identify the small-study effect - significant beta1 with correct sign
sse_lnRR <- model_est_lnRR %>% subset(model_est_lnRR$pval_beta1 < 0.05 & model_est_lnRR$beta0Tbeta1 > 0)
## check which meta-analyses have small-study effects
sse_lnRR$case 



## identify the decline effect - significant beta2 with correct sign 
de_lnRR <- model_est_lnRR %>% subset(model_est_lnRR$pval_beta2 < 0.05 & model_est_lnRR$beta0Tbeta2 < 0)
## check which meta-analyses have decline effects
de_lnRR$case # 


## identify the concurrence of the small-study effect and decline effect
sse_de_lnRR <- model_est_lnRR %>% subset(model_est_lnRR$pval_beta1 < 0.05 & model_est_lnRR$beta0Tbeta1 > 0 & model_est_lnRR$pval_beta2 < 0.05 & model_est_lnRR$beta0Tbeta2 < 0) 
sse_de_lnRR$case
```

##### SMD
```{r}
### SMD
model_SMD_pb <- append(model_SMD_sei.year, model_SMD_ess.sei.year)
### extract model model coefficients and their significance test results
model_est_SMD <- data.frame(case = names(SMD),
                             es_type = rep("SMD", length(SMD)),
                             beta0 = sapply(model_SMD, function(x) x$beta), # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = sapply(model_SMD, function(x) x$se), # standard error of beta0
                             pval_beta0 = sapply(model_SMD, function(x) x$pval), # p value of beta0
                             beta0_c = sapply(model_SMD_pb, function(x) x$beta[1]), # beta0_c in equation 2 -  bias corrected overall mean
                             se_beta0_c = sapply(model_SMD_pb, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_SMD_pb, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_SMD_pb, function(x) x$beta[2]), # beta1 in Equation 2 - slope of sampling error 
                             se_beta1 = sapply(model_SMD_pb, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_SMD_pb, function(x) x$pval[2]), # p value of beta1
                             beta2 = sapply(model_SMD_pb, function(x) x$beta[3]), # beta2 in Equation 2 - slope of year 
                             se_beta2 = sapply(model_SMD_pb, function(x) x$se[3]), # standard error of beta2
                             pval_beta2 = sapply(model_SMD_pb, function(x) x$pval[3]) # p value of beta2
                            )


## we next aim to identify the presence of the small-study effect and decline effect for each meta-analysis. We also figure out the wrong directions of slope, which will be used to inform the parameterization of reduced models

## our rational is: for an effect that is expected to be positive, a small study effect and decline effect would be expressed in a positive value of beta1 and negative value of beta2, respectively. In such a case, a slope (beta1] or beta2)) with opposing direction (unexpected sign) indicates no detectable publication bias and subsequently does not require correction for such a bias

## we use the product of beta0 and beta1 (i.e., beta*beta1) as the signal, that is, if beta0*beta1 is positive, it indicates the examined meta-analysis has a small-study effect (beta1 is in a correct direction)

## of relevance, when the value of beta0*beta2 is negative, the examined meta-analysis has a decline effect (beta 2 is in a correct direction)

## so we first to create two new columns to contain the two products:  beta0*beta1 and beta0*beta2

## SMD
model_est_SMD[15:16] <- data.frame(beta0Tbeta1 = model_est_SMD$beta0 * model_est_SMD$beta1, beta0Tbeta2 = model_est_SMD$beta0 * model_est_SMD$beta2) # model_est_SMD has 14 column (ncol(model_est_SMD)), so we add columns 15 and 16

## visual check
model_est_SMD

## identify the small-study effect - significant beta1 with correct sign
sse_SMD <- model_est_SMD %>% subset(model_est_SMD$pval_beta1 < 0.05 & model_est_SMD$beta0Tbeta1 > 0)
## check which meta-analyses have small-study effects
sse_SMD$case 



## identify the decline effect - significant beta2 with correct sign 
de_SMD <- model_est_SMD %>% subset(model_est_SMD$pval_beta2 < 0.05 & model_est_SMD$beta0Tbeta2 < 0)
## check which meta-analyses have decline effects
de_SMD$case # 

## identify the concurrence of the small-study effect and decline effect
sse_de_SMD <- model_est_SMD %>% subset(model_est_SMD$pval_beta1 < 0.05 & model_est_SMD$beta0Tbeta1 > 0 & model_est_SMD$pval_beta2 < 0.05 & model_est_SMD$beta0Tbeta2 < 0) 
sse_de_SMD$case # no meta-analysis surfers from both a small-study effect and decline effect
```

##### Zr
```{r}
### Zr
model_Zr_pb <- model_Zr_sei.year # for consistence, we use a similar naming
### extract model model coefficients and their significance test results
model_est_Zr <- data.frame(case = names(Zr),
                             es_type = rep("Zr", length(Zr)),
                             beta0 = sapply(model_Zr, function(x) x$beta), # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = sapply(model_Zr, function(x) x$se), # standard error of beta0
                             pval_beta0 = sapply(model_Zr, function(x) x$pval), # p value of beta0
                             beta0_c = sapply(model_Zr_pb, function(x) x$beta[1]), # beta0_c in equation 2 -  bias corrected overall mean
                             se_beta0_c = sapply(model_Zr_pb, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_Zr_pb, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_Zr_pb, function(x) x$beta[2]), # beta1 in Equation 2 - slope of sampling error 
                             se_beta1 = sapply(model_Zr_pb, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_Zr_pb, function(x) x$pval[2]), # p value of beta1
                             beta2 = sapply(model_Zr_pb, function(x) x$beta[3]), # beta2 in Equation 2 - slope of year 
                             se_beta2 = sapply(model_Zr_pb, function(x) x$se[3]), # standard error of beta2
                             pval_beta2 = sapply(model_Zr_pb, function(x) x$pval[3]) # p value of beta2
                            )


#*************************************************************************#
#             Identify the presence of publication bias 
#*************************************************************************#

## check the significance and direction of model regressions from each meta-analysis to identify whether it presents a small-study effect or decline effect

## first to create a dataframe containing full-model's parameter estimates
### combine two types full-models (with sei and ess.sei as a predictor, respectively)
### Zr
model_Zr_pb <- model_Zr_sei.year
### extract model model coefficients and their significance test results
model_est_Zr <- data.frame(case = names(Zr),
                             es_type = rep("Zr", length(Zr)),
                             beta0 = sapply(model_Zr, function(x) x$beta), # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = sapply(model_Zr, function(x) x$se), # standard error of beta0
                             pval_beta0 = sapply(model_Zr, function(x) x$pval), # p value of beta0
                             beta0_c = sapply(model_Zr_pb, function(x) x$beta[1]), # beta0_c in equation 2 -  bias corrected overall mean
                             se_beta0_c = sapply(model_Zr_pb, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_Zr_pb, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_Zr_pb, function(x) x$beta[2]), # beta1 in Equation 2 - slope of sampling error 
                             se_beta1 = sapply(model_Zr_pb, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_Zr_pb, function(x) x$pval[2]), # p value of beta1
                             beta2 = sapply(model_Zr_pb, function(x) x$beta[3]), # beta2 in Equation 2 - slope of year 
                             se_beta2 = sapply(model_Zr_pb, function(x) x$se[3]), # standard error of beta2
                             pval_beta2 = sapply(model_Zr_pb, function(x) x$pval[3]) # p value of beta2
                            )

## we next aim to identify the presence of the small-study effect and decline effect for each meta-analysis. We also figure out the wrong directions of slope, which will be used to inform the parameterization of reduced models

## our rational is: for an effect that is expected to be positive, a small study effect and decline effect would be expressed in a positive value of beta1 and negative value of beta2, respectively. In such a case, a slope (beta1] or beta2)) with opposing direction (unexpected sign) indicates no detectable publication bias and subsequently does not require correction for such a bias

## we use the product of beta0 and beta1 (i.e., beta*beta1) as the signal, that is, if beta0*beta1 is positive, it indicates the examined meta-analysis has a small-study effect (beta1 is in a correct direction)

## of relevance, when the value of beta0*beta2 is negative, the examined meta-analysis has a decline effect (beta 2 is in a correct direction)

## so we first to create two new columns to contain the two products:  beta0*beta1 and beta0*beta2

## Zr
model_est_Zr[15:16] <- data.frame(beta0Tbeta1 = model_est_Zr$beta0 * model_est_Zr$beta1, beta0Tbeta2 = model_est_Zr$beta0 * model_est_Zr$beta2) # model_est_Zr has 14 column (ncol(model_est_Zr)), so we add columns 15 and 16

## visual check
model_est_Zr

## identify the small-study effect - significant beta1 with correct sign
sse_Zr <- model_est_Zr %>% subset(model_est_Zr$pval_beta1 < 0.05 & model_est_Zr$beta0Tbeta1 > 0)
## check which meta-analyses have small-study effects
sse_Zr$case 



## identify the decline effect - significant beta2 with correct sign 
de_Zr <- model_est_Zr %>% subset(model_est_Zr$pval_beta2 < 0.05 & model_est_Zr$beta0Tbeta2 < 0)
## check which meta-analyses have decline effects
de_Zr$case 

## identify the concurrence of the small-study effect and decline effect
sse_de_Zr <- model_est_Zr %>% subset(model_est_Zr$pval_beta1 < 0.05 & model_est_Zr$beta0Tbeta1 > 0 & model_est_Zr$pval_beta2 < 0.05 & model_est_Zr$beta0Tbeta2 < 0) 
sse_de_Zr$case
```



## (iii) correct for publication bias  
we aim for estimating bias-corrected beta0 according to 4 scenarios as outlined in the main text (permutation of the signs of beta1 and beta2)

### lnRR
```{r}
#*************************************************************************#
#        Multilevel models to estimate bias-corrected effect
#*************************************************************************#

#*****************************scenario 1****************************#
## both beta1 and beta2 has a correct direction
beta1c_beta2c_lnRR <- model_est_lnRR %>% subset(model_est_lnRR$beta0Tbeta1 > 0 & model_est_lnRR$beta0Tbeta2 < 0) 
beta1c_beta2c_lnRR$case 

## if a model slope (beta1 and beta2) has a wrong direction, we need to take out it when fitting model to estimate the bias-corrected mean
## in scenario 1, both of the two slopes have a correct direction, we use can use full model directly, no need to use take out any predictor

## full model based on scenario 1 - both beta1 and beta2 has a correct direction
## make a data list which only contains scenario1's data
s1_file <- beta1c_beta2c_lnRR$case
## fit ess.sei where possible
## subset of sei
s1_sei_file <- lnRR_filenames[lnRR_filenames %in% s1_file] # this subset should use sei as a predictor and belong to scenario 1
## model fitting - fit a full model
model_lnRR_sei_s1 <- NA
for (i in 1:length(s1_sei_file)) {
  model_lnRR_sei_s1[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ sei + year_pub.l, method = "REML", test = "t", data = lnRR[s1_sei_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## replace sei by var
model_lnRR_var_s1 <- NA
for (i in 1:length(s1_sei_file)) {
  model_lnRR_var_s1[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ var + year_pub.l, method = "REML", test = "t", data = lnRR[s1_sei_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}


## extract model model coefficients and their significance test results for 'sei' in scenario 1
model_est_lnRR_sei_s1 <- data.frame(case = s1_sei_file,
                             es_type = rep("lnRR", length(s1_sei_file)),
                             beta0 = model_est_lnRR[model_est_lnRR$case %in% s1_sei_file, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_lnRR[model_est_lnRR$case %in% s1_sei_file, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_lnRR[model_est_lnRR$case %in% s1_sei_file, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_lnRR_sei_s1, function(x) x$beta[1]), # beta0_c in equation 6 -  bias corrected overall mean
                             se_beta0_c = sapply(model_lnRR_sei_s1, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_lnRR_sei_s1, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_lnRR_sei_s1, function(x) x$beta[2]), # beta1 in Equation 6 - slope of sampling error 
                             se_beta1 = sapply(model_lnRR_sei_s1, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_lnRR_sei_s1, function(x) x$pval[2]), # p value of beta1
                             beta2 = sapply(model_lnRR_sei_s1, function(x) x$beta[3]), # beta2 in Equation 6 - slope of year 
                             se_beta2 = sapply(model_lnRR_sei_s1, function(x) x$se[3]), # standard error of beta2
                             pval_beta2 = sapply(model_lnRR_sei_s1, function(x) x$pval[3]), # p value of beta2
                             beta0_c2 = sapply(model_lnRR_var_s1, function(x) x$beta[1]), # beta0_c in equation 6 -  bias corrected overall mean
                             se_beta0_c2 = sapply(model_lnRR_var_s1, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_lnRR_var_s1, function(x) x$pval[1]) # p valuer of beta0_c
                            )



## subset of ess.sei
s1_ess.sei_file <- lnRR_des_filenames[lnRR_des_filenames %in% s1_file] # this subset should fit ess.sei as a predictor and belong to scenario 1
## model fitting - full model, which does not need to take out any predictor
model_lnRR_ess.sei_s1 <- NA
for (i in 1:length(s1_ess.sei_file)) {
  model_lnRR_ess.sei_s1[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ ess.sei + year_pub.l, method = "REML", test = "t", data = lnRR[s1_ess.sei_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

# replace sei by var
model_lnRR_ess.var_s1 <- NA
for (i in 1:length(s1_ess.sei_file)) {
  model_lnRR_ess.var_s1[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ ess.var + year_pub.l, method = "REML", test = "t", data = lnRR[s1_ess.sei_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model model coefficients and their significance test results 'see.sei' for scenario 2 
model_est_lnRR_ess.sei_s1 <- data.frame(case = s1_ess.sei_file,
                             es_type = rep("lnRR", length(s1_ess.sei_file)),
                             beta0 = model_est_lnRR[model_est_lnRR$case %in% s1_ess.sei_file, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_lnRR[model_est_lnRR$case %in% s1_ess.sei_file, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_lnRR[model_est_lnRR$case %in% s1_ess.sei_file, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_lnRR_ess.sei_s1, function(x) x$beta[1]), # beta0_c in equation 6 -  bias corrected overall mean
                             se_beta0_c = sapply(model_lnRR_ess.sei_s1, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_lnRR_ess.sei_s1, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_lnRR_ess.sei_s1, function(x) x$beta[2]), # beta1 in Equation 6 - slope of sampling error 
                             se_beta1 = sapply(model_lnRR_ess.sei_s1, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_lnRR_ess.sei_s1, function(x) x$pval[2]), # p value of beta1
                             beta2 = sapply(model_lnRR_ess.sei_s1, function(x) x$beta[3]), # beta2 in Equation 6 - slope of year 
                             se_beta2 = sapply(model_lnRR_ess.sei_s1, function(x) x$se[3]), # standard error of beta2
                             pval_beta2 = sapply(model_lnRR_ess.sei_s1, function(x) x$pval[3]), # p value of beta2
                             beta0_c2 = sapply(model_lnRR_ess.var_s1, function(x) x$beta[1]), # beta0_c in equation 6 -  bias corrected overall mean
                             se_beta0_c2 = sapply(model_lnRR_ess.var_s1, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_lnRR_ess.var_s1, function(x) x$pval[1]) # p valuer of beta0_c
                            )


#*****************************scenario 2****************************#
## beta1 has a wrong direction, while beta2 has a correct direction
beta1w_beta2c_lnRR <- model_est_lnRR %>% subset(model_est_lnRR$beta0Tbeta1 < 0 & model_est_lnRR$beta0Tbeta2 < 0) 
beta1w_beta2c_lnRR$case 


## reduced model based on scenario 2 - beta1 has a wrong direction, while beta2 has a correct direction
## make a data list which only contains scenario2's data
s2_file <-  beta1w_beta2c_lnRR$case
## model fitting -  take out beta1-related predictor (sei or esss.sei) and keep beta2-related predictor (year_pub.l)
## beta1-related predictor is removed, so no need to fit ess.sei where possible
model_lnRR_s2 <- NA
for (i in 1:length(s2_file)) {
  model_lnRR_s2[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ year_pub.l, method = "REML", test = "t", data = lnRR[s2_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model model coefficients and their significance test results scenario 2
model_est_lnRR_s2 <- data.frame(case = s2_file,
                             es_type = rep("lnRR", length(s2_file)),
                             beta0 = model_est_lnRR[model_est_lnRR$case %in% s2_file, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_lnRR[model_est_lnRR$case %in% s2_file, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_lnRR[model_est_lnRR$case %in% s2_file, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_lnRR_s2, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c = sapply(model_lnRR_s2, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_lnRR_s2, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = 0, # beta1 in Equation 5 - slope of sampling error: no error term
                             se_beta1 = 0, # standard error of beta1
                             pval_beta1 = 0, # p value of beta1
                             beta2 = sapply(model_lnRR_s2, function(x) x$beta[2]), # beta2 in Equation 5 - slope of year 
                             se_beta2 = sapply(model_lnRR_s2, function(x) x$se[2]), # standard error of beta2
                             pval_beta2 = sapply(model_lnRR_s2, function(x) x$pval[2]), # p value of beta2
                             beta0_c2 = sapply(model_lnRR_s2, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c2 = sapply(model_lnRR_s2, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_lnRR_s2, function(x) x$pval[1]) # p valuer of beta0_c
                            )


#*****************************scenario 3****************************#
## beta1 has a correct direction, while beta2 has a wrong direction
beta1c_beta2w_lnRR <- model_est_lnRR %>% subset(model_est_lnRR$beta0Tbeta1 > 0 & model_est_lnRR$beta0Tbeta2 > 0) 
beta1c_beta2w_lnRR$case 

## reduced model based on scenario 3 - beta1 has a correct direction, while beta2 has a wrong direction
## make a data list which only contains scenario3's data
s3_file <-  beta1c_beta2w_lnRR$case
## fit ess.sei where possible
## subset of sei
s3_sei_file <- lnRR_filenames[lnRR_filenames %in% s3_file] # this subset should fit sei as a predictor and belong to scenario 3
## model fitting - keep beta1-related predictor (sei) and take out beta2-related predictor (year_pub.l)
model_lnRR_sei_s3 <- NA
for (i in 1:length(s3_sei_file)) {
  model_lnRR_sei_s3[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ sei, method = "REML", test = "t", data = lnRR[s3_sei_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

# replace var by sei
model_lnRR_var_s3 <- NA
for (i in 1:length(s3_sei_file)) {
  model_lnRR_var_s3[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ var, method = "REML", test = "t", data = lnRR[s3_sei_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}


## subset of ess.sei
s3_ess.sei_file <- lnRR_des_filenames[lnRR_des_filenames %in% s3_file] # this subset should fit ess.sei as a predictor and belong to scenario 3
## model fitting - keep beta1-related predictor (sei) and take out beta2-related predictor (year_pub.l)
model_lnRR_ess.sei_s3 <- NA
for (i in 1:length(s3_ess.sei_file)) {
  model_lnRR_ess.sei_s3[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ ess.sei, method = "REML", test = "t", data = lnRR[s3_ess.sei_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

# replace sei by var
model_lnRR_ess.var_s3 <- NA
for (i in 1:length(s3_ess.sei_file)) {
  model_lnRR_ess.var_s3[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ ess.var, method = "REML", test = "t", data = lnRR[s3_ess.sei_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model model coefficients and their significance test results for 'sei' in scenario 3
model_est_lnRR_sei_s3 <- data.frame(case = s3_sei_file,
                             es_type = rep("lnRR", length(s3_sei_file)),
                             beta0 = model_est_lnRR[model_est_lnRR$case %in% s3_sei_file, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_lnRR[model_est_lnRR$case %in% s3_sei_file, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_lnRR[model_est_lnRR$case %in% s3_sei_file, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_lnRR_sei_s3, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c = sapply(model_lnRR_sei_s3, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_lnRR_sei_s3, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_lnRR_sei_s3, function(x) x$beta[2]), # beta1 in Equation 5 - slope of sampling error 
                             se_beta1 = sapply(model_lnRR_sei_s3, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_lnRR_sei_s3, function(x) x$pval[2]), # p value of beta1
                             beta2 = 0, # beta2 in Equation 5 - slope of year: no year term 
                             se_beta2 = 0, # standard error of beta2
                             pval_beta2 = 0, # p value of beta2
                             beta0_c2 = sapply(model_lnRR_var_s3, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c2 = sapply(model_lnRR_var_s3, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_lnRR_var_s3, function(x) x$pval[1]) # p valuer of beta0_c
                            )

## extract model model coefficients and their significance test results for 'ess.sei' in scenario 3
model_est_lnRR_ess.sei_s3 <- data.frame(case = s3_ess.sei_file,
                             es_type = rep("lnRR", length(s3_ess.sei_file)),
                             beta0 = model_est_lnRR[model_est_lnRR$case %in% s3_ess.sei_file, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_lnRR[model_est_lnRR$case %in% s3_ess.sei_file, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_lnRR[model_est_lnRR$case %in% s3_ess.sei_file, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_lnRR_ess.sei_s3, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c = sapply(model_lnRR_ess.sei_s3, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_lnRR_ess.sei_s3, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_lnRR_ess.sei_s3, function(x) x$beta[2]), # beta1 in Equation 5 - slope of sampling error 
                             se_beta1 = sapply(model_lnRR_ess.sei_s3, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_lnRR_ess.sei_s3, function(x) x$pval[2]), # p value of beta1
                             beta2 = 0, # beta2 in Equation 5 - slope of year: no year term 
                             se_beta2 = 0, # standard error of beta2
                             pval_beta2 = 0, # p value of beta2
                             beta0_c2 = sapply(model_lnRR_ess.sei_s3, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c2 = sapply(model_lnRR_ess.sei_s3, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_lnRR_ess.sei_s3, function(x) x$pval[1]) # p valuer of beta0_c
                            )


#*****************************scenario 4****************************#
## both beta1 and beta2 has a wrong direction
beta1w_beta2w_lnRR <- model_est_lnRR %>% subset(model_est_lnRR$beta0Tbeta1 < 0 & model_est_lnRR$beta0Tbeta2 > 0) 
beta1w_beta2w_lnRR$case 

## reduced model based on scenario 4 - beta1 has a wrong direction and beta2 has a wrong direction 
## this reduced model needs to take out both of the two predictors (sei and pub_year.l). This is equivalent to a null model (intercept-only model), which is used to estimate (uncorrected) meta-analytic overall mean

## make a data list which only contains scenario4's data
s4_file <-  beta1w_beta2w_lnRR$case
## no need to subset sei and ess.sei because scenario4 fits a null model (without any predictor)
## model fitting - take out both beta1-related predictor (sei or ess.sei) and  beta2-related predictor (year_pub.l)
model_lnRR_s4 <- NA
for (i in 1:length(s4_file)) {
  model_lnRR_s4[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), method = "REML", test = "t", data = lnRR[s4_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model model coefficients and their significance test results scenario 4
model_est_lnRR_s4 <- data.frame(case = s4_file,
                             es_type = rep("lnRR", length(s4_file)),
                             beta0 = model_est_lnRR[model_est_lnRR$case %in% s4_file, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean # or sapply(model_lnRR_s4, function(x) x$beta[1])
                             se_beta0 = model_est_lnRR[model_est_lnRR$case %in% s4_file, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_lnRR[model_est_lnRR$case %in% s4_file, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_lnRR_s4, function(x) x$beta[1]), # beta0_c in equation 1 -  bias corrected overall mean: this is equal to uncorrected overall mean (i.e., beta0)
                             se_beta0_c = sapply(model_lnRR_s4, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_lnRR_s4, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = 0, # beta1 in Equation 1 - slope of sampling error: no error term
                             se_beta1 = 0, # standard error of beta1
                             pval_beta1 = 0, # p value of beta1
                             beta2 = 0, # beta2 in Equation 1 - slope of year 
                             se_beta2 = 0, # standard error of beta2
                             pval_beta2 = 0, # p value of beta2
                             beta0_c2 = sapply(model_lnRR_s4, function(x) x$beta[1]), # beta0_c in equation 1 -  bias corrected overall mean: this is equal to uncorrected overall mean (i.e., beta0)
                             se_beta0_c2 = sapply(model_lnRR_s4, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_lnRR_s4, function(x) x$pval[1]) # p valuer of beta0_c
                            )

```



### SMD
```{r}
#*************************************************************************#
#        Multilevel models to estimate bias-corrected effect
#*************************************************************************#

#*****************************scenario 1****************************#
## both beta1 and beta2 has a correct direction
beta1c_beta2c_SMD <- model_est_SMD %>% subset(model_est_SMD$beta0Tbeta1 > 0 & model_est_SMD$beta0Tbeta2 < 0) 
beta1c_beta2c_SMD$case  

## if a model slope (beta1 and beta2) has a wrong direction, we need to take out it when fitting model to estimate the bias-corrected mean
## in scenario 1, both of the two slopes have a correct direction, we use can use full model directly, no need to use take out any predictor

## full model based on scenario 1 - both beta1 and beta2 has a correct direction
## make a data list which only contains scenario1's data
s1_file <- beta1c_beta2c_SMD$case
## fit ess.sei where possible
## subset of sei
s1_sei_file <- SMD_filenames[SMD_filenames %in% s1_file] # this subset should use sei as a predictor and belong to scenario 1
## model fitting - fit a full model
model_SMD_sei_s1 <- NA
for (i in 1:length(s1_sei_file)) {
  model_SMD_sei_s1[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ sei + year_pub.l, method = "REML", test = "t", data = SMD[s1_sei_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## replace sei by var
model_SMD_var_s1 <- NA
for (i in 1:length(s1_sei_file)) {
  model_SMD_var_s1[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ var + year_pub.l, method = "REML", test = "t", data = SMD[s1_sei_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model model coefficients and their significance test results for 'sei' in scenario 1
model_est_SMD_sei_s1 <- data.frame(case = s1_sei_file,
                             es_type = rep("SMD", length(s1_sei_file)),
                             beta0 = model_est_SMD[model_est_SMD$case %in% s1_sei_file, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_SMD[model_est_SMD$case %in% s1_sei_file, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_SMD[model_est_SMD$case %in% s1_sei_file, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_SMD_sei_s1, function(x) x$beta[1]), # beta0_c in equation 6 -  bias corrected overall mean
                             se_beta0_c = sapply(model_SMD_sei_s1, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_SMD_sei_s1, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_SMD_sei_s1, function(x) x$beta[2]), # beta1 in Equation 6 - slope of sampling error 
                             se_beta1 = sapply(model_SMD_sei_s1, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_SMD_sei_s1, function(x) x$pval[2]), # p value of beta1
                             beta2 = sapply(model_SMD_sei_s1, function(x) x$beta[3]), # beta2 in Equation 6 - slope of year 
                             se_beta2 = sapply(model_SMD_sei_s1, function(x) x$se[3]), # standard error of beta2
                             pval_beta2 = sapply(model_SMD_sei_s1, function(x) x$pval[3]), # p value of beta2
                             beta0_c2 = sapply(model_SMD_var_s1, function(x) x$beta[1]), # beta0_c in equation 6 -  bias corrected overall mean
                             se_beta0_c2 = sapply(model_SMD_var_s1, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_SMD_var_s1, function(x) x$pval[1]) # p valuer of beta0_c
                            )



## subset of ess.sei
s1_ess.sei_file <- SMD_des_filenames[SMD_des_filenames %in% s1_file] # this subset should fit ess.sei as a predictor and belong to scenario 1
## model fitting - full model, which does not need to take out any predictor
model_SMD_ess.sei_s1 <- NA
for (i in 1:length(s1_ess.sei_file)) {
  model_SMD_ess.sei_s1[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ ess.sei + year_pub.l, method = "REML", test = "t", data = SMD[s1_ess.sei_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## replace sei by var
model_SMD_ess.var_s1 <- NA
for (i in 1:length(s1_ess.sei_file)) {
  model_SMD_ess.var_s1[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ ess.var + year_pub.l, method = "REML", test = "t", data = SMD[s1_ess.sei_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model model coefficients and their significance test results 'see.sei' for scenario 2 
model_est_SMD_ess.sei_s1 <- data.frame(case = s1_ess.sei_file,
                             es_type = rep("SMD", length(s1_ess.sei_file)),
                             beta0 = model_est_SMD[model_est_SMD$case %in% s1_ess.sei_file, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_SMD[model_est_SMD$case %in% s1_ess.sei_file, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_SMD[model_est_SMD$case %in% s1_ess.sei_file, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_SMD_ess.sei_s1, function(x) x$beta[1]), # beta0_c in equation 6 -  bias corrected overall mean
                             se_beta0_c = sapply(model_SMD_ess.sei_s1, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_SMD_ess.sei_s1, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_SMD_ess.sei_s1, function(x) x$beta[2]), # beta1 in Equation 6 - slope of sampling error 
                             se_beta1 = sapply(model_SMD_ess.sei_s1, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_SMD_ess.sei_s1, function(x) x$pval[2]), # p value of beta1
                             beta2 = sapply(model_SMD_ess.sei_s1, function(x) x$beta[3]), # beta2 in Equation 6 - slope of year 
                             se_beta2 = sapply(model_SMD_ess.sei_s1, function(x) x$se[3]), # standard error of beta2
                             pval_beta2 = sapply(model_SMD_ess.sei_s1, function(x) x$pval[3]), # p value of beta2
                             beta0_c2 = sapply(model_SMD_ess.var_s1, function(x) x$beta[1]), # beta0_c in equation 6 -  bias corrected overall mean
                             se_beta0_c2 = sapply(model_SMD_ess.var_s1, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_SMD_ess.var_s1, function(x) x$pval[1]) # p valuer of beta0_c
                            )


#*****************************scenario 2****************************#
## beta1 has a wrong direction, while beta2 has a correct direction
beta1w_beta2c_SMD <- model_est_SMD %>% subset(model_est_SMD$beta0Tbeta1 < 0 & model_est_SMD$beta0Tbeta2 < 0) 
beta1w_beta2c_SMD$case 

## reduced model based on scenario 2 - beta1 has a wrong direction, while beta2 has a correct direction
## make a data list which only contains scenario2's data
s2_file <-  beta1w_beta2c_SMD$case
## model fitting -  take out beta1-related predictor (sei or esss.sei) and keep beta2-related predictor (year_pub.l)
## beta1-related predictor is removed, so no need to fit ess.sei where possible
model_SMD_s2 <- NA
for (i in 1:length(s2_file)) {
  model_SMD_s2[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ year_pub.l, method = "REML", test = "t", data = SMD[s2_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model model coefficients and their significance test results scenario 2
model_est_SMD_s2 <- data.frame(case = s2_file,
                             es_type = rep("SMD", length(s2_file)),
                             beta0 = model_est_SMD[model_est_SMD$case %in% s2_file, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_SMD[model_est_SMD$case %in% s2_file, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_SMD[model_est_SMD$case %in% s2_file, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_SMD_s2, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c = sapply(model_SMD_s2, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_SMD_s2, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = 0, # beta1 in Equation 5 - slope of sampling error: no error term
                             se_beta1 = 0, # standard error of beta1
                             pval_beta1 = 0, # p value of beta1
                             beta2 = sapply(model_SMD_s2, function(x) x$beta[2]), # beta2 in Equation 5 - slope of year 
                             se_beta2 = sapply(model_SMD_s2, function(x) x$se[2]), # standard error of beta2
                             pval_beta2 = sapply(model_SMD_s2, function(x) x$pval[2]), # p value of beta2
                             beta0_c2 = sapply(model_SMD_s2, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c2 = sapply(model_SMD_s2, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_SMD_s2, function(x) x$pval[1]) # p valuer of beta0_c
                            )


#*****************************scenario 3****************************#
## beta1 has a correct direction, while beta2 has a wrong direction
beta1c_beta2w_SMD <- model_est_SMD %>% subset(model_est_SMD$beta0Tbeta1 > 0 & model_est_SMD$beta0Tbeta2 > 0) 
beta1c_beta2w_SMD$case 

## reduced model based on scenario 3 - beta1 has a correct direction, while beta2 has a wrong direction
## make a data list which only contains scenario3's data
s3_file <-  beta1c_beta2w_SMD$case
## fit ess.sei where possible
## subset of sei
s3_sei_file <- SMD_filenames[SMD_filenames %in% s3_file] # this subset should fit sei as a predictor and belong to scenario 3
## model fitting - keep beta1-related predictor (sei) and take out beta2-related predictor (year_pub.l)
model_SMD_sei_s3 <- NA
for (i in 1:length(s3_sei_file)) {
  model_SMD_sei_s3[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ sei, method = "REML", test = "t", data = SMD[s3_sei_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## replace sei by vari=
model_SMD_var_s3 <- NA
for (i in 1:length(s3_sei_file)) {
  model_SMD_var_s3[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ var, method = "REML", test = "t", data = SMD[s3_sei_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## subset of ess.sei
s3_ess.sei_file <- SMD_des_filenames[SMD_des_filenames %in% s3_file] # this subset should fit ess.sei as a predictor and belong to scenario 3
## model fitting - keep beta1-related predictor (sei) and take out beta2-related predictor (year_pub.l)
model_SMD_ess.sei_s3 <- NA
for (i in 1:length(s3_ess.sei_file)) {
  model_SMD_ess.sei_s3[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ ess.sei, method = "REML", test = "t", data = SMD[s3_ess.sei_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

# replace sei by var
model_SMD_ess.var_s3 <- NA
for (i in 1:length(s3_ess.sei_file)) {
  model_SMD_ess.var_s3[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ ess.var, method = "REML", test = "t", data = SMD[s3_ess.sei_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model model coefficients and their significance test results for 'sei' in scenario 3
model_est_SMD_sei_s3 <- data.frame(case = s3_sei_file,
                             es_type = rep("SMD", length(s3_sei_file)),
                             beta0 = model_est_SMD[model_est_SMD$case %in% s3_sei_file, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_SMD[model_est_SMD$case %in% s3_sei_file, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_SMD[model_est_SMD$case %in% s3_sei_file, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_SMD_sei_s3, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c = sapply(model_SMD_sei_s3, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_SMD_sei_s3, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_SMD_sei_s3, function(x) x$beta[2]), # beta1 in Equation 5 - slope of sampling error 
                             se_beta1 = sapply(model_SMD_sei_s3, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_SMD_sei_s3, function(x) x$pval[2]), # p value of beta1
                             beta2 = 0, # beta2 in Equation 5 - slope of year: no year term 
                             se_beta2 = 0, # standard error of beta2
                             pval_beta2 = 0, # p value of beta2
                             beta0_c2 = sapply(model_SMD_var_s3, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c2 = sapply(model_SMD_var_s3, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_SMD_var_s3, function(x) x$pval[1]) # p valuer of beta0_c
                            )

## extract model model coefficients and their significance test results for 'ess.sei' in scenario 3
model_est_SMD_ess.sei_s3 <- data.frame(case = s3_ess.sei_file,
                             es_type = rep("SMD", length(s3_ess.sei_file)),
                             beta0 = model_est_SMD[model_est_SMD$case %in% s3_ess.sei_file, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_SMD[model_est_SMD$case %in% s3_ess.sei_file, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_SMD[model_est_SMD$case %in% s3_ess.sei_file, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_SMD_ess.sei_s3, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c = sapply(model_SMD_ess.sei_s3, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_SMD_ess.sei_s3, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_SMD_ess.sei_s3, function(x) x$beta[2]), # beta1 in Equation 5 - slope of sampling error 
                             se_beta1 = sapply(model_SMD_ess.sei_s3, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_SMD_ess.sei_s3, function(x) x$pval[2]), # p value of beta1
                             beta2 = 0, # beta2 in Equation 5 - slope of year: no year term 
                             se_beta2 = 0, # standard error of beta2
                             pval_beta2 = 0, # p value of beta2
                             beta0_c2 = sapply(model_SMD_ess.var_s3, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c2 = sapply(model_SMD_ess.var_s3, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_SMD_ess.var_s3, function(x) x$pval[1]) # p valuer of beta0_c
                            )


#*****************************scenario 4****************************#
## both beta1 and beta2 has a wrong direction
beta1w_beta2w_SMD <- model_est_SMD %>% subset(model_est_SMD$beta0Tbeta1 < 0 & model_est_SMD$beta0Tbeta2 > 0) 
beta1w_beta2w_SMD$case   

## reduced model based on scenario 4 - beta1 has a wrong direction and beta2 has a wrong direction 
## this reduced model needs to take out both of the two predictors (sei and pub_year.l). This is equivalent to a null model (intercept-only model), which is used to estimate (uncorrected) meta-analytic overall mean

## make a data list which only contains scenario4's data
s4_file <-  beta1w_beta2w_SMD$case
## no need to subset sei and ess.sei because scenario4 fits a null model (without any predictor)
## model fitting - take out both beta1-related predictor (sei or ess.sei) and  beta2-related predictor (year_pub.l)
model_SMD_s4 <- NA
for (i in 1:length(s4_file)) {
  model_SMD_s4[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), method = "REML", test = "t", data = SMD[s4_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model model coefficients and their significance test results scenario 4
model_est_SMD_s4 <- data.frame(case = s4_file,
                             es_type = rep("SMD", length(s4_file)),
                             beta0 = model_est_SMD[model_est_SMD$case %in% s4_file, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean # or sapply(model_SMD_s4, function(x) x$beta[1])
                             se_beta0 = model_est_SMD[model_est_SMD$case %in% s4_file, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_SMD[model_est_SMD$case %in% s4_file, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_SMD_s4, function(x) x$beta[1]), # beta0_c in equation 1 -  bias corrected overall mean: this is equal to uncorrected overall mean (i.e., beta0)
                             se_beta0_c = sapply(model_SMD_s4, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_SMD_s4, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = 0, # beta1 in Equation 1 - slope of sampling error: no error term
                             se_beta1 = 0, # standard error of beta1
                             pval_beta1 = 0, # p value of beta1
                             beta2 = 0, # beta2 in Equation 1 - slope of year 
                             se_beta2 = 0, # standard error of beta2
                             pval_beta2 = 0, # p value of beta2
                             beta0_c2 = sapply(model_SMD_s4, function(x) x$beta[1]), # beta0_c in equation 1 -  bias corrected overall mean: this is equal to uncorrected overall mean (i.e., beta0)
                             se_beta0_c2 = sapply(model_SMD_s4, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_SMD_s4, function(x) x$pval[1]) # p valuer of beta0_c
                            )
```


### Zr
```{r}
#*************************************************************************#
#        Multilevel models to estimate bias-corrected effect
#*************************************************************************#

#*****************************scenario 1****************************#
## both beta1 and beta2 has a correct direction
beta1c_beta2c_Zr <- model_est_Zr %>% subset(model_est_Zr$beta0Tbeta1 > 0 & model_est_Zr$beta0Tbeta2 < 0) 
beta1c_beta2c_Zr$case 

## if a model slope (beta1 and beta2) has a wrong direction, we need to take out it when fitting model to estimate the bias-corrected mean
## in scenario 1, both of the two slopes have a correct direction, we use can use full model directly, no need to use take out any predictor

## full model based on scenario 1 - both beta1 and beta2 has a correct direction
## make a data list which only contains scenario1's data
s1_file <- beta1c_beta2c_Zr$case
## fit ess.sei where possible
## subset of sei
s1_file <- Zr_filenames[Zr_filenames %in% s1_file] # for Zr, do not need to subset sei and ess.sei
## model fitting - fit a full model
model_Zr_s1 <- NA
for (i in 1:length(s1_file)) {
  model_Zr_s1[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ sei + year_pub.l, method = "REML", test = "t", data = Zr[s1_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

# replace sei by var
model_Zr_var.s1 <- NA
for (i in 1:length(s1_file)) {
  model_Zr_var.s1[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ var + year_pub.l, method = "REML", test = "t", data = Zr[s1_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model coefficients and their significance test results for 'sei' in scenario 1
model_est_Zr_s1 <- data.frame(case = s1_file,
                             es_type = rep("Zr", length(s1_file)),
                             beta0 = model_est_Zr[model_est_Zr$case %in% s1_file, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_Zr[model_est_Zr$case %in% s1_file, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_Zr[model_est_Zr$case %in% s1_file, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_Zr_s1, function(x) x$beta[1]), # beta0_c in equation 6 -  bias corrected overall mean
                             se_beta0_c = sapply(model_Zr_s1, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_Zr_s1, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_Zr_s1, function(x) x$beta[2]), # beta1 in Equation 6 - slope of sampling error 
                             se_beta1 = sapply(model_Zr_s1, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_Zr_s1, function(x) x$pval[2]), # p value of beta1
                             beta2 = sapply(model_Zr_s1, function(x) x$beta[3]), # beta2 in Equation 6 - slope of year 
                             se_beta2 = sapply(model_Zr_s1, function(x) x$se[3]), # standard error of beta2
                             pval_beta2 = sapply(model_Zr_s1, function(x) x$pval[3]), # p value of beta2
                             beta0_c2 = sapply(model_Zr_var.s1, function(x) x$beta[1]), # beta0_c in equation 6 -  bias corrected overall mean
                             se_beta0_c2 = sapply(model_Zr_var.s1, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_Zr_var.s1, function(x) x$pval[1]) # p valuer of beta0_c
                            )


#*****************************scenario 2****************************#
## beta1 has a wrong direction, while beta2 has a correct direction
beta1w_beta2c_Zr <- model_est_Zr %>% subset(model_est_Zr$beta0Tbeta1 < 0 & model_est_Zr$beta0Tbeta2 < 0) 
beta1w_beta2c_Zr$case 

## reduced model based on scenario 2 - beta1 has a wrong direction, while beta2 has a correct direction
## make a data list which only contains scenario2's data
s2_file <-  beta1w_beta2c_Zr$case
## model fitting -  take out beta1-related predictor (sei or esss.sei) and keep beta2-related predictor (year_pub.l)
## beta1-related predictor is removed, so no need to fit ess.sei where possible
model_Zr_s2 <- NA
for (i in 1:length(s2_file)) {
  model_Zr_s2[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ year_pub.l, method = "REML", test = "t", data = Zr[s2_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model model coefficients and their significance test results scenario 2
model_est_Zr_s2 <- data.frame(case = s2_file,
                             es_type = rep("Zr", length(s2_file)),
                             beta0 = model_est_Zr[model_est_Zr$case %in% s2_file, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_Zr[model_est_Zr$case %in% s2_file, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_Zr[model_est_Zr$case %in% s2_file, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_Zr_s2, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c = sapply(model_Zr_s2, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_Zr_s2, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = 0, # beta1 in Equation 5 - slope of sampling error: no error term
                             se_beta1 = 0, # standard error of beta1
                             pval_beta1 = 0, # p value of beta1
                             beta2 = sapply(model_Zr_s2, function(x) x$beta[2]), # beta2 in Equation 5 - slope of year 
                             se_beta2 = sapply(model_Zr_s2, function(x) x$se[2]), # standard error of beta2
                             pval_beta2 = sapply(model_Zr_s2, function(x) x$pval[2]), # p value of beta2
                             beta0_c2 = sapply(model_Zr_s2, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c2 = sapply(model_Zr_s2, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_Zr_s2, function(x) x$pval[1]) # p valuer of beta0_c
                            )


#*****************************scenario 3****************************#
## beta1 has a correct direction, while beta2 has a wrong direction
beta1c_beta2w_Zr <- model_est_Zr %>% subset(model_est_Zr$beta0Tbeta1 > 0 & model_est_Zr$beta0Tbeta2 > 0) 
beta1c_beta2w_Zr$case 

## reduced model based on scenario 3 - beta1 has a correct direction, while beta2 has a wrong direction
## make a data list which only contains scenario3's data
s3_file <- beta1c_beta2w_Zr$case
## Zr does not need to subset sei and ess.sei
s3_file <- Zr_filenames[Zr_filenames %in% s3_file] 
## model fitting - keep beta1-related predictor (sei) and take out beta2-related predictor (year_pub.l)
model_Zr_s3 <- NA
for (i in 1:length(s3_file)) {
  model_Zr_s3[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ sei, method = "REML", test = "t", data = Zr[s3_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

# replace sei by var
model_Zr_var.s3 <- NA
for (i in 1:length(s3_file)) {
  model_Zr_var.s3[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ var, method = "REML", test = "t", data = Zr[s3_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model coefficients and their significance test results for 'sei' in scenario 3
model_est_Zr_s3 <- data.frame(case = s3_file,
                             es_type = rep("Zr", length(s3_file)),
                             beta0 = model_est_Zr[model_est_Zr$case %in% s3_file, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_Zr[model_est_Zr$case %in% s3_file, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_Zr[model_est_Zr$case %in% s3_file, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_Zr_s3, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c = sapply(model_Zr_s3, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_Zr_s3, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_Zr_s3, function(x) x$beta[2]), # beta1 in Equation 5 - slope of sampling error 
                             se_beta1 = sapply(model_Zr_s3, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_Zr_s3, function(x) x$pval[2]), # p value of beta1
                             beta2 = 0, # beta2 in Equation 5 - slope of year: no year term 
                             se_beta2 = 0, # standard error of beta2
                             pval_beta2 = 0, # p value of beta2
                             beta0_c2 = sapply(model_Zr_var.s3, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c2 = sapply(model_Zr_var.s3, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_Zr_var.s3, function(x) x$pval[1]) # p valuer of beta0_c
                            )



#*****************************scenario 4****************************#
## both beta1 and beta2 has a wrong direction
beta1w_beta2w_Zr <- model_est_Zr %>% subset(model_est_Zr$beta0Tbeta1 < 0 & model_est_Zr$beta0Tbeta2 > 0) 
beta1w_beta2w_Zr$case 

## reduced model based on scenario 4 - beta1 has a wrong direction and beta2 has a wrong direction 
## this reduced model needs to take out both of the two predictors (sei and pub_year.l). This is equivalent to a null model (intercept-only model), which is used to estimate (uncorrected) meta-analytic overall mean

## make a data list which only contains scenario4's data
s4_file <-  beta1w_beta2w_Zr$case
## no need to subset sei and ess.sei because scenario4 fits a null model (without any predictor)
## model fitting - take out both beta1-related predictor (sei or ess.sei) and  beta2-related predictor (year_pub.l)
model_Zr_s4 <- NA
for (i in 1:length(s4_file)) {
  model_Zr_s4[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), method = "REML", test = "t", data = Zr[s4_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model coefficients and their significance test results for scenario 4
model_est_Zr_s4 <- data.frame(case = s4_file,
                             es_type = rep("Zr", length(s4_file)),
                             beta0 = model_est_Zr[model_est_Zr$case %in% s4_file, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean # or sapply(model_Zr_s4, function(x) x$beta[1])
                             se_beta0 = model_est_Zr[model_est_Zr$case %in% s4_file, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_Zr[model_est_Zr$case %in% s4_file, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_Zr_s4, function(x) x$beta[1]), # beta0_c in equation 1 -  bias corrected overall mean: this is equal to uncorrected overall mean (i.e., beta0)
                             se_beta0_c = sapply(model_Zr_s4, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_Zr_s4, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = 0, # beta1 in Equation 1 - slope of sampling error: no error term
                             se_beta1 = 0, # standard error of beta1
                             pval_beta1 = 0, # p value of beta1
                             beta2 = 0, # beta2 in Equation 1 - slope of year 
                             se_beta2 = 0, # standard error of beta2
                             pval_beta2 = 0, # p value of beta2
                             beta0_c2 = sapply(model_Zr_s4, function(x) x$beta[1]), # beta0_c in equation 1 -  bias corrected overall mean: this is equal to uncorrected overall mean (i.e., beta0)
                             se_beta0_c2 = sapply(model_Zr_s4, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_Zr_s4, function(x) x$pval[1]) # p valuer of beta0_c
                            )
```


### Contrust bias-corrected dataframe

Compare the absolute values of beta0 and beta0_c2 to decide whether the publication bias is correctly adjusted. The principle is that when the bias-corrected meta-analytic overall mean (beta0_c2) is smaller than the original meta-analytic overall mean (beta0), the publication bias is correctly adjusted. Otherwise, the publication bias is not correctly adjusted because the slope of sampling variance (beta1) and publication year (beta2) are not in a expected directionality (which is caused by the un-accounted heterogeneity).

```{r}
# combine reduced model estimates from lnRR, SMD, and Zr
model_est_all_corrected_original <- rbind(model_est_lnRR_sei_s1,
                                          model_est_lnRR_ess.sei_s1,
                                          model_est_lnRR_s2,
                                          model_est_lnRR_sei_s3,
                                          model_est_lnRR_ess.sei_s3,
                                          model_est_lnRR_s4,
                                          model_est_SMD_sei_s1,
                                          model_est_SMD_ess.sei_s1,
                                          model_est_SMD_s2,
                                          model_est_SMD_sei_s3,
                                          model_est_SMD_ess.sei_s3,
                                          model_est_SMD_s4,
                                          model_est_Zr_s1,
                                          model_est_Zr_s2,
                                          model_est_Zr_s3,
                                          model_est_Zr_s4)

# create a variable to decide whether beta0 is larger than beta0_c2
## model_est_all_corrected_scaled %>% mutate(beta0Mbeta0_c2 = abs(beta0) - abs(beta0_c2))
model_est_all_corrected_original$beta0Mbeta0_c2 <- abs(model_est_all_corrected_original$beta0) - abs(model_est_all_corrected_original$beta0_c2)

# create a variable to contain the 'real' bias-corrected meta-analytic mean
model_est_all_corrected_original$beta0_c3 <- ifelse(model_est_all_corrected_original$beta0Mbeta0_c2 > 0, model_est_all_corrected_original$beta0_c2, model_est_all_corrected_original$beta0)

```



# Meta-meta-analysis of (original) beta0

we use random-effect meta-analytic models to aggregate the (original) beta0 to see the overall magnitude of these meta-analyses.

```{r}
# combine model estimates from lnRR, SMD, and Zr
model_est_all_original <- rbind(model_est_lnRR, model_est_SMD, model_est_Zr) 
# add unique identifier to each meta-analysis paper
model_est_all_original$MA <- c("ft053","ft053","ft053","ft077","ft084","ft089","ft125","ft127","ft181","ft181","ft027","ft030","ft078","ft095","ft095","ft095","ft109","ft144","ft155","ft166","ft056","ft056","ft068","ft096","ft096","ft101","ft107","ft107","ft112","ft117","ft126","ft133","ft134","ft140","ft140","ft151","ft167","ft179","ft037","ft037","ft037","ft037","ft040","ft045","ft067","ft087","ft092","ft093","ft098","ft105","ft115","ft120","ft139","ft143","ft173","ft174","ft003","ft012","ft017","ft017","ft018","ft025","ft028","ft028","ft028","ft028","ft028","ft044","ft047","ft054","ft061","ft061","ft061","ft061","ft061","ft061","ft061","ft065","ft081","ft081","ft082","ft091","ft122","ft135",  
"ft138","ft177","ft187")


# get folded mean and variance
model_est_all_original$beta0_folded <- folded_es(mean = model_est_all_original$beta0, variance = model_est_all_original$se_beta0^2)

model_est_all_original$beta0_var_folded <- folded_error(mean = model_est_all_original$beta0, variance = model_est_all_original$se_beta0^2)


# overall magnitude of beta0
MMA_beta0_all_original <- rma.mv(yi = beta0_folded, V = beta0_var_folded, random = list(~1 | MA, ~1 | case), method = "REML", data = model_est_all_original) 


# orchard for overall decline in effect size magnitude
png(filename = "./orchard_MMA_beta0_all_original.png", width = 6, height = 3.5, units = "in", res = 400, type = "windows")
orchard_MMA_beta0_all_original <- orchard_plot(MMA_beta0_all_original, 
             data = model_est_all_original,
             mod = "1", group = "case", 
             k = FALSE, g = FALSE,
             trunk.size = 1, branch.size = 1, twig.size = 0.5,
             xlab = "Overall mean",
             transfm = "none", 
             angle = 90) +
scale_x_discrete(labels = c("Overall magnitude of the meta-analytic mean")) + 
scale_fill_manual(values = c("#88CCEE")) +
scale_color_manual(values = c("#88CCEE")) +
theme(axis.text.x = element_text(size = 10, colour = "black"),
      axis.text.y = element_text(size = 10, colour = "black"),
      axis.title.x = element_text(size = 10, colour = "black"),
      plot.title = element_text(size = 10, colour = "black"))
orchard_MMA_beta0_all_original
dev.off()



# overall decline in different effect size measures
# reorder the level of effect size types
model_est_all_original$es_type <- factor(model_est_all_original$es_type, levels = c("Zr", "SMD", "lnRR"))

MMA_beta0_all_original_es_type <- rma.mv(yi = beta0_folded, V = beta0_var_folded, random = list(~1 | MA, ~1 | case), mods = ~ es_type - 1, method = "REML", test = "t", data = model_est_all_original)

# orchard for overall decline in effect size magnitude for effect size measures
png(filename = "./orchard_MMA_beta0_all_original_es_type.png", width = 6, height = 3.5, units = "in", res = 400, type = "windows")
orchard_MMA_beta0_all_original_es_type <- orchard_plot(MMA_beta0_all_original_es_type, 
             data = model_est_all_original,
             mod = "es_type", group = "case", 
             k = FALSE, g = FALSE,
             trunk.size = 1, branch.size = 1, twig.size = 0.5,
             xlab = "Overall mean",
             transfm = "none", 
             angle = 90) +
scale_x_discrete(labels = c("Zr", "SMD", "lnRR")) + 
scale_fill_manual(values = c("#CC6677", "#DDCC77", "#117733")) +
scale_color_manual(values = c("#CC6677", "#DDCC77", "#117733")) +
theme(axis.text.x = element_text(size = 10, colour = "black"),
      axis.text.y = element_text(size = 10, colour = "black"),
      axis.title.x = element_text(size = 10, colour = "black"),
      plot.title = element_text(size = 10, colour = "black"))
orchard_MMA_beta0_all_original_es_type
dev.off()


# assemble panels
png(filename = "./orchard_MMA_beta0_original_assemble.png", width = 5, height = 5, units = "in", res = 400, type = "windows")
plot_grid(orchard_MMA_beta0_all_original, orchard_MMA_beta0_all_original_es_type, labels = c("A","B"), nrow = 2)
dev.off()



# paired plot
dummy <- rep("Dummy", nrow(model_est_all_scaled))
wide.data <- 
  tibble::tibble(
    `Overall mean` = abs(model_est_all_scaled$beta0),
    `Bias-corrected mean` = abs(model_est_all_scaled$beta0_c),
     Dummy = dummy, ID = 1:length(dummy), case = model_est_all_scaled$case,es_type = model_est_all_scaled$es_type)

## find correct direction of slope
include.point <- which((wide.data$`Overall mean` - wide.data$`Bias-corrected mean`)>0)
## table only includes correct data
wide.data2 <- wide.data[include.point,]


# use ggpaired() in ggpubr to show the pairwise comparisons 
## lnRR
png(filename = "./paired.plot_lnRR.png", width = 5, height = 5, units = "in", type = "windows", res = 400)
paired.plot_lnRR <- ggpaired(data = wide.data2 %>% filter(es_type=="lnRR"), cond1 = "Overall mean", cond2 = "Bias-corrected mean",fill = "condition", palette = "jco") + 
  scale_y_continuous(limits = c(0,1)) +
  scale_fill_manual(values = c("#E69F00", "#56B4E9")) + 
  guides(fill = "none") + labs(x = "", y = "Effect size estimate of lnRR")
paired.plot_lnRR
dev.off()

## SMD
png(filename = "./paired.plot_SMD.png", width = 5, height = 5, units = "in", type = "windows", res = 400)
paired.plot_SMD <- ggpaired(data = wide.data2 %>% filter(es_type=="SMD"), cond1 = "Overall mean", cond2 = "Bias-corrected mean",fill = "condition", palette = "jco") + 
  scale_y_continuous(limits = c(0,1)) +
  scale_fill_manual(values = c("#009E73", "#CC79A7")) + 
  guides(fill = "none") + labs(x = "", y = "Effect size estimate of SMD")
paired.plot_SMD
dev.off()

## Zr
png(filename = "./paired.plot_Zr.png", width = 5, height = 5, units = "in", type = "windows", res = 400)
paired.plot_Zr <- ggpaired(data = wide.data2 %>% filter(es_type=="Zr"), cond1 = "Overall mean", cond2 = "Bias-corrected mean",fill = "condition", palette = "jco") + 
  scale_y_continuous(limits = c(0,1)) +
  scale_fill_manual(values = c("#0072B2", "#D55E00")) + 
  guides(fill = "none") + labs(x = "", y = "Effect size estimate of Zr")
paired.plot_Zr
dev.off()

## put together
png(filename = "./paired.plot3.png", width = 4, height = 8, units = "in", type = "windows", res = 400)
plot_grid(paired.plot_lnRR,paired.plot_SMD,paired.plot_Zr,ncol = 1)
dev.off()

```


# Meta-science

### lnRR
#### Meta-analysis level
```{r}
#****************************************************************#
#-------------------------------lnRR-----------------------------#
#****************************************************************#

#-----------------------------------------------------------#
#          (1) two-tailed power for meta-analyses
#-----------------------------------------------------------#
# power to detect meta-analytic overall mean
model_est_lnRR$MA.power <- power.ma_Shinichi(mu=model_est_lnRR$beta0,SE=model_est_lnRR$se_beta0)

# power to detect bias-corrected meta-analytic overall mean
## add bias-corrected mean to model_est_lnRR
beta0_c3_lnRR <- (model_est_all_corrected_original %>% filter(es_type == "lnRR")) %>% select(case,beta0_c3)

model_est_lnRR <- left_join(model_est_lnRR,beta0_c3_lnRR,by="case") # reorder rows according to vector of case

## calculate bias-corrected power
model_est_lnRR$MA.power_c <- power.ma_Shinichi(mu=model_est_lnRR$beta0_c3,SE=model_est_lnRR$se_beta0) # note to still use the unconditional se rather than the conditional se (se of beta0c_3)

# power to detect small-study effect
model_est_lnRR$sse.power <- power.ma_Shinichi(mu=model_est_lnRR$beta1,SE=model_est_lnRR$se_beta1)

# power to detect decline effect
model_est_lnRR$de.power <- power.ma_Shinichi(mu=model_est_lnRR$beta2,SE=model_est_lnRR$se_beta2)

#-----------------------------------------------------------#
#            (2) type S error for meta-analyses
#-----------------------------------------------------------#
# meta-analytic overall mean
MA.power.S <- NA
for (i in 1:length(model_est_lnRR$case)) {
  MA.power.S[i] <- error_S(mu=model_est_lnRR$beta0[i],se=model_est_lnRR$se_beta0[i],alpha=0.05) %>% unlist()
}

model_est_lnRR$MA.power.S <- MA.power.S

# bias-corrected version
MA.power.S_c <- NA
for (i in 1:length(model_est_lnRR$case)) {
  MA.power.S_c[i] <- error_S(mu=model_est_lnRR$beta0_c3[i],se=model_est_lnRR$se_beta0[i],alpha=0.05) %>% unlist()
}

model_est_lnRR$MA.power.S_c <- MA.power.S_c


#-----------------------------------------------------------#
#   (3) type M error (overestimate ratio) for meta-analyses
#-----------------------------------------------------------#
# meta-analytic overall mean
MA.power.M <- NA
for (i in 1:length(model_est_lnRR$case)) {
  MA.power.M[i] <- error_M(mu=model_est_lnRR$beta0[i],se=model_est_lnRR$se_beta0[i],alpha=0.05,N=10000) %>% unlist()
}

model_est_lnRR$MA.power.M <- MA.power.M

# bias-corrected version
MA.power.M_c <- NA
for (i in 1:length(model_est_lnRR$case)) {
  MA.power.M_c[i] <- error_M(mu=model_est_lnRR$beta0_c3[i],se=model_est_lnRR$se_beta0[i],alpha=0.05,N=10000) %>% unlist()
}

model_est_lnRR$MA.power.M_c <- MA.power.M_c
```


#### Single experiment level

```{r}
#***************************************************************#
#        power for single experiments within meta-analysis       #
#***************************************************************#


#****************************************************************#
#------------------------------lnRR-----------------------------#
#****************************************************************#

#--------------------- (1) two tailed power ---------------------#
# meta-analytic overall mean
# lnRR
power_lnRR <- NA
for (i in 1:length(lnRR)) {
  power_lnRR[i] <- power.individual_Shinichi(mu=model_est_lnRR$beta0[i], se=lnRR[[i]]$sei) %>% list()}

# allocate each set of power into corresponding dataset
for (i in 1:length(power_lnRR)) {
  lnRR[[i]]$power_lnRR <- power_lnRR[[i]]
}


# bias-corrected version
# lnRR
power_c_lnRR <- NA
for (i in 1:length(lnRR)) {
  power_c_lnRR[i] <- power.individual_Shinichi(mu=model_est_lnRR$beta0_c3[i], se=lnRR[[i]]$sei) %>% list()}

# allocate each set of power into corresponding dataset
for (i in 1:length(power_lnRR)) {
  lnRR[[i]]$power_c_lnRR <- power_c_lnRR[[i]]
}

#---------------------- (2) type S error -----------------------#
# meta-analytic overall mean
power.S_lnRR <- NA
for (i in 1:length(lnRR)) {
  power.S_lnRR[i] <- mapply(error_S,mu=model_est_lnRR$beta0[i], se=lnRR[[i]]$sei) %>% list()}

# allocate each set of type S error into corresponding dataset
for (i in 1:length(power.S_lnRR)) {
  lnRR[[i]]$power.S_lnRR <- power.S_lnRR[[i]]
}


# bias-corrected version
power.S_c_lnRR <- NA
for (i in 1:length(lnRR)) {
  power.S_c_lnRR[i] <- mapply(error_S,mu=model_est_lnRR$beta0_c3[i], se=lnRR[[i]]$sei) %>% list()}

# allocate each set of type S error into corresponding dataset
for (i in 1:length(power.S_lnRR)) {
  lnRR[[i]]$power.S_c_lnRR <- power.S_c_lnRR[[i]]
}

#---------------- (3) type M error (overestimate ratio) --------------#
# meta-analytic overall mean
power.M_lnRR <- NA
for (i in 1:length(lnRR)) {
  power.M_lnRR[i] <- mapply(error_M,mu=model_est_lnRR$beta0[i], se=lnRR[[i]]$sei) %>% list()}

# allocate each set of type S error into corresponding dataset
for (i in 1:length(power.M_lnRR)) {
  lnRR[[i]]$power.M_lnRR <- power.M_lnRR[[i]]
}


# bias-corrected version
power.M_c_lnRR <- NA
for (i in 1:length(lnRR)) {
  power.M_c_lnRR[i] <- mapply(error_M,mu=model_est_lnRR$beta0_c3[i], se=lnRR[[i]]$sei) %>% list()}

# allocate each set of type S error into corresponding dataset
for (i in 1:length(power.M_lnRR)) {
  lnRR[[i]]$power.M_c_lnRR <- power.M_c_lnRR[[i]]
}


#*********************************************************************#
#------------------- summary of experimental power --------------------#
#*********************************************************************#

#****************************************************************#
#-----------------------------lnRR------------------------------#
#****************************************************************#

#--------------------- (1) two tailed power ---------------------#
# meta-analytic overall mean
power_summary_lnRR <- data.frame(case=names(lnRR),
                   Minimum=mapply(summary, sapply(lnRR, function(x) x$power_lnRR))[1,],      
                   `First quarter`=mapply(summary, sapply(lnRR, function(x) x$power_lnRR))[2,],
                   Median=mapply(summary, sapply(lnRR, function(x) x$power_lnRR))[3,],
                   Mean=mapply(summary, sapply(lnRR, function(x) x$power_lnRR))[4,], 
                   `Third quarter`=mapply(summary, sapply(lnRR, function(x) x$power_lnRR))[5,], 
                   Maximum=mapply(summary, sapply(lnRR, function(x) x$power_lnRR))[6,]) 

# bias-corrected version
power_c_summary_lnRR <- data.frame(case=names(lnRR),
                   Minimum=mapply(summary, sapply(lnRR, function(x) x$power_c_lnRR))[1,],      
                   `First quarter`=mapply(summary, sapply(lnRR, function(x) x$power_c_lnRR))[2,],
                   Median=mapply(summary, sapply(lnRR, function(x) x$power_c_lnRR))[3,],
                   Mean=mapply(summary, sapply(lnRR, function(x) x$power_c_lnRR))[4,], 
                   `Third quarter`=mapply(summary, sapply(lnRR, function(x) x$power_c_lnRR))[5,], 
                   Maximum=mapply(summary, sapply(lnRR, function(x) x$power_c_lnRR))[6,]) 

#---------------------- (2) type S error -----------------------#
# meta-analytic overall mean
power.S_summary_lnRR <-  data.frame(case=names(lnRR),
                   Minimum=mapply(summary, sapply(lnRR, function(x) x$power.S_lnRR))[1,],      
                   `First quarter`=mapply(summary, sapply(lnRR, function(x) x$power.S_lnRR))[2,],
                   Median=mapply(summary, sapply(lnRR, function(x) x$power.S_lnRR))[3,],
                   Mean=mapply(summary, sapply(lnRR, function(x) x$power.S_lnRR))[4,], 
                   `Third quarter`=mapply(summary, sapply(lnRR, function(x) x$power.S_lnRR))[5,], 
                   Maximum=mapply(summary, sapply(lnRR, function(x) x$power.S_lnRR))[6,]) 

# bias-corrected version
power.S_c_summary_lnRR <-  data.frame(case=names(lnRR),
                   Minimum=mapply(summary, sapply(lnRR, function(x) x$power.S_c_lnRR))[1,],      
                   `First quarter`=mapply(summary, sapply(lnRR, function(x) x$power.S_c_lnRR))[2,],
                   Median=mapply(summary, sapply(lnRR, function(x) x$power.S_c_lnRR))[3,],
                   Mean=mapply(summary, sapply(lnRR, function(x) x$power.S_c_lnRR))[4,], 
                   `Third quarter`=mapply(summary, sapply(lnRR, function(x) x$power.S_c_lnRR))[5,], 
                   Maximum=mapply(summary, sapply(lnRR, function(x) x$power.S_c_lnRR))[6,]) 

#-------------- (3) type M error (overestimate ratio) -------------#
# meta-analytic overall mean
power.M_summary_lnRR <-  data.frame(case=names(lnRR),
                   Minimum=mapply(summary, sapply(lnRR, function(x) x$power.M_lnRR))[1,],      
                   `First quarter`=mapply(summary, sapply(lnRR, function(x) x$power.M_lnRR))[2,],
                   Median=mapply(summary, sapply(lnRR, function(x) x$power.M_lnRR))[3,],
                   Mean=mapply(summary, sapply(lnRR, function(x) x$power.M_lnRR))[4,], 
                   `Third quarter`=mapply(summary, sapply(lnRR, function(x) x$power.M_lnRR))[5,], 
                   Maximum=mapply(summary, sapply(lnRR, function(x) x$power.M_lnRR))[6,]) 

# bias-corrected version
power.M_c_summary_lnRR <-  data.frame(case=names(lnRR),
                   Minimum=mapply(summary, sapply(lnRR, function(x) x$power.M_c_lnRR))[1,],      
                   `First quarter`=mapply(summary, sapply(lnRR, function(x) x$power.M_c_lnRR))[2,],
                   Median=mapply(summary, sapply(lnRR, function(x) x$power.M_c_lnRR))[3,],
                   Mean=mapply(summary, sapply(lnRR, function(x) x$power.M_c_lnRR))[4,], 
                   `Third quarter`=mapply(summary, sapply(lnRR, function(x) x$power.M_c_lnRR))[5,], 
                   Maximum=mapply(summary, sapply(lnRR, function(x) x$power.M_c_lnRR))[6,]) 
```

#### Aggregation

This section is used to obtain overall estimates of the three parameters across different meta-analyses (which provided us with comparable summaries of the three parameters).

We used weighted regression to statistically aggregate over the three parameters obtained at the within-meta-analysis level whereas we used mixed effects models to aggregate these parameters at the experiment level. Both procedures involved aggregating the parameters across meta-analyses (i.e., between-meta-analysis modelling). 
```{r}
#***************************************************************#
#      estimate overall power for meta-analysis level power     #
#***************************************************************#

# add N and k
N_lnRR <- NA
for (i in 1:length(lnRR)) {
  N_lnRR[i] <- lnRR[[i]]$study_ID %>% unique() %>% length()
}
model_est_lnRR$N <- N_lnRR

k_lnRR <- NA
for (i in 1:length(lnRR)) {
  k_lnRR[i] <- lnRR[[i]]$obs_ID %>% length()
}
model_est_lnRR$k <- k_lnRR

#--------------------- (1) two tailed power ---------------------#
# meta-analytic overall mean
# log
MMA_MA.power_lnRR <- lm(log(MA.power) ~ 1, weights = k, data = model_est_lnRR)
# original scale
MMA_MA.power_lnRR2 <- lm(MA.power ~ 1, weights = k, data = model_est_lnRR)
MMA_MA.power_lnRR2$coefficients

# this is median
MMA_MA.power_lnRR$coefficients  %>% exp() 
# this is mean
(MMA_MA.power_lnRR$coefficients + 0.5*var(log(model_est_lnRR$MA.power))) %>% exp() 
#confidence interval of median
confint(MMA_MA.power_lnRR) %>% exp()

# compare residuals
par(mfrow = c(1, 2))
residuals(MMA_MA.power_lnRR) %>% hist(main = paste("log power"), xlab = "Residual")
residuals(MMA_MA.power_lnRR2) %>% hist(main = paste("original power"), xlab = "Residual") 

# bias-corrected version
# log
MMA_MA.power_c_lnRR <- lm(log(MA.power_c) ~ 1, weights = k, data = model_est_lnRR)

# this is median
MMA_MA.power_c_lnRR$coefficients  %>% exp() 
# this is mean
(MMA_MA.power_c_lnRR$coefficients + 0.5*var(log(model_est_lnRR$MA.power))) %>% exp() 
#confidence interval of median
confint(MMA_MA.power_c_lnRR) %>% exp()


#---------------------- (2) type S error -----------------------#
# meta-analytic overall mean
# log
MMA_MA.power.S_lnRR <- lm(log(MA.power.S+0.025) ~ 1, weights = k, data = model_est_lnRR) # add an offset of 0.025(25%) to avoid ln(0) = infinity 
# this is median
MMA_MA.power.S_lnRR$coefficients %>% exp() - 0.025
# this is mean
(MMA_MA.power.S_lnRR$coefficients + 0.5*var(log(model_est_lnRR$MA.power.S + 0.025))) %>% exp() - 0.025
#confidence interval of median
confint(MMA_MA.power.S_lnRR) %>% exp() - 0.025 # if the lower boundary was negative (probably caused by the negative variance), we used 0 to replace it

# bias-corrected version
# log
MMA_MA.power.S_c_lnRR <- lm(log(MA.power.S_c+0.025) ~ 1, weights = k, data = model_est_lnRR) # add an offset of 0.025(25%) to avoid ln(0) = infinity 
# this is median
MMA_MA.power.S_c_lnRR$coefficients %>% exp() - 0.025
# this is mean
(MMA_MA.power.S_c_lnRR$coefficients + 0.5*var(log(model_est_lnRR$MA.power.S_c + 0.025))) %>% exp() - 0.025
#confidence interval of median
confint(MMA_MA.power.S_c_lnRR) %>% exp() - 0.025 # if the lower boundary was negative (probably caused by the negative variance), we used 0 to replace it


#-------------- (3) type M error (overestimate ratio) -------------#
# meta-analytic overall mean
# log
MMA_MA.power.M_lnRR <- lm(log(MA.power.M) ~ 1, weights = k, data = model_est_lnRR)
# this is median
MMA_MA.power.M_lnRR$coefficients %>% exp() 
# this is mean
(MMA_MA.power.M_lnRR$coefficients + 0.5*var(log(model_est_lnRR$MA.power.M))) %>% exp() 

#confidence interval of median
confint(MMA_MA.power.M_lnRR) %>% exp()


# bias-corrected version
# log
MMA_MA.power.M_c_lnRR <- lm(log(MA.power.M_c) ~ 1, weights = k, data = model_est_lnRR)
# this is median
MMA_MA.power.M_c_lnRR$coefficients %>% exp() 
# this is mean
(MMA_MA.power.M_c_lnRR$coefficients + 0.5*var(log(model_est_lnRR$MA.power.M_c))) %>% exp() 

#confidence interval of median
confint(MMA_MA.power.M_c_lnRR) %>% exp()

#***************************************************************#
#      estimate overall power for experimental level power     #
#***************************************************************#

#****************************************************************#
#-----------------------------lnRR------------------------------#
#****************************************************************#


study_ID_lnRR <- sapply(lnRR, function(x) x$study_ID) %>% unlist()
power_lnRR <- sapply(lnRR, function(x) x$power_lnRR) %>% unlist()
power.S_lnRR <- sapply(lnRR, function(x) x$power.S_lnRR) %>% unlist()
power.M_lnRR <- sapply(lnRR, function(x) x$power.M_lnRR) %>% unlist()
power_c_lnRR <- sapply(lnRR, function(x) x$power_c_lnRR) %>% unlist()
power.S_c_lnRR <- sapply(lnRR, function(x) x$power.S_c_lnRR) %>% unlist()
power.M_c_lnRR <- sapply(lnRR, function(x) x$power.M_c_lnRR) %>% unlist()

individual_est_lnRR <- data.frame("study_ID_lnRR" = study_ID_lnRR,
                                 "power_lnRR" = power_lnRR,
                                 "power.S_lnRR" = power.S_lnRR,
                                 "power.M_lnRR" = power.M_lnRR,
                                 "power_c_lnRR" = power_c_lnRR,
                                 "power.S_c_lnRR" = power.S_c_lnRR,
                                 "power.M_c_lnRR" = power.M_c_lnRR
                                  )

#--------------------- (1) two tailed power ---------------------#
# meta-analytic overall mean
# log
MMA_EXP.power_lnRR <- lmer(log(power_lnRR) ~ 1 + (1 | study_ID_lnRR), data = individual_est_lnRR)
# original scale
MMA_EXP.power_lnRR2 <- lmer(power_lnRR ~ 1 + (1 | study_ID_lnRR), data = individual_est_lnRR)
summary(MMA_EXP.power_lnRR2)$coefficients[1]

# this is median 
summary(MMA_EXP.power_lnRR)$coefficients[1] %>% exp()
# this is mean
(summary(MMA_EXP.power_lnRR)$coefficients[1] + 0.5*var(log(individual_est_lnRR$power_lnRR))) %>% exp()
# confidence interval of median
confint(MMA_EXP.power_lnRR) %>% exp()

# compare residual
par(mfrow = c(1, 2))
residuals(MMA_EXP.power_lnRR) %>% hist(main = paste("log power"), xlab = "Residual")
residuals(MMA_EXP.power_lnRR2) %>% hist(main = paste("orignal power"), xlab = "Residual")

# bias-corrected version
# log
MMA_EXP.power_c_lnRR <- lmer(log(power_c_lnRR) ~ 1 + (1 | study_ID_lnRR), data = individual_est_lnRR)

# this is median 
summary(MMA_EXP.power_c_lnRR)$coefficients[1] %>% exp()
# this is mean
(summary(MMA_EXP.power_c_lnRR)$coefficients[1] + 0.5*var(log(individual_est_lnRR$power_lnRR))) %>% exp()
# confidence interval of median
confint(MMA_EXP.power_c_lnRR) %>% exp()


#---------------------- (2) type S error -----------------------#
# meta-analytic overall mean
# log
MMA_EXP.power.S_lnRR <- lmer( log(power.S_lnRR + 0.025) ~ 1 + (1 | study_ID_lnRR), data = individual_est_lnRR) # add an offset of 0.025 to avoid log(0) = inf
# this is median 
summary(MMA_EXP.power.S_lnRR)$coefficients[1] %>% exp() - 0.025 # - 0.025 is important
# this is mean
(summary(MMA_EXP.power.S_lnRR)$coefficients[1] + 
                0.5*(summary(MMA_EXP.power.S_lnRR)$varcor[[1]][[1]] + # sigma^2 for study level
                     summary(MMA_EXP.power.S_lnRR)$sigma^2) # residual level - we cannot do like what we did above as we added 0.025
        ) %>% exp() - 0.025

#confidence interval of median
confint(MMA_EXP.power.S_lnRR) %>% exp() - 0.025 # if the lower boundary was negative (probably caused by the negative variance), we used 0 to replace it

# bias-corrected version
# log
MMA_EXP.power.S_c_lnRR <- lmer( log(power.S_c_lnRR + 0.025) ~ 1 + (1 | study_ID_lnRR), data = individual_est_lnRR) # add an offset of 0.025 to avoid log(0) = inf
# this is median 
summary(MMA_EXP.power.S_c_lnRR)$coefficients[1] %>% exp() - 0.025 # - 0.025 is important
# this is mean
(summary(MMA_EXP.power.S_c_lnRR)$coefficients[1] + 
                0.5*(summary(MMA_EXP.power.S_lnRR)$varcor[[1]][[1]] + # sigma^2 for study level
                     summary(MMA_EXP.power.S_lnRR)$sigma^2) # residual level - we cannot do like what we did above as we added 0.025
        ) %>% exp() - 0.025

#confidence interval of median
confint(MMA_EXP.power.S_c_lnRR) %>% exp() - 0.025 # if the lower boundary was negative (probably caused by the negative variance), we used 0 to replace it

#--------------------- (3) type M error ---------------------#
# log
MMA_EXP.power.M_lnRR <- lmer(log(power.M_lnRR) ~ 1 + (1 | study_ID_lnRR), data = individual_est_lnRR)

# this is median 
summary(MMA_EXP.power.M_lnRR)$coefficients[1] %>% exp()
# this is mean
(summary(MMA_EXP.power.M_lnRR)$coefficients[1] + 0.5*var(log(individual_est_lnRR$power.M_lnRR))) %>% exp()
# confidence interval of median
confint(MMA_EXP.power.M_lnRR) %>% exp()

# bias-corrected version
# log
MMA_EXP.power.M_c_lnRR <- lmer(log(power.M_c_lnRR) ~ 1 + (1 | study_ID_lnRR), data = individual_est_lnRR)

# this is median 
summary(MMA_EXP.power.M_c_lnRR)$coefficients[1] %>% exp()
# this is mean
(summary(MMA_EXP.power.M_c_lnRR)$coefficients[1] + 0.5*var(log(individual_est_lnRR$power.M_lnRR))) %>% exp()
# confidence interval of median
confint(MMA_EXP.power.M_c_lnRR) %>% exp()

```


### SMD
#### Meta-analysis level
```{r}
#****************************************************************#
#-------------------------------SMD-----------------------------#
#****************************************************************#

#-----------------------------------------------------------#
#          (1) two-tailed power for meta-analyses
#-----------------------------------------------------------#
# power to detect meta-analytic overall mean
model_est_SMD$MA.power <- power.ma_Shinichi(mu=model_est_SMD$beta0,SE=model_est_SMD$se_beta0)

# power to detect bias-corrected meta-analytic overall mean
## add bias-corrected mean to model_est_SMD
beta0_c3_SMD <- (model_est_all_corrected_original %>% filter(es_type == "SMD")) %>% select(case,beta0_c3)

model_est_SMD <- left_join(model_est_SMD,beta0_c3_SMD,by="case") # reorder rows according to vector of case

## calculate bias-corrected power
model_est_SMD$MA.power_c <- power.ma_Shinichi(mu=model_est_SMD$beta0_c3,SE=model_est_SMD$se_beta0) # note to still use the unconditional se rather than the conditional se (se of beta0c_3)

# power to detect small-study effect
model_est_SMD$sse.power <- power.ma_Shinichi(mu=model_est_SMD$beta1,SE=model_est_SMD$se_beta1)

# power to detect decline effect
model_est_SMD$de.power <- power.ma_Shinichi(mu=model_est_SMD$beta2,SE=model_est_SMD$se_beta2)

#-----------------------------------------------------------#
#            (2) type S error for meta-analyses
#-----------------------------------------------------------#
# meta-analytic overall mean
MA.power.S <- NA
for (i in 1:length(model_est_SMD$case)) {
  MA.power.S[i] <- error_S(mu=model_est_SMD$beta0[i],se=model_est_SMD$se_beta0[i],alpha=0.05) %>% unlist()
}

model_est_SMD$MA.power.S <- MA.power.S

# bias-corrected version
MA.power.S_c <- NA
for (i in 1:length(model_est_SMD$case)) {
  MA.power.S_c[i] <- error_S(mu=model_est_SMD$beta0_c3[i],se=model_est_SMD$se_beta0[i],alpha=0.05) %>% unlist()
}

model_est_SMD$MA.power.S_c <- MA.power.S_c


#-----------------------------------------------------------#
#   (3) type M error (overestimate ratio) for meta-analyses
#-----------------------------------------------------------#
# meta-analytic overall mean
MA.power.M <- NA
for (i in 1:length(model_est_SMD$case)) {
  MA.power.M[i] <- error_M(mu=model_est_SMD$beta0[i],se=model_est_SMD$se_beta0[i],alpha=0.05,N=10000) %>% unlist()
}

model_est_SMD$MA.power.M <- MA.power.M

# bias-corrected version
MA.power.M_c <- NA
for (i in 1:length(model_est_SMD$case)) {
  MA.power.M_c[i] <- error_M(mu=model_est_SMD$beta0_c3[i],se=model_est_SMD$se_beta0[i],alpha=0.05,N=10000) %>% unlist()
}

model_est_SMD$MA.power.M_c <- MA.power.M_c
```


#### Single experiment level

```{r}
#***************************************************************#
#        power for single experiments within meta-analysis       #
#***************************************************************#


#****************************************************************#
#------------------------------SMD-----------------------------#
#****************************************************************#

#--------------------- (1) two tailed power ---------------------#
# meta-analytic overall mean
# SMD
power_SMD <- NA
for (i in 1:length(SMD)) {
  power_SMD[i] <- power.individual_Shinichi(mu=model_est_SMD$beta0[i], se=SMD[[i]]$sei) %>% list()}

# allocate each set of power into corresponding dataset
for (i in 1:length(power_SMD)) {
  SMD[[i]]$power_SMD <- power_SMD[[i]]
}

# bias-corrected version
# SMD
power_c_SMD <- NA
for (i in 1:length(SMD)) {
  power_c_SMD[i] <- power.individual_Shinichi(mu=model_est_SMD$beta0_c3[i], se=SMD[[i]]$sei) %>% list()}

# allocate each set of power into corresponding dataset
for (i in 1:length(power_SMD)) {
  SMD[[i]]$power_c_SMD <- power_c_SMD[[i]]
}

#---------------------- (2) type S error -----------------------#
# meta-analytic overall mean
power.S_SMD <- NA
for (i in 1:length(SMD)) {
  power.S_SMD[i] <- mapply(error_S,mu=model_est_SMD$beta0[i], se=SMD[[i]]$sei) %>% list()}

# allocate each set of type S error into corresponding dataset
for (i in 1:length(power.S_SMD)) {
  SMD[[i]]$power.S_SMD <- power.S_SMD[[i]]
}


# bias-corrected version
power.S_c_SMD <- NA
for (i in 1:length(SMD)) {
  power.S_c_SMD[i] <- mapply(error_S,mu=model_est_SMD$beta0_c3[i], se=SMD[[i]]$sei) %>% list()}

# allocate each set of type S error into corresponding dataset
for (i in 1:length(power.S_SMD)) {
  SMD[[i]]$power.S_c_SMD <- power.S_c_SMD[[i]]
}

#---------------- (3) type M error (overestimate ratio) --------------#
# meta-analytic overall mean
power.M_SMD <- NA
for (i in 1:length(SMD)) {
  power.M_SMD[i] <- mapply(error_M,mu=model_est_SMD$beta0[i], se=SMD[[i]]$sei) %>% list()}

# allocate each set of type S error into corresponding dataset
for (i in 1:length(power.M_SMD)) {
  SMD[[i]]$power.M_SMD <- power.M_SMD[[i]]
}


# bias-corrected version
power.M_c_SMD <- NA
for (i in 1:length(SMD)) {
  power.M_c_SMD[i] <- mapply(error_M,mu=model_est_SMD$beta0_c3[i], se=SMD[[i]]$sei) %>% list()}

# allocate each set of type S error into corresponding dataset
for (i in 1:length(power.M_SMD)) {
  SMD[[i]]$power.M_c_SMD <- power.M_c_SMD[[i]]
}


#*********************************************************************#
#------------------- summary of experimental power --------------------#
#*********************************************************************#

#****************************************************************#
#-----------------------------SMD------------------------------#
#****************************************************************#

#--------------------- (1) two tailed power ---------------------#
# meta-analytic overall mean
power_summary_SMD <- data.frame(case=names(SMD),
                   Minimum=mapply(summary, sapply(SMD, function(x) x$power_SMD))[1,],      
                   `First quarter`=mapply(summary, sapply(SMD, function(x) x$power_SMD))[2,],
                   Median=mapply(summary, sapply(SMD, function(x) x$power_SMD))[3,],
                   Mean=mapply(summary, sapply(SMD, function(x) x$power_SMD))[4,], 
                   `Third quarter`=mapply(summary, sapply(SMD, function(x) x$power_SMD))[5,], 
                   Maximum=mapply(summary, sapply(SMD, function(x) x$power_SMD))[6,]) 

# bias-corrected version
power_c_summary_SMD <- data.frame(case=names(SMD),
                   Minimum=mapply(summary, sapply(SMD, function(x) x$power_c_SMD))[1,],      
                   `First quarter`=mapply(summary, sapply(SMD, function(x) x$power_c_SMD))[2,],
                   Median=mapply(summary, sapply(SMD, function(x) x$power_c_SMD))[3,],
                   Mean=mapply(summary, sapply(SMD, function(x) x$power_c_SMD))[4,], 
                   `Third quarter`=mapply(summary, sapply(SMD, function(x) x$power_c_SMD))[5,], 
                   Maximum=mapply(summary, sapply(SMD, function(x) x$power_c_SMD))[6,]) 

#---------------------- (2) type S error -----------------------#
# meta-analytic overall mean
power.S_summary_SMD <-  data.frame(case=names(SMD),
                   Minimum=mapply(summary, sapply(SMD, function(x) x$power.S_SMD))[1,],      
                   `First quarter`=mapply(summary, sapply(SMD, function(x) x$power.S_SMD))[2,],
                   Median=mapply(summary, sapply(SMD, function(x) x$power.S_SMD))[3,],
                   Mean=mapply(summary, sapply(SMD, function(x) x$power.S_SMD))[4,], 
                   `Third quarter`=mapply(summary, sapply(SMD, function(x) x$power.S_SMD))[5,], 
                   Maximum=mapply(summary, sapply(SMD, function(x) x$power.S_SMD))[6,]) 

# bias-corrected version
power.S_c_summary_SMD <-  data.frame(case=names(SMD),
                   Minimum=mapply(summary, sapply(SMD, function(x) x$power.S_c_SMD))[1,],      
                   `First quarter`=mapply(summary, sapply(SMD, function(x) x$power.S_c_SMD))[2,],
                   Median=mapply(summary, sapply(SMD, function(x) x$power.S_c_SMD))[3,],
                   Mean=mapply(summary, sapply(SMD, function(x) x$power.S_c_SMD))[4,], 
                   `Third quarter`=mapply(summary, sapply(SMD, function(x) x$power.S_c_SMD))[5,], 
                   Maximum=mapply(summary, sapply(SMD, function(x) x$power.S_c_SMD))[6,]) 

#-------------- (3) type M error (overestimate ratio) -------------#
# meta-analytic overall mean
power.M_summary_SMD <-  data.frame(case=names(SMD),
                   Minimum=mapply(summary, sapply(SMD, function(x) x$power.M_SMD))[1,],      
                   `First quarter`=mapply(summary, sapply(SMD, function(x) x$power.M_SMD))[2,],
                   Median=mapply(summary, sapply(SMD, function(x) x$power.M_SMD))[3,],
                   Mean=mapply(summary, sapply(SMD, function(x) x$power.M_SMD))[4,], 
                   `Third quarter`=mapply(summary, sapply(SMD, function(x) x$power.M_SMD))[5,], 
                   Maximum=mapply(summary, sapply(SMD, function(x) x$power.M_SMD))[6,]) 

# bias-corrected version
power.M_c_summary_SMD <-  data.frame(case=names(SMD),
                   Minimum=mapply(summary, sapply(SMD, function(x) x$power.M_c_SMD))[1,],      
                   `First quarter`=mapply(summary, sapply(SMD, function(x) x$power.M_c_SMD))[2,],
                   Median=mapply(summary, sapply(SMD, function(x) x$power.M_c_SMD))[3,],
                   Mean=mapply(summary, sapply(SMD, function(x) x$power.M_c_SMD))[4,], 
                   `Third quarter`=mapply(summary, sapply(SMD, function(x) x$power.M_c_SMD))[5,], 
                   Maximum=mapply(summary, sapply(SMD, function(x) x$power.M_c_SMD))[6,]) 

```

#### Aggregation

This section is used to obtain overall estimates of the three parameters across different meta-analyses (which provided us with comparable summaries of the three parameters).

We used weighted regression to statistically aggregate over the three parameters obtained at the within-meta-analysis level whereas we used mixed effects models to aggregate these parameters at the experiment level. Both procedures involved aggregating the parameters across meta-analyses (i.e., between-meta-analysis modelling). 
```{r}
#***************************************************************#
#      estimate overall power for meta-analysis level power     #
#***************************************************************#

# add N and k
N_SMD <- NA
for (i in 1:length(SMD)) {
  N_SMD[i] <- SMD[[i]]$study_ID %>% unique() %>% length()
}
model_est_SMD$N <- N_SMD

k_SMD <- NA
for (i in 1:length(SMD)) {
  k_SMD[i] <- SMD[[i]]$obs_ID %>% length()
}
model_est_SMD$k <- k_SMD

#--------------------- (1) two tailed power ---------------------#
# meta-analytic overall mean
# log
MMA_MA.power_SMD <- lm(log(MA.power) ~ 1, weights = k, data = model_est_SMD)
# original scale
MMA_MA.power_SMD2 <- lm(MA.power ~ 1, weights = k, data = model_est_SMD)
MMA_MA.power_SMD2$coefficients

# this is median
MMA_MA.power_SMD$coefficients  %>% exp() 
# this is mean
(MMA_MA.power_SMD$coefficients + 0.5*var(log(model_est_SMD$MA.power))) %>% exp() 
#confidence interval of median
confint(MMA_MA.power_SMD) %>% exp()

# compare residuals
par(mfrow = c(1, 2))
residuals(MMA_MA.power_SMD) %>% hist(main = paste("log power"), xlab = "Residual")
residuals(MMA_MA.power_SMD2) %>% hist(main = paste("original power"), xlab = "Residual") 

# bias-corrected version
# log
MMA_MA.power_c_SMD <- lm(log(MA.power_c) ~ 1, weights = k, data = model_est_SMD)

# this is median
MMA_MA.power_c_SMD$coefficients  %>% exp() 
# this is mean
(MMA_MA.power_c_SMD$coefficients + 0.5*var(log(model_est_SMD$MA.power))) %>% exp() 
#confidence interval of median
confint(MMA_MA.power_c_SMD) %>% exp()


#---------------------- (2) type S error -----------------------#
# meta-analytic overall mean
# log
MMA_MA.power.S_SMD <- lm(log(MA.power.S+0.025) ~ 1, weights = k, data = model_est_SMD) # add an offset of 0.025(25%) to avoid ln(0) = infinity 
# this is median
MMA_MA.power.S_SMD$coefficients %>% exp() - 0.025
# this is mean
(MMA_MA.power.S_SMD$coefficients + 0.5*var(log(model_est_SMD$MA.power.S + 0.025))) %>% exp() - 0.025
#confidence interval of median
confint(MMA_MA.power.S_SMD) %>% exp() - 0.025 # if the lower boundary was negative (probably caused by the negative variance), we used 0 to replace it

# bias-corrected version
# log
MMA_MA.power.S_c_SMD <- lm(log(MA.power.S_c+0.025) ~ 1, weights = k, data = model_est_SMD) # add an offset of 0.025(25%) to avoid ln(0) = infinity 
# this is median
MMA_MA.power.S_c_SMD$coefficients %>% exp() - 0.025
# this is mean
(MMA_MA.power.S_c_SMD$coefficients + 0.5*var(log(model_est_SMD$MA.power.S_c + 0.025))) %>% exp() - 0.025
#confidence interval of median
confint(MMA_MA.power.S_c_SMD) %>% exp() - 0.025 # if the lower boundary was negative (probably caused by the negative variance), we used 0 to replace it


#-------------- (3) type M error (overestimate ratio) -------------#
# meta-analytic overall mean
# log
MMA_MA.power.M_SMD <- lm(log(MA.power.M) ~ 1, weights = k, data = model_est_SMD)
# this is median
MMA_MA.power.M_SMD$coefficients %>% exp() 
# this is mean
(MMA_MA.power.M_SMD$coefficients + 0.5*var(log(model_est_SMD$MA.power.M))) %>% exp() 

#confidence interval of median
confint(MMA_MA.power.M_SMD) %>% exp()


# bias-corrected version
# log
MMA_MA.power.M_c_SMD <- lm(log(MA.power.M_c) ~ 1, weights = k, data = model_est_SMD)
# this is median
MMA_MA.power.M_c_SMD$coefficients %>% exp() 
# this is mean
(MMA_MA.power.M_c_SMD$coefficients + 0.5*var(log(model_est_SMD$MA.power.M_c))) %>% exp() 

#confidence interval of median
confint(MMA_MA.power.M_c_SMD) %>% exp()

#***************************************************************#
#      estimate overall power for experimental level power     #
#***************************************************************#

#****************************************************************#
#-----------------------------SMD------------------------------#
#****************************************************************#


study_ID_SMD <- sapply(SMD, function(x) x$study_ID) %>% unlist()
power_SMD <- sapply(SMD, function(x) x$power_SMD) %>% unlist()
power.S_SMD <- sapply(SMD, function(x) x$power.S_SMD) %>% unlist()
power.M_SMD <- sapply(SMD, function(x) x$power.M_SMD) %>% unlist()
power_c_SMD <- sapply(SMD, function(x) x$power_c_SMD) %>% unlist()
power.S_c_SMD <- sapply(SMD, function(x) x$power.S_c_SMD) %>% unlist()
power.M_c_SMD <- sapply(SMD, function(x) x$power.M_c_SMD) %>% unlist()

individual_est_SMD <- data.frame("study_ID_SMD" = study_ID_SMD,
                                 "power_SMD" = power_SMD,
                                 "power.S_SMD" = power.S_SMD,
                                 "power.M_SMD" = power.M_SMD,
                                 "power_c_SMD" = power_c_SMD,
                                 "power.S_c_SMD" = power.S_c_SMD,
                                 "power.M_c_SMD" = power.M_c_SMD
                                  )

#--------------------- (1) two tailed power ---------------------#
# meta-analytic overall mean
# log
MMA_EXP.power_SMD <- lmer(log(power_SMD) ~ 1 + (1 | study_ID_SMD), data = individual_est_SMD)
# original scale
MMA_EXP.power_SMD2 <- lmer(power_SMD ~ 1 + (1 | study_ID_SMD), data = individual_est_SMD)
summary(MMA_EXP.power_SMD2)$coefficients[1]

# this is median 
summary(MMA_EXP.power_SMD)$coefficients[1] %>% exp()
# this is mean
(summary(MMA_EXP.power_SMD)$coefficients[1] + 0.5*var(log(individual_est_SMD$power_SMD))) %>% exp()
# confidence interval of median
confint(MMA_EXP.power_SMD) %>% exp()

# compare residual
par(mfrow = c(1, 2))
residuals(MMA_EXP.power_SMD) %>% hist(main = paste("log power"), xlab = "Residual")
residuals(MMA_EXP.power_SMD2) %>% hist(main = paste("orignal power"), xlab = "Residual")

# bias-corrected version
# log
MMA_EXP.power_c_SMD <- lmer(log(power_c_SMD) ~ 1 + (1 | study_ID_SMD), data = individual_est_SMD)

# this is median 
summary(MMA_EXP.power_c_SMD)$coefficients[1] %>% exp()
# this is mean
(summary(MMA_EXP.power_c_SMD)$coefficients[1] + 0.5*var(log(individual_est_SMD$power_SMD))) %>% exp()
# confidence interval of median
confint(MMA_EXP.power_c_SMD) %>% exp()


#---------------------- (2) type S error -----------------------#
# meta-analytic overall mean
# log
MMA_EXP.power.S_SMD <- lmer( log(power.S_SMD + 0.025) ~ 1 + (1 | study_ID_SMD), data = individual_est_SMD) # add an offset of 0.025 to avoid log(0) = inf
# this is median 
summary(MMA_EXP.power.S_SMD)$coefficients[1] %>% exp() - 0.025 # - 0.025 is important
# this is mean
(summary(MMA_EXP.power.S_SMD)$coefficients[1] + 
                0.5*(summary(MMA_EXP.power.S_SMD)$varcor[[1]][[1]] + # sigma^2 for study level
                     summary(MMA_EXP.power.S_SMD)$sigma^2) # residual level - we cannot do like what we did above as we added 0.025
        ) %>% exp() - 0.025

#confidence interval of median
confint(MMA_EXP.power.S_SMD) %>% exp() - 0.025 # if the lower boundary was negative (probably caused by the negative variance), we used 0 to replace it

# bias-corrected version
# log
MMA_EXP.power.S_c_SMD <- lmer( log(power.S_c_SMD + 0.025) ~ 1 + (1 | study_ID_SMD), data = individual_est_SMD) # add an offset of 0.025 to avoid log(0) = inf
# this is median 
summary(MMA_EXP.power.S_c_SMD)$coefficients[1] %>% exp() - 0.025 # - 0.025 is important
# this is mean
(summary(MMA_EXP.power.S_c_SMD)$coefficients[1] + 
                0.5*(summary(MMA_EXP.power.S_SMD)$varcor[[1]][[1]] + # sigma^2 for study level
                     summary(MMA_EXP.power.S_SMD)$sigma^2) # residual level - we cannot do like what we did above as we added 0.025
        ) %>% exp() - 0.025

#confidence interval of median
confint(MMA_EXP.power.S_c_SMD) %>% exp() - 0.025 # if the lower boundary was negative (probably caused by the negative variance), we used 0 to replace it

#--------------------- (3) type M error ---------------------#
# log
MMA_EXP.power.M_SMD <- lmer(log(power.M_SMD) ~ 1 + (1 | study_ID_SMD), data = individual_est_SMD)

# this is median 
summary(MMA_EXP.power.M_SMD)$coefficients[1] %>% exp()
# this is mean
(summary(MMA_EXP.power.M_SMD)$coefficients[1] + 0.5*var(log(individual_est_SMD$power.M_SMD))) %>% exp()
# confidence interval of median
confint(MMA_EXP.power.M_SMD) %>% exp()

# bias-corrected version
# log
MMA_EXP.power.M_c_SMD <- lmer(log(power.M_c_SMD) ~ 1 + (1 | study_ID_SMD), data = individual_est_SMD)

# this is median 
summary(MMA_EXP.power.M_c_SMD)$coefficients[1] %>% exp()
# this is mean
(summary(MMA_EXP.power.M_c_SMD)$coefficients[1] + 0.5*var(log(individual_est_SMD$power.M_SMD))) %>% exp()
# confidence interval of median
confint(MMA_EXP.power.M_c_SMD) %>% exp()

```



### Zr
#### Meta-analysis level
```{r}
#****************************************************************#
#-------------------------------Zr-----------------------------#
#****************************************************************#

#-----------------------------------------------------------#
#          (1) two-tailed power for meta-analyses
#-----------------------------------------------------------#
# power to detect meta-analytic overall mean
model_est_Zr$MA.power <- power.ma_Shinichi(mu=model_est_Zr$beta0,SE=model_est_Zr$se_beta0)

# power to detect bias-corrected meta-analytic overall mean
## add bias-corrected mean to model_est_Zr
beta0_c3_Zr <- (model_est_all_corrected_original %>% filter(es_type == "Zr")) %>% select(case,beta0_c3)

model_est_Zr <- left_join(model_est_Zr,beta0_c3_Zr,by="case") # reorder rows according to vector of case

## calculate bias-corrected power
model_est_Zr$MA.power_c <- power.ma_Shinichi(mu=model_est_Zr$beta0_c3,SE=model_est_Zr$se_beta0) # note to still use the unconditional se rather than the conditional se (se of beta0c_3)

# power to detect small-study effect
model_est_Zr$sse.power <- power.ma_Shinichi(mu=model_est_Zr$beta1,SE=model_est_Zr$se_beta1)

# power to detect decline effect
model_est_Zr$de.power <- power.ma_Shinichi(mu=model_est_Zr$beta2,SE=model_est_Zr$se_beta2)

#-----------------------------------------------------------#
#            (2) type S error for meta-analyses
#-----------------------------------------------------------#
# meta-analytic overall mean
MA.power.S <- NA
for (i in 1:length(model_est_Zr$case)) {
  MA.power.S[i] <- error_S(mu=model_est_Zr$beta0[i],se=model_est_Zr$se_beta0[i],alpha=0.05) %>% unlist()
}

model_est_Zr$MA.power.S <- MA.power.S

# bias-corrected version
MA.power.S_c <- NA
for (i in 1:length(model_est_Zr$case)) {
  MA.power.S_c[i] <- error_S(mu=model_est_Zr$beta0_c3[i],se=model_est_Zr$se_beta0[i],alpha=0.05) %>% unlist()
}

model_est_Zr$MA.power.S_c <- MA.power.S_c


#-----------------------------------------------------------#
#   (3) type M error (overestimate ratio) for meta-analyses
#-----------------------------------------------------------#
# meta-analytic overall mean
MA.power.M <- NA
for (i in 1:length(model_est_Zr$case)) {
  MA.power.M[i] <- error_M(mu=model_est_Zr$beta0[i],se=model_est_Zr$se_beta0[i],alpha=0.05,N=10000) %>% unlist()
}

model_est_Zr$MA.power.M <- MA.power.M

# bias-corrected version
MA.power.M_c <- NA
for (i in 1:length(model_est_Zr$case)) {
  MA.power.M_c[i] <- error_M(mu=model_est_Zr$beta0_c3[i],se=model_est_Zr$se_beta0[i],alpha=0.05,N=10000) %>% unlist()
}

model_est_Zr$MA.power.M_c <- MA.power.M_c
```


#### Single experiment level

```{r}
#***************************************************************#
#        power for single experiments within meta-analysis       #
#***************************************************************#


#****************************************************************#
#------------------------------Zr-----------------------------#
#****************************************************************#

#--------------------- (1) two tailed power ---------------------#
# meta-analytic overall mean
# Zr
power_Zr <- NA
for (i in 1:length(Zr)) {
  power_Zr[i] <- power.individual_Shinichi(mu=model_est_Zr$beta0[i], se=Zr[[i]]$sei) %>% list()}

# allocate each set of power into corresponding dataset
for (i in 1:length(power_Zr)) {
  Zr[[i]]$power_Zr <- power_Zr[[i]]
}


# bias-corrected version
# Zr
power_c_Zr <- NA
for (i in 1:length(Zr)) {
  power_c_Zr[i] <- power.individual_Shinichi(mu=model_est_Zr$beta0_c3[i], se=Zr[[i]]$sei) %>% list()}

# allocate each set of power into corresponding dataset
for (i in 1:length(power_Zr)) {
  Zr[[i]]$power_c_Zr <- power_c_Zr[[i]]
}

#---------------------- (2) type S error -----------------------#
# meta-analytic overall mean
power.S_Zr <- NA
for (i in 1:length(Zr)) {
  power.S_Zr[i] <- mapply(error_S,mu=model_est_Zr$beta0[i], se=Zr[[i]]$sei) %>% list()}

# allocate each set of type S error into corresponding dataset
for (i in 1:length(power.S_Zr)) {
  Zr[[i]]$power.S_Zr <- power.S_Zr[[i]]
}


# bias-corrected version
power.S_c_Zr <- NA
for (i in 1:length(Zr)) {
  power.S_c_Zr[i] <- mapply(error_S,mu=model_est_Zr$beta0_c3[i], se=Zr[[i]]$sei) %>% list()}

# allocate each set of type S error into corresponding dataset
for (i in 1:length(power.S_Zr)) {
  Zr[[i]]$power.S_c_Zr <- power.S_c_Zr[[i]]
}

#---------------- (3) type M error (overestimate ratio) --------------#
# meta-analytic overall mean
power.M_Zr <- NA
for (i in 1:length(Zr)) {
  power.M_Zr[i] <- mapply(error_M,mu=model_est_Zr$beta0[i], se=Zr[[i]]$sei) %>% list()}

# allocate each set of type S error into corresponding dataset
for (i in 1:length(power.M_Zr)) {
  Zr[[i]]$power.M_Zr <- power.M_Zr[[i]]
}


# bias-corrected version
power.M_c_Zr <- NA
for (i in 1:length(Zr)) {
  power.M_c_Zr[i] <- mapply(error_M,mu=model_est_Zr$beta0_c3[i], se=Zr[[i]]$sei) %>% list()}

# allocate each set of type S error into corresponding dataset
for (i in 1:length(power.M_Zr)) {
  Zr[[i]]$power.M_c_Zr <- power.M_c_Zr[[i]]
}


#*********************************************************************#
#------------------- summary of experimental power --------------------#
#*********************************************************************#

#****************************************************************#
#-----------------------------Zr------------------------------#
#****************************************************************#

#--------------------- (1) two tailed power ---------------------#
# meta-analytic overall mean
power_summary_Zr <- data.frame(case=names(Zr),
                   Minimum=mapply(summary, sapply(Zr, function(x) x$power_Zr))[1,],      
                   `First quarter`=mapply(summary, sapply(Zr, function(x) x$power_Zr))[2,],
                   Median=mapply(summary, sapply(Zr, function(x) x$power_Zr))[3,],
                   Mean=mapply(summary, sapply(Zr, function(x) x$power_Zr))[4,], 
                   `Third quarter`=mapply(summary, sapply(Zr, function(x) x$power_Zr))[5,], 
                   Maximum=mapply(summary, sapply(Zr, function(x) x$power_Zr))[6,]) 

# bias-corrected version
power_c_summary_Zr <- data.frame(case=names(Zr),
                   Minimum=mapply(summary, sapply(Zr, function(x) x$power_c_Zr))[1,],      
                   `First quarter`=mapply(summary, sapply(Zr, function(x) x$power_c_Zr))[2,],
                   Median=mapply(summary, sapply(Zr, function(x) x$power_c_Zr))[3,],
                   Mean=mapply(summary, sapply(Zr, function(x) x$power_c_Zr))[4,], 
                   `Third quarter`=mapply(summary, sapply(Zr, function(x) x$power_c_Zr))[5,], 
                   Maximum=mapply(summary, sapply(Zr, function(x) x$power_c_Zr))[6,]) 

#---------------------- (2) type S error -----------------------#
# meta-analytic overall mean
power.S_summary_Zr <-  data.frame(case=names(Zr),
                   Minimum=mapply(summary, sapply(Zr, function(x) x$power.S_Zr))[1,],      
                   `First quarter`=mapply(summary, sapply(Zr, function(x) x$power.S_Zr))[2,],
                   Median=mapply(summary, sapply(Zr, function(x) x$power.S_Zr))[3,],
                   Mean=mapply(summary, sapply(Zr, function(x) x$power.S_Zr))[4,], 
                   `Third quarter`=mapply(summary, sapply(Zr, function(x) x$power.S_Zr))[5,], 
                   Maximum=mapply(summary, sapply(Zr, function(x) x$power.S_Zr))[6,]) 

# bias-corrected version
power.S_c_summary_Zr <-  data.frame(case=names(Zr),
                   Minimum=mapply(summary, sapply(Zr, function(x) x$power.S_c_Zr))[1,],      
                   `First quarter`=mapply(summary, sapply(Zr, function(x) x$power.S_c_Zr))[2,],
                   Median=mapply(summary, sapply(Zr, function(x) x$power.S_c_Zr))[3,],
                   Mean=mapply(summary, sapply(Zr, function(x) x$power.S_c_Zr))[4,], 
                   `Third quarter`=mapply(summary, sapply(Zr, function(x) x$power.S_c_Zr))[5,], 
                   Maximum=mapply(summary, sapply(Zr, function(x) x$power.S_c_Zr))[6,]) 

#-------------- (3) type M error (overestimate ratio) -------------#
# meta-analytic overall mean
power.M_summary_Zr <-  data.frame(case=names(Zr),
                   Minimum=mapply(summary, sapply(Zr, function(x) x$power.M_Zr))[1,],      
                   `First quarter`=mapply(summary, sapply(Zr, function(x) x$power.M_Zr))[2,],
                   Median=mapply(summary, sapply(Zr, function(x) x$power.M_Zr))[3,],
                   Mean=mapply(summary, sapply(Zr, function(x) x$power.M_Zr))[4,], 
                   `Third quarter`=mapply(summary, sapply(Zr, function(x) x$power.M_Zr))[5,], 
                   Maximum=mapply(summary, sapply(Zr, function(x) x$power.M_Zr))[6,]) 

# bias-corrected version
power.M_c_summary_Zr <-  data.frame(case=names(Zr),
                   Minimum=mapply(summary, sapply(Zr, function(x) x$power.M_c_Zr))[1,],      
                   `First quarter`=mapply(summary, sapply(Zr, function(x) x$power.M_c_Zr))[2,],
                   Median=mapply(summary, sapply(Zr, function(x) x$power.M_c_Zr))[3,],
                   Mean=mapply(summary, sapply(Zr, function(x) x$power.M_c_Zr))[4,], 
                   `Third quarter`=mapply(summary, sapply(Zr, function(x) x$power.M_c_Zr))[5,], 
                   Maximum=mapply(summary, sapply(Zr, function(x) x$power.M_c_Zr))[6,]) 

```

#### Aggregation

This section is used to obtain overall estimates of the three parameters across different meta-analyses (which provided us with comparable summaries of the three parameters).

We used weighted regression to statistically aggregate over the three parameters obtained at the within-meta-analysis level whereas we used mixed effects models to aggregate these parameters at the experiment level. Both procedures involved aggregating the parameters across meta-analyses (i.e., between-meta-analysis modelling). 
```{r}
#***************************************************************#
#      estimate overall power for meta-analysis level power     #
#***************************************************************#

# add N and k
N_Zr <- NA
for (i in 1:length(Zr)) {
  N_Zr[i] <- Zr[[i]]$study_ID %>% unique() %>% length()
}
model_est_Zr$N <- N_Zr

k_Zr <- NA
for (i in 1:length(Zr)) {
  k_Zr[i] <- Zr[[i]]$obs_ID %>% length()
}
model_est_Zr$k <- k_Zr

#--------------------- (1) two tailed power ---------------------#
# meta-analytic overall mean
# log
MMA_MA.power_Zr <- lm(log(MA.power) ~ 1, weights = k, data = model_est_Zr)
# original scale
MMA_MA.power_Zr2 <- lm(MA.power ~ 1, weights = k, data = model_est_Zr)
MMA_MA.power_Zr2$coefficients

# this is median
MMA_MA.power_Zr$coefficients  %>% exp() 
# this is mean
(MMA_MA.power_Zr$coefficients + 0.5*var(log(model_est_Zr$MA.power))) %>% exp() 
#confidence interval of median
confint(MMA_MA.power_Zr) %>% exp()

# compare residuals
par(mfrow = c(1, 2))
residuals(MMA_MA.power_Zr) %>% hist(main = paste("log power"), xlab = "Residual")
residuals(MMA_MA.power_Zr2) %>% hist(main = paste("original power"), xlab = "Residual") 

# bias-corrected version
# log
MMA_MA.power_c_Zr <- lm(log(MA.power_c) ~ 1, weights = k, data = model_est_Zr)

# this is median
MMA_MA.power_c_Zr$coefficients  %>% exp() 
# this is mean
(MMA_MA.power_c_Zr$coefficients + 0.5*var(log(model_est_Zr$MA.power))) %>% exp() 
#confidence interval of median
confint(MMA_MA.power_c_Zr) %>% exp()


#---------------------- (2) type S error -----------------------#
# meta-analytic overall mean
# log
MMA_MA.power.S_Zr <- lm(log(MA.power.S+0.025) ~ 1, weights = k, data = model_est_Zr) # add an offset of 0.025(25%) to avoid ln(0) = infinity 
# this is median
MMA_MA.power.S_Zr$coefficients %>% exp() - 0.025
# this is mean
(MMA_MA.power.S_Zr$coefficients + 0.5*var(log(model_est_Zr$MA.power.S + 0.025))) %>% exp() - 0.025
#confidence interval of median
confint(MMA_MA.power.S_Zr) %>% exp() - 0.025 # if the lower boundary was negative (probably caused by the negative variance), we used 0 to replace it

# bias-corrected version
# log
MMA_MA.power.S_c_Zr <- lm(log(MA.power.S_c+0.025) ~ 1, weights = k, data = model_est_Zr) # add an offset of 0.025(25%) to avoid ln(0) = infinity 
# this is median
MMA_MA.power.S_c_Zr$coefficients %>% exp() - 0.025
# this is mean
(MMA_MA.power.S_c_Zr$coefficients + 0.5*var(log(model_est_Zr$MA.power.S_c + 0.025))) %>% exp() - 0.025
#confidence interval of median
confint(MMA_MA.power.S_c_Zr) %>% exp() - 0.025 # if the lower boundary was negative (probably caused by the negative variance), we used 0 to replace it


#-------------- (3) type M error (overestimate ratio) -------------#
# meta-analytic overall mean
# log
MMA_MA.power.M_Zr <- lm(log(MA.power.M) ~ 1, weights = k, data = model_est_Zr)
# this is median
MMA_MA.power.M_Zr$coefficients %>% exp() 
# this is mean
(MMA_MA.power.M_Zr$coefficients + 0.5*var(log(model_est_Zr$MA.power.M))) %>% exp() 

#confidence interval of median
confint(MMA_MA.power.M_Zr) %>% exp()


# bias-corrected version
# log
MMA_MA.power.M_c_Zr <- lm(log(MA.power.M_c) ~ 1, weights = k, data = model_est_Zr)
# this is median
MMA_MA.power.M_c_Zr$coefficients %>% exp() 
# this is mean
(MMA_MA.power.M_c_Zr$coefficients + 0.5*var(log(model_est_Zr$MA.power.M_c))) %>% exp() 

#confidence interval of median
confint(MMA_MA.power.M_c_Zr) %>% exp()

#***************************************************************#
#      estimate overall power for experimental level power     #
#***************************************************************#

#****************************************************************#
#-----------------------------Zr------------------------------#
#****************************************************************#


study_ID_Zr <- sapply(Zr, function(x) x$study_ID) %>% unlist()
power_Zr <- sapply(Zr, function(x) x$power_Zr) %>% unlist()
power.S_Zr <- sapply(Zr, function(x) x$power.S_Zr) %>% unlist()
power.M_Zr <- sapply(Zr, function(x) x$power.M_Zr) %>% unlist()
power_c_Zr <- sapply(Zr, function(x) x$power_c_Zr) %>% unlist()
power.S_c_Zr <- sapply(Zr, function(x) x$power.S_c_Zr) %>% unlist()
power.M_c_Zr <- sapply(Zr, function(x) x$power.M_c_Zr) %>% unlist()

individual_est_Zr <- data.frame("study_ID_Zr" = study_ID_Zr,
                                 "power_Zr" = power_Zr,
                                 "power.S_Zr" = power.S_Zr,
                                 "power.M_Zr" = power.M_Zr,
                                 "power_c_Zr" = power_c_Zr,
                                 "power.S_c_Zr" = power.S_c_Zr,
                                 "power.M_c_Zr" = power.M_c_Zr
                                  )

#--------------------- (1) two tailed power ---------------------#
# meta-analytic overall mean
# log
MMA_EXP.power_Zr <- lmer(log(power_Zr) ~ 1 + (1 | study_ID_Zr), data = individual_est_Zr)
# original scale
MMA_EXP.power_Zr2 <- lmer(power_Zr ~ 1 + (1 | study_ID_Zr), data = individual_est_Zr)
summary(MMA_EXP.power_Zr2)$coefficients[1]

# this is median 
summary(MMA_EXP.power_Zr)$coefficients[1] %>% exp()
# this is mean
(summary(MMA_EXP.power_Zr)$coefficients[1] + 0.5*var(log(individual_est_Zr$power_Zr))) %>% exp()
# confidence interval of median
confint(MMA_EXP.power_Zr) %>% exp()

# compare residual
par(mfrow = c(1, 2))
residuals(MMA_EXP.power_Zr) %>% hist(main = paste("log power"), xlab = "Residual")
residuals(MMA_EXP.power_Zr2) %>% hist(main = paste("orignal power"), xlab = "Residual")

# bias-corrected version
# log
MMA_EXP.power_c_Zr <- lmer(log(power_c_Zr) ~ 1 + (1 | study_ID_Zr), data = individual_est_Zr)

# this is median 
summary(MMA_EXP.power_c_Zr)$coefficients[1] %>% exp()
# this is mean
(summary(MMA_EXP.power_c_Zr)$coefficients[1] + 0.5*var(log(individual_est_Zr$power_Zr))) %>% exp()
# confidence interval of median
confint(MMA_EXP.power_c_Zr) %>% exp()


#---------------------- (2) type S error -----------------------#
# meta-analytic overall mean
# log
MMA_EXP.power.S_Zr <- lmer( log(power.S_Zr + 0.025) ~ 1 + (1 | study_ID_Zr), data = individual_est_Zr) # add an offset of 0.025 to avoid log(0) = inf
# this is median 
summary(MMA_EXP.power.S_Zr)$coefficients[1] %>% exp() - 0.025 # - 0.025 is important
# this is mean
(summary(MMA_EXP.power.S_Zr)$coefficients[1] + 
                0.5*(summary(MMA_EXP.power.S_Zr)$varcor[[1]][[1]] + # sigma^2 for study level
                     summary(MMA_EXP.power.S_Zr)$sigma^2) # residual level - we cannot do like what we did above as we added 0.025
        ) %>% exp() - 0.025

#confidence interval of median
confint(MMA_EXP.power.S_Zr) %>% exp() - 0.025 # if the lower boundary was negative (probably caused by the negative variance), we used 0 to replace it

# bias-corrected version
# log
MMA_EXP.power.S_c_Zr <- lmer( log(power.S_c_Zr + 0.025) ~ 1 + (1 | study_ID_Zr), data = individual_est_Zr) # add an offset of 0.025 to avoid log(0) = inf
# this is median 
summary(MMA_EXP.power.S_c_Zr)$coefficients[1] %>% exp() - 0.025 # - 0.025 is important
# this is mean
(summary(MMA_EXP.power.S_c_Zr)$coefficients[1] + 
                0.5*(summary(MMA_EXP.power.S_Zr)$varcor[[1]][[1]] + # sigma^2 for study level
                     summary(MMA_EXP.power.S_Zr)$sigma^2) # residual level - we cannot do like what we did above as we added 0.025
        ) %>% exp() - 0.025

#confidence interval of median
confint(MMA_EXP.power.S_c_Zr) %>% exp() - 0.025 # if the lower boundary was negative (probably caused by the negative variance), we used 0 to replace it

#--------------------- (3) type M error ---------------------#
# log
MMA_EXP.power.M_Zr <- lmer(log(power.M_Zr) ~ 1 + (1 | study_ID_Zr), data = individual_est_Zr)

# this is median 
summary(MMA_EXP.power.M_Zr)$coefficients[1] %>% exp()
# this is mean
(summary(MMA_EXP.power.M_Zr)$coefficients[1] + 0.5*var(log(individual_est_Zr$power.M_Zr))) %>% exp()
# confidence interval of median
confint(MMA_EXP.power.M_Zr) %>% exp()

# bias-corrected version
# log
MMA_EXP.power.M_c_Zr <- lmer(log(power.M_c_Zr) ~ 1 + (1 | study_ID_Zr), data = individual_est_Zr)

# this is median 
summary(MMA_EXP.power.M_c_Zr)$coefficients[1] %>% exp()
# this is mean
(summary(MMA_EXP.power.M_c_Zr)$coefficients[1] + 0.5*var(log(individual_est_Zr$power.M_Zr))) %>% exp()
# confidence interval of median
confint(MMA_EXP.power.M_c_Zr) %>% exp()

```


### Power across 3 types of effect size measures

#### Meta-analysis level
```{r}


#***************************************************************#
#      estimate overall power for meta-analysis level power     #
#***************************************************************#
#-----------------------------------------------------------#
#          (1) two-tailed power for meta-analyses
#-----------------------------------------------------------#
# power to detect meta-analytic overall mean
model_est_all_corrected_original$MA.power <- power.ma_Shinichi(mu=model_est_all_original$beta0,SE=model_est_all_corrected_original$se_beta0)

# power to detect bias-corrected meta-analytic overall mean
model_est_all_corrected_original$MA.power_c <- power.ma_Shinichi(mu=model_est_all_corrected_original$beta0_c3,SE=model_est_all_corrected_original$se_beta0) # note to still use the unconditional se rather than the conditional se (se of beta0c_3)

# power to detect small-study effect
model_est_all_corrected_original$sse.power <- power.ma_Shinichi(mu=model_est_all_corrected_original$beta1,SE=model_est_all_corrected_original$se_beta1)

# power to detect decline effect
model_est_all_corrected_original$de.power <- power.ma_Shinichi(mu=model_est_all_corrected_original$beta2,SE=model_est_all_corrected_original$se_beta2)

#-----------------------------------------------------------#
#            (2) type S error for meta-analyses
#-----------------------------------------------------------#
# meta-analytic overall mean
MA.power.S <- NA
for (i in 1:length(model_est_all_corrected_original$case)) {
  MA.power.S[i] <- error_S(mu=model_est_all_corrected_original$beta0[i],se=model_est_all_corrected_original$se_beta0[i],alpha=0.05) %>% unlist()
}

model_est_all_corrected_original$MA.power.S <- MA.power.S

# bias-corrected version
MA.power.S_c <- NA
for (i in 1:length(model_est_all_corrected_original$case)) {
  MA.power.S_c[i] <- error_S(mu=model_est_all_corrected_original$beta0_c3[i],se=model_est_all_corrected_original$se_beta0[i],alpha=0.05) %>% unlist()
}

model_est_all_corrected_original$MA.power.S_c <- MA.power.S_c


#-----------------------------------------------------------#
#   (3) type M error (overestimate ratio) for meta-analyses
#-----------------------------------------------------------#
# meta-analytic overall mean
MA.power.M <- NA
for (i in 1:length(model_est_all_corrected_original$case)) {
  MA.power.M[i] <- error_M(mu=model_est_all_corrected_original$beta0[i],se=model_est_all_corrected_original$se_beta0[i],alpha=0.05,N=10000) %>% unlist()
}

model_est_all_corrected_original$MA.power.M <- MA.power.M

# bias-corrected version
MA.power.M_c <- NA
for (i in 1:length(model_est_all_corrected_original$case)) {
  MA.power.M_c[i] <- error_M(mu=model_est_all_corrected_original$beta0_c3[i],se=model_est_all_corrected_original$se_beta0[i],alpha=0.05,N=10000) %>% unlist()
}

model_est_all_corrected_original$MA.power.M_c <- MA.power.M_c
```


#### Single experimental level

```{r}
individual_est_all <- data.frame(study_ID_all = c(individual_est_lnRR$study_ID_lnRR, individual_est_SMD$study_ID_SMD, individual_est_Zr$study_ID_Zr),
                                 power_all = c(individual_est_lnRR$power_lnRR,individual_est_SMD$power_SMD,individual_est_Zr$power_Zr),
                                 power.S_all = c(individual_est_lnRR$power.S_lnRR,individual_est_SMD$power.S_SMD,individual_est_Zr$power.S_Zr),
                                 power.M_all = c(individual_est_lnRR$power.M_lnRR,individual_est_SMD$power.M_SMD,individual_est_Zr$power.M_Zr),
                                 power_c_all = c(individual_est_lnRR$power_c_lnRR,individual_est_SMD$power_c_SMD,individual_est_Zr$power_c_Zr),
                                 power.S_c_all = c(individual_est_lnRR$power.S_c_lnRR,individual_est_SMD$power.S_c_SMD,individual_est_Zr$power.S_c_Zr),
                                 power.M_c_all = c(individual_est_lnRR$power.M_c_lnRR,individual_est_SMD$power.M_c_SMD,individual_est_Zr$power.M_c_Zr))


```



#### Aggregation
```{r}
#***************************************************************#
#      estimate overall power for meta-analysis level power     #
#***************************************************************#

# combine lnRR, SMD, and Zr
model_est_all_corrected_original2 <- rbind(model_est_lnRR,model_est_SMD,model_est_Zr)


#--------------------- (1) two tailed power ---------------------#
# meta-analytic overall mean
# log
MMA_MA.power_all <- lm(log(MA.power) ~ 1, weights = k, data = model_est_all_corrected_original2)
# original scale
MMA_MA.power_all2 <- lm(MA.power ~ 1, weights = k, data = model_est_all_corrected_original2)
MMA_MA.power_all2$coefficients

# this is median
MMA_MA.power_all$coefficients  %>% exp() 
# this is mean
(MMA_MA.power_all$coefficients + 0.5*var(log(individual_est_all_original$MA.power))) %>% exp() 
#confidence interval of median
confint(MMA_MA.power_all) %>% exp()

# compare residuals
par(mfrow = c(1, 2))
residuals(MMA_MA.power_all) %>% hist(main = paste("log power"), xlab = "Residual")
residuals(MMA_MA.power_all2) %>% hist(main = paste("original power"), xlab = "Residual") 

# bias-corrected version
# log
MMA_MA.power_c_all <- lm(log(MA.power_c) ~ 1, weights = k, data = model_est_all_corrected_original2)

# this is median
MMA_MA.power_c_all$coefficients  %>% exp() 
# this is mean
(MMA_MA.power_c_all$coefficients + 0.5*var(log(model_est_all_corrected_original2$MA.power))) %>% exp() 
#confidence interval of median
confint(MMA_MA.power_c_all) %>% exp()


#---------------------- (2) type S error -----------------------#
# meta-analytic overall mean
# log
MMA_MA.power.S_all <- lm(log(MA.power.S+0.025) ~ 1, weights = k, data = model_est_all_corrected_original2) # add an offset of 0.025(25%) to avoid ln(0) = infinity 
# this is median
MMA_MA.power.S_all$coefficients %>% exp() - 0.025
# this is mean
(MMA_MA.power.S_all$coefficients + 0.5*var(log(model_est_all_corrected_original2$MA.power.S + 0.025))) %>% exp() - 0.025
#confidence interval of median
confint(MMA_MA.power.S_all) %>% exp() - 0.025 # if the lower boundary was negative (probably caused by the negative variance), we used 0 to replace it

# bias-corrected version
# log
MMA_MA.power.S_c_all <- lm(log(MA.power.S_c+0.025) ~ 1, weights = k, data = model_est_all_corrected_original2) # add an offset of 0.025(25%) to avoid ln(0) = infinity 
# this is median
MMA_MA.power.S_c_all$coefficients %>% exp() - 0.025
# this is mean
(MMA_MA.power.S_c_all$coefficients + 0.5*var(log(model_est_all_corrected_original2$MA.power.S_c + 0.025))) %>% exp() - 0.025
#confidence interval of median
confint(MMA_MA.power.S_c_all) %>% exp() - 0.025 # if the lower boundary was negative (probably caused by the negative variance), we used 0 to replace it


#-------------- (3) type M error (overestimate ratio) -------------#
# meta-analytic overall mean
# log
MMA_MA.power.M_all <- lm(log(MA.power.M) ~ 1, weights = k, data = model_est_all_corrected_original2)
# this is median
MMA_MA.power.M_all$coefficients %>% exp() 
# this is mean
(MMA_MA.power.M_all$coefficients + 0.5*var(log(model_est_all_corrected_original2$MA.power.M))) %>% exp() 

#confidence interval of median
confint(MMA_MA.power.M_all) %>% exp()


# bias-corrected version
# log
MMA_MA.power.M_c_all <- lm(log(MA.power.M_c) ~ 1, weights = k, data = model_est_all_corrected_original2)
# this is median
MMA_MA.power.M_c_all$coefficients %>% exp() 
# this is mean
(MMA_MA.power.M_c_all$coefficients + 0.5*var(log(model_est_all_corrected_original2$MA.power.M_c))) %>% exp() 

#confidence interval of median
confint(MMA_MA.power.M_c_all) %>% exp()


#***************************************************************#
#      estimate overall power for experimental level power     #
#***************************************************************#

#****************************************************************#
#-----------------------------all------------------------------#
#****************************************************************#

#--------------------- (1) two tailed power ---------------------#
# meta-analytic overall mean
# log
MMA_EXP.power_all <- lmer(log(power_all) ~ 1 + (1 | study_ID_all), data = individual_est_all)
# original scale
MMA_EXP.power_all2 <- lmer(power_all ~ 1 + (1 | study_ID_all), data = individual_est_all)
summary(MMA_EXP.power_all2)$coefficients[1]

# this is median 
summary(MMA_EXP.power_all)$coefficients[1] %>% exp()
# this is mean
(summary(MMA_EXP.power_all)$coefficients[1] + 0.5*var(log(individual_est_all$power_all))) %>% exp()
# confidence interval of median
confint(MMA_EXP.power_all) %>% exp()

# compare residual
par(mfrow = c(1, 2))
residuals(MMA_EXP.power_all) %>% hist(main = paste("log power"), xlab = "Residual")
residuals(MMA_EXP.power_all2) %>% hist(main = paste("orignal power"), xlab = "Residual")

# bias-corrected version
# log
MMA_EXP.power_c_all <- lmer(log(power_c_all) ~ 1 + (1 | study_ID_all), data = individual_est_all)

# this is median 
summary(MMA_EXP.power_c_all)$coefficients[1] %>% exp()
# this is mean
(summary(MMA_EXP.power_c_all)$coefficients[1] + 0.5*var(log(individual_est_all$power_all))) %>% exp()
# confidence interval of median
confint(MMA_EXP.power_c_all) %>% exp()


#---------------------- (2) type S error -----------------------#
# meta-analytic overall mean
# log
MMA_EXP.power.S_all <- lmer( log(power.S_all + 0.025) ~ 1 + (1 | study_ID_all), data = individual_est_all) # add an offset of 0.025 to avoid log(0) = inf
# this is median 
summary(MMA_EXP.power.S_all)$coefficients[1] %>% exp() - 0.025 # - 0.025 is important
# this is mean
(summary(MMA_EXP.power.S_all)$coefficients[1] + 
                0.5*(summary(MMA_EXP.power.S_all)$varcor[[1]][[1]] + # sigma^2 for study level
                     summary(MMA_EXP.power.S_all)$sigma^2) # residual level - we cannot do like what we did above as we added 0.025
        ) %>% exp() - 0.025

#confidence interval of median
confint(MMA_EXP.power.S_all) %>% exp() - 0.025 # if the lower boundary was negative (probably caused by the negative variance), we used 0 to replace it

# bias-corrected version
# log
MMA_EXP.power.S_c_all <- lmer( log(power.S_c_all + 0.025) ~ 1 + (1 | study_ID_all), data = individual_est_all) # add an offset of 0.025 to avoid log(0) = inf
# this is median 
summary(MMA_EXP.power.S_c_all)$coefficients[1] %>% exp() - 0.025 # - 0.025 is important
# this is mean
(summary(MMA_EXP.power.S_c_all)$coefficients[1] + 
                0.5*(summary(MMA_EXP.power.S_all)$varcor[[1]][[1]] + # sigma^2 for study level
                     summary(MMA_EXP.power.S_all)$sigma^2) # residual level - we cannot do like what we did above as we added 0.025
        ) %>% exp() - 0.025

#confidence interval of median
confint(MMA_EXP.power.S_c_all) %>% exp() - 0.025 # if the lower boundary was negative (probably caused by the negative variance), we used 0 to replace it

#--------------------- (3) type M error ---------------------#
# log
MMA_EXP.power.M_all <- lmer(log(power.M_all) ~ 1 + (1 | study_ID_all), data = individual_est_all)

# this is median 
summary(MMA_EXP.power.M_all)$coefficients[1] %>% exp()
# this is mean
(summary(MMA_EXP.power.M_all)$coefficients[1] + 0.5*var(log(individual_est_all$power.M_all))) %>% exp()
# confidence interval of median
confint(MMA_EXP.power.M_all) %>% exp()

# bias-corrected version
# log
MMA_EXP.power.M_c_all <- lmer(log(power.M_c_all) ~ 1 + (1 | study_ID_all), data = individual_est_all)

# this is median 
summary(MMA_EXP.power.M_c_all)$coefficients[1] %>% exp()
# this is mean
(summary(MMA_EXP.power.M_c_all)$coefficients[1] + 0.5*var(log(individual_est_all$power.M_all))) %>% exp()
# confidence interval of median
confint(MMA_EXP.power.M_c_all) %>% exp()

```



# 2.2 scaled data

## (i) estimates of beta0
fit intercept-only multilevel model to each dataset

```{r}

#*************************************************************************#
#                        meta-analytic overall mean                    
#*************************************************************************#

## lnRR
model_lnRR_scaled <- NA
for (i in 1:length(lnRR)) {
  model_lnRR_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), method = "REML", test = "t", data = lnRR[[i]], sparse = TRUE, control = list(optimizer = "optim")) %>% list()
}

## SMD
model_SMD_scaled <- NA
for (i in 1:length(SMD)) {
  model_SMD_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), method = "REML", test = "t", data = SMD[[i]], sparse = TRUE, control = list(optimizer = "optim")) %>% list()
}


## Zr
model_Zr_scaled <- NA
for (i in 1:length(Zr)) {
  model_Zr_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), method = "REML", test = "t", data = Zr[[i]], sparse = TRUE, control = list(optimizer = "optim")) %>% list()
}
```

## (ii) detect and corrected publication bias

we aim for detecting two forms of publication bias: small-study effectand decline effect.

we use a full model with sampling error (sei) and publication year (year_pub.l) as moderators to detect publication bias.

of relevance, sei's slope (beta1) and year_pub.l's slope (beta2) can be used to indicate the occurrence of small-study effect and decline effect, respectively.

note that for SMD we need to model 'effective sample size' based sampling error analogue (ees.sei), where possible

### (a) fit full models with samping error and year as predictors

note that in the subsequent models, we keep still year as its original scale.

this is to ease interpretation, where the beta2 can be interpreted as 'one year increase leads to xx unit decrease of for, example, SMD'

```{r}

#*************************************************************************#
#       Full model with error and latest year as predictors
#*************************************************************************#

## use sampling error (sei) and latest year (year_pub.l) as predictors for those with calculated effect sizes and sampling variance

## lnRR
model_lnRR_sei.year_scaled <- NA
for (i in 1:length(lnRR[lnRR_filenames])) {
  model_lnRR_sei.year_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ sei_zscore + year_pub.l, method = "REML", test = "t", data = lnRR[lnRR_filenames][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
} # note that here only fit the subset of lnRR[lnRR_filenames] - lnRR with calculated effect sizes and sampling variance

## use effective-sample-size based sampling error (ess.sei) where possible (lnRR with descriptive statistics)
model_lnRR_ess.sei.year_scaled <- NA
for (i in 1:length(lnRR[lnRR_des_filenames])) {
  model_lnRR_ess.sei.year_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ ess.sei_zscore + year_pub.l, method = "REML", test = "t", data = lnRR[lnRR_des_filenames][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
} 

## SMD
model_SMD_sei.year_scaled <- NA
for (i in 1:length(SMD[SMD_filenames])) {
  model_SMD_sei.year_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ sei_zscore + year_pub.l, method = "REML", test = "t", data = SMD[SMD_filenames][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
} # note that here only fit the subset of SMD[SMD_filenames] - SMD with calculated effect sizes and sampling variance

## use effective-sample-size based sampling error (ess.sei) where possible (SMD with descriptive statistics)

model_SMD_ess.sei.year_scaled <- NA
for (i in 1:length(SMD[SMD_des_filenames])) {
  model_SMD_ess.sei.year_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ ess.sei_zscore + year_pub.l, method = "REML", test = "t", data = SMD[SMD_des_filenames][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
} 

## Zr - Zr does not have the concern of ‘artefactual’ correlation between effect size and sampling error (because the formula to Zr's estimate sampling error has no component of point estimate: 1/(n-3)). So we only need to fit the regression model with sampling error (sei) as a predictor
model_Zr_sei.year_scaled <- NA
for (i in 1:length(Zr)) {
  model_Zr_sei.year_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ sei_zscore + year_pub.l, method = "REML", test = "t", data = Zr[[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}


```


### (b) identify the presence of publication bias

we next aim for identify the presence of the small-study effect and decline effect for each meta-analysis. 

our rational is: for an effect that is expected to be positive, a small study effect and decline effect would be expressed in a positive value of beta1 and negative value of beta2, respectively. In this respect, a slope (beta1 or beta2)) with opposing direction (unexpected sign) indicates no detectable publication bias and subsequently does not require correction for such a bias.

we use the product of beta0 and beta1 (i.e., beta×beta1) as the signal, that is, if beta0×beta1 is positive, it indicates the examined meta-analysis has a small-study effect (beta1 is in a correct direction).

similarly, when the product of beta0×beta2 is negative, the examined meta-analysis has a decline effect (beta 2 is in a correct direction)

#### lnRR

```{r}
#*************************************************************************#
#             Identify the presence of publication bias 
#*************************************************************************#

## check the significance and direction of model regressions from each meta-analysis to identify whether it presents a small-study effect or decline effect

## first to create a dataframe containing full-model's parameter estimates
### combine two types full-models (with sei and ess.sei as a predictor, respectively)
### lnRR
model_lnRR_pb_scaled <- append(model_lnRR_sei.year_scaled, model_lnRR_ess.sei.year_scaled)
### extract model model coefficients and their significance test results
model_est_lnRR_scaled <- data.frame(case = names(lnRR),
                             es_type = rep("lnRR", length(lnRR)),
                             beta0 = sapply(model_lnRR_scaled, function(x) x$beta), # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = sapply(model_lnRR_scaled, function(x) x$se), # standard error of beta0
                             pval_beta0 = sapply(model_lnRR_scaled, function(x) x$pval), # p value of beta0
                             beta0_c = sapply(model_lnRR_pb_scaled, function(x) x$beta[1]), # beta0_c in equation 2 -  bias corrected overall mean
                             se_beta0_c = sapply(model_lnRR_pb_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_lnRR_pb_scaled, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_lnRR_pb_scaled, function(x) x$beta[2]), # beta1 in Equation 2 - slope of sampling error 
                             se_beta1 = sapply(model_lnRR_pb_scaled, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_lnRR_pb_scaled, function(x) x$pval[2]), # p value of beta1
                             beta2 = sapply(model_lnRR_pb_scaled, function(x) x$beta[3]), # beta2 in Equation 2 - slope of year 
                             se_beta2 = sapply(model_lnRR_pb_scaled, function(x) x$se[3]), # standard error of beta2
                             pval_beta2 = sapply(model_lnRR_pb_scaled, function(x) x$pval[3]) # p value of beta2
                            )

## we next aim to identify the presence of the small-study effect and decline effect for each meta-analysis. We also figure out the wrong directions of slope, which will be used to inform the parameterization of reduced models

## our rational is: for an effect that is expected to be positive, a small study effect and decline effect would be expressed in a positive value of beta1 and negative value of beta2, respectively. In such a case, a slope (beta1] or beta2)) with opposing direction (unexpected sign) indicates no detectable publication bias and subsequently does not require correction for such a bias

## we use the product of beta0 and beta1 (i.e., beta*beta1) as the signal, that is, if beta0*beta1 is positive, it indicates the examined meta-analysis has a small-study effect (beta1 is in a correct direction)

## of relevance, when the value of beta0*beta2 is negative, the examined meta-analysis has a decline effect (beta 2 is in a correct direction)

## so we first to create two new columns to contain the two products:  beta0*beta1 and beta0*beta2

## lnRR
model_est_lnRR_scaled[15:16] <- data.frame(beta0Tbeta1 = model_est_lnRR_scaled$beta0 * model_est_lnRR_scaled$beta1, beta0Tbeta2 = model_est_lnRR_scaled$beta0 * model_est_lnRR_scaled$beta2) # model_est_lnRR has 14 column (ncol(model_est_lnRR)), so we add columns 15 and 16

## visual check
model_est_lnRR_scaled

## identify the small-study effect - significant beta1 with correct sign
sse_lnRR_scaled <- model_est_lnRR_scaled %>% subset(model_est_lnRR_scaled$pval_beta1 < 0.05 & model_est_lnRR_scaled$beta0Tbeta1 > 0)
## check which meta-analyses have small-study effects
sse_lnRR_scaled$case 

# proportion graph
# lnRR
# the number of correct direction of significant beta1
model_est_lnRR_scaled %>% filter(beta0Tbeta1 > 0, pval_beta1 < 0.05) %>% nrow() -> n.beta1.sig.lnRR # 1
# the number of correct direction of non-significant beta1
model_est_lnRR_scaled %>% filter(beta0Tbeta1 > 0, pval_beta1 > 0.05) %>% nrow() -> n.beta1.nonsig.lnRR # 9
# the number of wrong direction beta1
model_est_lnRR_scaled %>% filter(beta0Tbeta1 < 0 ) %>% nrow() -> n.beta1.wrong.lnRR # 10

# the number of correct direction of significant beta2
model_est_lnRR_scaled %>% filter(beta0Tbeta2 < 0, pval_beta2 < 0.05) %>% nrow() -> n.beta2.sig.lnRR # 1
# the number of correct direction of non-significant beta2
model_est_lnRR_scaled %>% filter(beta0Tbeta2 < 0, pval_beta2 > 0.05) %>% nrow() -> n.beta2.nonsig.lnRR # 11
# the number of wrong direction beta2
model_est_lnRR_scaled %>% filter(beta0Tbeta2 > 0 ) %>% nrow() -> n.beta2.wrong.lnRR # 8

# SMD
# the number of correct direction of significant beta1
model_est_SMD_scaled %>% filter(beta0Tbeta1 > 0, pval_beta1 < 0.05) %>% nrow() -> n.beta1.sig.SMD # 8
# the number of correct direction of non-significant beta1
model_est_SMD_scaled %>% filter(beta0Tbeta1 > 0, pval_beta1 > 0.05) %>% nrow() -> n.beta1.nonsig.SMD # 14
# the number of wrong direction beta1
model_est_SMD_scaled %>% filter(beta0Tbeta1 < 0 ) %>% nrow() -> n.beta1.wrong.SMD # 14

# the number of correct direction of significant beta2
model_est_SMD_scaled %>% filter(beta0Tbeta2 < 0, pval_beta2 < 0.05) %>% nrow() -> n.beta2.sig.SMD # 8
# the number of correct direction of non-significant beta2
model_est_SMD_scaled %>% filter(beta0Tbeta2 < 0, pval_beta2 > 0.05) %>% nrow() -> n.beta2.nonsig.SMD # 17
# the number of wrong direction beta1
model_est_SMD_scaled %>% filter(beta0Tbeta2 > 0 ) %>% nrow() -> n.beta2.wrong.SMD # 11

# Zr
# the number of correct direction of significant beta1
model_est_Zr_scaled %>% filter(beta0Tbeta1 > 0, pval_beta1 < 0.05) %>% nrow() -> n.beta1.sig.Zr # 6
# the number of correct direction of non-significant beta1
model_est_Zr_scaled %>% filter(beta0Tbeta1 > 0, pval_beta1 > 0.05) %>% nrow() -> n.beta1.nonsig.Zr # 16
# the number of wrong direction beta1
model_est_Zr_scaled %>% filter(beta0Tbeta1 < 0 ) %>% nrow() -> n.beta1.wrong.Zr # 9

# the number of correct direction of significant beta2
model_est_Zr_scaled %>% filter(beta0Tbeta2 < 0, pval_beta2 < 0.05) %>% nrow() -> n.beta2.sig.Zr # 6
# the number of correct direction of non-significant beta2
model_est_Zr_scaled %>% filter(beta0Tbeta2 < 0, pval_beta2 > 0.05) %>% nrow() -> n.beta2.nonsig.Zr # 11
# the number of wrong direction beta1
model_est_Zr_scaled %>% filter(beta0Tbeta2 > 0) %>% nrow() -> n.beta2.wrong.Zr # 14


# make a dataframe of beta1
freq_pb_df1 <- data.frame(percentage = c(n.beta1.sig.lnRR/20,n.beta1.nonsig.lnRR/20,n.beta1.wrong.lnRR/20,
  n.beta1.sig.SMD/36,n.beta1.nonsig.SMD/36,n.beta1.wrong.SMD/36,
  n.beta1.sig.Zr/31,n.beta1.nonsig.Zr/31,n.beta1.wrong.Zr/31),
                         trend = rep(c("Expected sign & p < 0.05", "Expected sign & p > 0.05", "Unexpected sign"),3),
                         es_type = c(rep("lnRR",3),rep("SMD",3),rep("Zr",3)))
# reorder
freq_pb_df1$es_type <- factor(freq_pb_df1$es_type, levels = c("Zr","SMD","lnRR"))
                    
# make stacked Barplot to show beta1
freq_beta1_p <- ggplot(freq_pb_df1, aes(fill=trend, y=percentage*100, x=es_type)) + 
  geom_bar(position='stack', stat='identity') + 
   geom_text(aes(label=scales::percent(percentage, accuracy = 1)), size = 3, position = position_stack(vjust = 0.5), color = "white") +
  labs(fill="", x="", y="% of meta-analyses showing evidence of a small-study effect") +
  theme(legend.position="top",legend.direction="horizontal",
        axis.text = element_text(colour = "black", size = 10)) +
  scale_fill_discrete(guide = guide_legend(reverse=TRUE)) +
  coord_flip() + 
  scale_fill_colorblind()



# make a dataframe of beta2
freq_pb_df2 <- data.frame(percentage = c(n.beta2.sig.lnRR/20,n.beta2.nonsig.lnRR/20,n.beta2.wrong.lnRR/20,
  n.beta2.sig.SMD/36,n.beta2.nonsig.SMD/36,n.beta2.wrong.SMD/36,
  n.beta2.sig.Zr/31,n.beta2.nonsig.Zr/31,n.beta2.wrong.Zr/31),
                         trend = rep(c("Expected sign & p < 0.05", "Expected sign & p > 0.05", "Unexpected sign"),3),
                         es_type = c(rep("lnRR",3),rep("SMD",3),rep("Zr",3)))
  
                  
# make stacked Barplot to show beta2
# reorder
freq_pb_df2$es_type <- factor(freq_pb_df2$es_type, levels = c("Zr","SMD","lnRR"))
freq_beta2_p <- ggplot(freq_pb_df2, aes(fill=trend, y=percentage*100, x=es_type)) + 
  geom_bar(position='stack', stat='identity') + 
  geom_text(aes(label = scales::percent(percentage, accuracy = 1)), size = 3, position = position_stack(vjust = 0.5), color = "white") +
  labs(fill="", x="", y="% of meta-analyses showing evidence of a decline effect") +
  theme(legend.position="top",legend.direction="horizontal",
        axis.text = element_text(colour = "black", size = 10)) +
  scale_fill_discrete(guide = guide_legend(reverse=TRUE)) +
  coord_flip() + 
  scale_fill_colorblind()


## put together
png(filename = "./freq_publication_bias.png", width = 7, height = 5, units = "in", type = "windows", res = 400)
plot_grid(freq_beta1_p,freq_beta2_p,labels = c("A","B"),ncol = 1)
dev.off()





## identify the decline effect - significant beta2 with correct sign 
de_lnRR_scaled <- model_est_lnRR_scaled %>% subset(model_est_lnRR_scaled$pval_beta2 < 0.05 & model_est_lnRR_scaled$beta0Tbeta2 < 0)
## check which meta-analyses have decline effects
de_lnRR_scaled$case # "ft181_1", "ft030.csv"  has a decline effect

## identify the concurrence of the small-study effect and decline effect
sse_de_lnRR_scaled <- model_est_lnRR_scaled %>% subset(model_est_lnRR_scaled$pval_beta1 < 0.05 & model_est_lnRR_scaled$beta0Tbeta1 > 0 & model_est_lnRR_scaled$pval_beta2 < 0.05 & model_est_lnRR_scaled$beta0Tbeta2 < 0) 
sse_de_lnRR_scaled$case # no meta-analysis surfers from both a small-study effect and decline effect



#*************************************************************************#
#        Multilevel models to estimate bias-corrected effect
#*************************************************************************#

#*****************************scenario 1****************************#
## both beta1 and beta2 has a correct direction
beta1c_beta2c_lnRR_scaled <- model_est_lnRR_scaled %>% subset(model_est_lnRR_scaled$beta0Tbeta1 > 0 & model_est_lnRR_scaled$beta0Tbeta2 < 0) 
beta1c_beta2c_lnRR_scaled$case 

## if a model slope (beta1 and beta2) has a wrong direction, we need to take out it when fitting model to estimate the bias-corrected mean
## in scenario 1, both of the two slopes have a correct direction, we use can use full model directly, no need to use take out any predictor

## full model based on scenario 1 - both beta1 and beta2 has a correct direction
## make a data list which only contains scenario1's data
s1_file_scaled <- beta1c_beta2c_lnRR_scaled$case
## fit ess.sei where possible
## subset of sei
s1_sei_file_scaled <- lnRR_filenames[lnRR_filenames %in% s1_file_scaled] # this subset should use sei as a predictor and belong to scenario 1
## model fitting - fit a full model
model_lnRR_sei_s1_scaled <- NA
for (i in 1:length(s1_sei_file_scaled)) {
  model_lnRR_sei_s1_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ sei_zscore + year_pub.l, method = "REML", test = "t", data = lnRR[s1_sei_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## replace sei by var
model_lnRR_var_s1_scaled <- NA
for (i in 1:length(s1_sei_file_scaled)) {
  model_lnRR_var_s1_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ var_zscore + year_pub.l, method = "REML", test = "t", data = lnRR[s1_sei_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}


## extract model model coefficients and their significance test results for 'sei' in scenario 1
model_est_lnRR_sei_s1_scaled <- data.frame(case = s1_sei_file_scaled,
                             es_type = rep("lnRR", length(s1_sei_file_scaled)),
                             beta0 = model_est_lnRR_scaled[model_est_lnRR_scaled$case %in% s1_sei_file_scaled, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_lnRR_scaled[model_est_lnRR_scaled$case %in% s1_sei_file_scaled, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_lnRR_scaled[model_est_lnRR_scaled$case %in% s1_sei_file_scaled, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_lnRR_sei_s1_scaled, function(x) x$beta[1]), # beta0_c in equation 6 -  bias corrected overall mean
                             se_beta0_c = sapply(model_lnRR_sei_s1_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_lnRR_sei_s1_scaled, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_lnRR_sei_s1_scaled, function(x) x$beta[2]), # beta1 in Equation 6 - slope of sampling error 
                             se_beta1 = sapply(model_lnRR_sei_s1_scaled, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_lnRR_sei_s1_scaled, function(x) x$pval[2]), # p value of beta1
                             beta2 = sapply(model_lnRR_sei_s1_scaled, function(x) x$beta[3]), # beta2 in Equation 6 - slope of year 
                             se_beta2 = sapply(model_lnRR_sei_s1_scaled, function(x) x$se[3]), # standard error of beta2
                             pval_beta2 = sapply(model_lnRR_sei_s1_scaled, function(x) x$pval[3]), # p value of beta2
                             beta0_c2 = sapply(model_lnRR_var_s1_scaled, function(x) x$beta[1]), # beta0_c in equation 6 -  bias corrected overall mean
                             se_beta0_c2 = sapply(model_lnRR_var_s1_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_lnRR_var_s1_scaled, function(x) x$pval[1]) # p valuer of beta0_c
                            )



## subset of ess.sei
s1_ess.sei_file_scaled <- lnRR_des_filenames[lnRR_des_filenames %in% s1_file_scaled] # this subset should fit ess.sei as a predictor and belong to scenario 1
## model fitting - full model, which does not need to take out any predictor
model_lnRR_ess.sei_s1_scaled <- NA
for (i in 1:length(s1_ess.sei_file_scaled)) {
  model_lnRR_ess.sei_s1_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ ess.sei_zscore + year_pub.l, method = "REML", test = "t", data = lnRR[s1_ess.sei_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

# replace sei by var
model_lnRR_ess.var_s1_scaled <- NA
for (i in 1:length(s1_ess.sei_file_scaled)) {
  model_lnRR_ess.var_s1_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ ess.var_zscore + year_pub.l, method = "REML", test = "t", data = lnRR[s1_ess.sei_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model model coefficients and their significance test results 'see.sei' for scenario 2 
model_est_lnRR_ess.sei_s1_scaled <- data.frame(case = s1_ess.sei_file_scaled,
                             es_type = rep("lnRR", length(s1_ess.sei_file_scaled)),
                             beta0 = model_est_lnRR_scaled[model_est_lnRR_scaled$case %in% s1_ess.sei_file_scaled, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_lnRR_scaled[model_est_lnRR_scaled$case %in% s1_ess.sei_file_scaled, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_lnRR_scaled[model_est_lnRR_scaled$case %in% s1_ess.sei_file_scaled, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_lnRR_ess.sei_s1_scaled, function(x) x$beta[1]), # beta0_c in equation 6 -  bias corrected overall mean
                             se_beta0_c = sapply(model_lnRR_ess.sei_s1_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_lnRR_ess.sei_s1_scaled, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_lnRR_ess.sei_s1_scaled, function(x) x$beta[2]), # beta1 in Equation 6 - slope of sampling error 
                             se_beta1 = sapply(model_lnRR_ess.sei_s1_scaled, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_lnRR_ess.sei_s1_scaled, function(x) x$pval[2]), # p value of beta1
                             beta2 = sapply(model_lnRR_ess.sei_s1_scaled, function(x) x$beta[3]), # beta2 in Equation 6 - slope of year 
                             se_beta2 = sapply(model_lnRR_ess.sei_s1_scaled, function(x) x$se[3]), # standard error of beta2
                             pval_beta2 = sapply(model_lnRR_ess.sei_s1_scaled, function(x) x$pval[3]), # p value of beta2
                             beta0_c2 = sapply(model_lnRR_ess.var_s1_scaled, function(x) x$beta[1]), # beta0_c in equation 6 -  bias corrected overall mean
                             se_beta0_c2 = sapply(model_lnRR_ess.var_s1_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_lnRR_ess.var_s1_scaled, function(x) x$pval[1]) # p valuer of beta0_c
                            )


#*****************************scenario 2****************************#
## beta1 has a wrong direction, while beta2 has a correct direction
beta1w_beta2c_lnRR_scaled <- model_est_lnRR_scaled %>% subset(model_est_lnRR_scaled$beta0Tbeta1 < 0 & model_est_lnRR_scaled$beta0Tbeta2 < 0) 
beta1w_beta2c_lnRR_scaled$case 


## reduced model based on scenario 2 - beta1 has a wrong direction, while beta2 has a correct direction
## make a data list which only contains scenario2's data
s2_file_scaled <-  beta1w_beta2c_lnRR_scaled$case
## model fitting -  take out beta1-related predictor (sei or esss.sei) and keep beta2-related predictor (year_pub.l)
## beta1-related predictor is removed, so no need to fit ess.sei where possible
model_lnRR_s2_scaled <- NA
for (i in 1:length(s2_file_scaled)) {
  model_lnRR_s2_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ year_pub.l, method = "REML", test = "t", data = lnRR[s2_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model model coefficients and their significance test results scenario 2
model_est_lnRR_s2_scaled <- data.frame(case = s2_file_scaled,
                             es_type = rep("lnRR", length(s2_file_scaled)),
                             beta0 = model_est_lnRR_scaled[model_est_lnRR_scaled$case %in% s2_file_scaled, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_lnRR_scaled[model_est_lnRR_scaled$case %in% s2_file_scaled, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_lnRR_scaled[model_est_lnRR_scaled$case %in% s2_file_scaled, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_lnRR_s2_scaled, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c = sapply(model_lnRR_s2_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_lnRR_s2_scaled, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = 0, # beta1 in Equation 5 - slope of sampling error: no error term
                             se_beta1 = 0, # standard error of beta1
                             pval_beta1 = 0, # p value of beta1
                             beta2 = sapply(model_lnRR_s2_scaled, function(x) x$beta[2]), # beta2 in Equation 5 - slope of year 
                             se_beta2 = sapply(model_lnRR_s2_scaled, function(x) x$se[2]), # standard error of beta2
                             pval_beta2 = sapply(model_lnRR_s2_scaled, function(x) x$pval[2]), # p value of beta2
                             beta0_c2 = sapply(model_lnRR_s2_scaled, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c2 = sapply(model_lnRR_s2_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_lnRR_s2_scaled, function(x) x$pval[1]) # p valuer of beta0_c
                            )


#*****************************scenario 3****************************#
## beta1 has a correct direction, while beta2 has a wrong direction
beta1c_beta2w_lnRR_scaled <- model_est_lnRR_scaled %>% subset(model_est_lnRR_scaled$beta0Tbeta1 > 0 & model_est_lnRR_scaled$beta0Tbeta2 > 0) 
beta1c_beta2w_lnRR_scaled$case 

## reduced model based on scenario 3 - beta1 has a correct direction, while beta2 has a wrong direction
## make a data list which only contains scenario3's data
s3_file_scaled <-  beta1c_beta2w_lnRR_scaled$case
## fit ess.sei where possible
## subset of sei
s3_sei_file_scaled <- lnRR_filenames[lnRR_filenames %in% s3_file_scaled] # this subset should fit sei as a predictor and belong to scenario 3
## model fitting - keep beta1-related predictor (sei) and take out beta2-related predictor (year_pub.l)
model_lnRR_sei_s3_scaled <- NA
for (i in 1:length(s3_sei_file_scaled)) {
  model_lnRR_sei_s3_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ sei_zscore, method = "REML", test = "t", data = lnRR[s3_sei_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

# replace var by sei
model_lnRR_var_s3_scaled <- NA
for (i in 1:length(s3_sei_file_scaled)) {
  model_lnRR_var_s3_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ var_zscore, method = "REML", test = "t", data = lnRR[s3_sei_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}


## subset of ess.sei
s3_ess.sei_file_scaled <- lnRR_des_filenames[lnRR_des_filenames %in% s3_file_scaled] # this subset should fit ess.sei as a predictor and belong to scenario 3
## model fitting - keep beta1-related predictor (sei) and take out beta2-related predictor (year_pub.l)
model_lnRR_ess.sei_s3_scaled <- NA
for (i in 1:length(s3_ess.sei_file_scaled)) {
  model_lnRR_ess.sei_s3_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ ess.sei_zscore, method = "REML", test = "t", data = lnRR[s3_ess.sei_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

# replace sei by var
model_lnRR_ess.var_s3_scaled <- NA
for (i in 1:length(s3_ess.sei_file_scaled)) {
  model_lnRR_ess.var_s3_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ ess.var_zscore, method = "REML", test = "t", data = lnRR[s3_ess.sei_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model model coefficients and their significance test results for 'sei' in scenario 3
model_est_lnRR_sei_s3_scaled <- data.frame(case = s3_sei_file_scaled,
                             es_type = rep("lnRR", length(s3_sei_file_scaled)),
                             beta0 = model_est_lnRR_scaled[model_est_lnRR_scaled$case %in% s3_sei_file_scaled, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_lnRR_scaled[model_est_lnRR_scaled$case %in% s3_sei_file_scaled, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_lnRR_scaled[model_est_lnRR_scaled$case %in% s3_sei_file_scaled, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_lnRR_sei_s3_scaled, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c = sapply(model_lnRR_sei_s3_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_lnRR_sei_s3_scaled, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_lnRR_sei_s3_scaled, function(x) x$beta[2]), # beta1 in Equation 5 - slope of sampling error 
                             se_beta1 = sapply(model_lnRR_sei_s3_scaled, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_lnRR_sei_s3_scaled, function(x) x$pval[2]), # p value of beta1
                             beta2 = 0, # beta2 in Equation 5 - slope of year: no year term 
                             se_beta2 = 0, # standard error of beta2
                             pval_beta2 = 0, # p value of beta2
                             beta0_c2 = sapply(model_lnRR_var_s3_scaled, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c2 = sapply(model_lnRR_var_s3_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_lnRR_var_s3_scaled, function(x) x$pval[1]) # p valuer of beta0_c
                            )

## extract model model coefficients and their significance test results for 'ess.sei' in scenario 3
model_est_lnRR_ess.sei_s3_scaled <- data.frame(case = s3_ess.sei_file_scaled,
                             es_type = rep("lnRR", length(s3_ess.sei_file_scaled)),
                             beta0 = model_est_lnRR_scaled[model_est_lnRR_scaled$case %in% s3_ess.sei_file_scaled, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_lnRR_scaled[model_est_lnRR_scaled$case %in% s3_ess.sei_file_scaled, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_lnRR_scaled[model_est_lnRR_scaled$case %in% s3_ess.sei_file_scaled, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_lnRR_ess.sei_s3_scaled, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c = sapply(model_lnRR_ess.sei_s3_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_lnRR_ess.sei_s3_scaled, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_lnRR_ess.sei_s3_scaled, function(x) x$beta[2]), # beta1 in Equation 5 - slope of sampling error 
                             se_beta1 = sapply(model_lnRR_ess.sei_s3_scaled, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_lnRR_ess.sei_s3_scaled, function(x) x$pval[2]), # p value of beta1
                             beta2 = 0, # beta2 in Equation 5 - slope of year: no year term 
                             se_beta2 = 0, # standard error of beta2
                             pval_beta2 = 0, # p value of beta2
                             beta0_c2 = sapply(model_lnRR_ess.sei_s3_scaled, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c2 = sapply(model_lnRR_ess.sei_s3_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_lnRR_ess.sei_s3_scaled, function(x) x$pval[1]) # p valuer of beta0_c
                            )


#*****************************scenario 4****************************#
## both beta1 and beta2 has a wrong direction
beta1w_beta2w_lnRR_scaled <- model_est_lnRR_scaled %>% subset(model_est_lnRR_scaled$beta0Tbeta1 < 0 & model_est_lnRR_scaled$beta0Tbeta2 > 0) 
beta1w_beta2w_lnRR_scaled$case 

## reduced model based on scenario 4 - beta1 has a wrong direction and beta2 has a wrong direction 
## this reduced model needs to take out both of the two predictors (sei and pub_year.l). This is equivalent to a null model (intercept-only model), which is used to estimate (uncorrected) meta-analytic overall mean

## make a data list which only contains scenario4's data
s4_file_scaled <-  beta1w_beta2w_lnRR_scaled$case
## no need to subset sei and ess.sei because scenario4 fits a null model (without any predictor)
## model fitting - take out both beta1-related predictor (sei or ess.sei) and  beta2-related predictor (year_pub.l)
model_lnRR_s4_scaled <- NA
for (i in 1:length(s4_file_scaled)) {
  model_lnRR_s4_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), method = "REML", test = "t", data = lnRR[s4_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model model coefficients and their significance test results scenario 4
model_est_lnRR_s4_scaled <- data.frame(case = s4_file_scaled,
                             es_type = rep("lnRR", length(s4_file_scaled)),
                             beta0 = model_est_lnRR_scaled[model_est_lnRR_scaled$case %in% s4_file_scaled, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean # or sapply(model_lnRR_s4_scaled, function(x) x$beta[1])
                             se_beta0 = model_est_lnRR_scaled[model_est_lnRR_scaled$case %in% s4_file_scaled, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_lnRR_scaled[model_est_lnRR_scaled$case %in% s4_file_scaled, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_lnRR_s4_scaled, function(x) x$beta[1]), # beta0_c in equation 1 -  bias corrected overall mean: this is equal to uncorrected overall mean (i.e., beta0)
                             se_beta0_c = sapply(model_lnRR_s4_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_lnRR_s4_scaled, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = 0, # beta1 in Equation 1 - slope of sampling error: no error term
                             se_beta1 = 0, # standard error of beta1
                             pval_beta1 = 0, # p value of beta1
                             beta2 = 0, # beta2 in Equation 1 - slope of year 
                             se_beta2 = 0, # standard error of beta2
                             pval_beta2 = 0, # p value of beta2
                             beta0_c2 = sapply(model_lnRR_s4_scaled, function(x) x$beta[1]), # beta0_c in equation 1 -  bias corrected overall mean: this is equal to uncorrected overall mean (i.e., beta0)
                             se_beta0_c2 = sapply(model_lnRR_s4_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_lnRR_s4_scaled, function(x) x$pval[1]) # p valuer of beta0_c
                            )

```


#### SMD

```{r}
#*************************************************************************#
#             Identify the presence of publication bias 
#*************************************************************************#

## check the significance and direction of model regressions from each meta-analysis to identify whether it presents a small-study effect or decline effect

## first to create a dataframe containing full-model's parameter estimates
### combine two types full-models (with sei and ess.sei as a predictor, respectively)
### SMD
model_SMD_pb_scaled <- append(model_SMD_sei.year_scaled, model_SMD_ess.sei.year_scaled)

### extract model model coefficients and their significance test results
model_est_SMD_scaled <- data.frame(case = names(SMD),
                             es_type = rep("SMD", length(SMD)),
                             beta0 = sapply(model_SMD_scaled, function(x) x$beta), # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = sapply(model_SMD_scaled, function(x) x$se), # standard error of beta0
                             pval_beta0 = sapply(model_SMD_scaled, function(x) x$pval), # p value of beta0
                             beta0_c = sapply(model_SMD_pb_scaled, function(x) x$beta[1]), # beta0_c in equation 2 -  bias corrected overall mean
                             se_beta0_c = sapply(model_SMD_pb_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_SMD_pb_scaled, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_SMD_pb_scaled, function(x) x$beta[2]), # beta1 in Equation 2 - slope of sampling error 
                             se_beta1 = sapply(model_SMD_pb_scaled, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_SMD_pb_scaled, function(x) x$pval[2]), # p value of beta1
                             beta2 = sapply(model_SMD_pb_scaled, function(x) x$beta[3]), # beta2 in Equation 2 - slope of year 
                             se_beta2 = sapply(model_SMD_pb_scaled, function(x) x$se[3]), # standard error of beta2
                             pval_beta2 = sapply(model_SMD_pb_scaled, function(x) x$pval[3]) # p value of beta2
                            )

## we next aim to identify the presence of the small-study effect and decline effect for each meta-analysis. We also figure out the wrong directions of slope, which will be used to inform the parameterization of reduced models

## our rational is: for an effect that is expected to be positive, a small study effect and decline effect would be expressed in a positive value of beta1 and negative value of beta2, respectively. In such a case, a slope (beta1] or beta2)) with opposing direction (unexpected sign) indicates no detectable publication bias and subsequently does not require correction for such a bias

## we use the product of beta0 and beta1 (i.e., beta*beta1) as the signal, that is, if beta0*beta1 is positive, it indicates the examined meta-analysis has a small-study effect (beta1 is in a correct direction)

## of relevance, when the value of beta0*beta2 is negative, the examined meta-analysis has a decline effect (beta 2 is in a correct direction)

## so we first to create two new columns to contain the two products:  beta0*beta1 and beta0*beta2

## SMD
model_est_SMD_scaled[15:16] <- data.frame(beta0Tbeta1 = model_est_SMD_scaled$beta0 * model_est_SMD_scaled$beta1, beta0Tbeta2 = model_est_SMD_scaled$beta0 * model_est_SMD_scaled$beta2) # model_est_SMD has 14 column (ncol(model_est_SMD)), so we add columns 15 and 16

## visual check
model_est_SMD_scaled

## identify the small-study effect - significant beta1 with correct sign
sse_SMD_scaled <- model_est_SMD_scaled %>% subset(model_est_SMD_scaled$pval_beta1 < 0.05 & model_est_SMD_scaled$beta0Tbeta1 > 0)
## check which meta-analyses have small-study effects
sse_SMD_scaled$case 



## identify the decline effect - significant beta2 with correct sign 
de_SMD_scaled <- model_est_SMD_scaled %>% subset(model_est_SMD_scaled$pval_beta2 < 0.05 & model_est_SMD_scaled$beta0Tbeta2 < 0)
## check which meta-analyses have decline effects
de_SMD_scaled$case 

## identify the concurrence of the small-study effect and decline effect
sse_de_SMD_scaled <- model_est_SMD_scaled %>% subset(model_est_SMD_scaled$pval_beta1 < 0.05 & model_est_SMD_scaled$beta0Tbeta1 > 0 & model_est_SMD_scaled$pval_beta2 < 0.05 & model_est_SMD_scaled$beta0Tbeta2 < 0) 
sse_de_SMD_scaled$case 



#*************************************************************************#
#        Multilevel models to estimate bias-corrected effect
#*************************************************************************#

#*****************************scenario 1****************************#
## both beta1 and beta2 has a correct direction
beta1c_beta2c_SMD_scaled <- model_est_SMD_scaled %>% subset(model_est_SMD_scaled$beta0Tbeta1 > 0 & model_est_SMD_scaled$beta0Tbeta2 < 0) 
beta1c_beta2c_SMD_scaled$case  

## if a model slope (beta1 and beta2) has a wrong direction, we need to take out it when fitting model to estimate the bias-corrected mean
## in scenario 1, both of the two slopes have a correct direction, we use can use full model directly, no need to use take out any predictor

## full model based on scenario 1 - both beta1 and beta2 has a correct direction
## make a data list which only contains scenario1's data
s1_file_scaled <- beta1c_beta2c_SMD_scaled$case
## fit ess.sei where possible
## subset of sei
s1_sei_file_scaled <- SMD_filenames[SMD_filenames %in% s1_file_scaled] # this subset should use sei as a predictor and belong to scenario 1
## model fitting - fit a full model
model_SMD_sei_s1_scaled <- NA
for (i in 1:length(s1_sei_file_scaled)) {
  model_SMD_sei_s1_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ sei_zscore + year_pub.l, method = "REML", test = "t", data = SMD[s1_sei_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## replace sei by var
model_SMD_var_s1_scaled <- NA
for (i in 1:length(s1_sei_file_scaled)) {
  model_SMD_var_s1_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ var_zscore + year_pub.l, method = "REML", test = "t", data = SMD[s1_sei_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model model coefficients and their significance test results for 'sei' in scenario 1
model_est_SMD_sei_s1_scaled <- data.frame(case = s1_sei_file_scaled,
                             es_type = rep("SMD", length(s1_sei_file_scaled)),
                             beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s1_sei_file_scaled, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s1_sei_file_scaled, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s1_sei_file_scaled, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_SMD_sei_s1_scaled, function(x) x$beta[1]), # beta0_c in equation 6 -  bias corrected overall mean
                             se_beta0_c = sapply(model_SMD_sei_s1_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_SMD_sei_s1_scaled, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_SMD_sei_s1_scaled, function(x) x$beta[2]), # beta1 in Equation 6 - slope of sampling error 
                             se_beta1 = sapply(model_SMD_sei_s1_scaled, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_SMD_sei_s1_scaled, function(x) x$pval[2]), # p value of beta1
                             beta2 = sapply(model_SMD_sei_s1_scaled, function(x) x$beta[3]), # beta2 in Equation 6 - slope of year 
                             se_beta2 = sapply(model_SMD_sei_s1_scaled, function(x) x$se[3]), # standard error of beta2
                             pval_beta2 = sapply(model_SMD_sei_s1_scaled, function(x) x$pval[3]), # p value of beta2
                             beta0_c2 = sapply(model_SMD_var_s1_scaled, function(x) x$beta[1]), # beta0_c in equation 6 -  bias corrected overall mean
                             se_beta0_c2 = sapply(model_SMD_var_s1_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_SMD_var_s1_scaled, function(x) x$pval[1]) # p valuer of beta0_c
                            )



## subset of ess.sei
s1_ess.sei_file_scaled <- SMD_des_filenames[SMD_des_filenames %in% s1_file_scaled] # this subset should fit ess.sei as a predictor and belong to scenario 1
## model fitting - full model, which does not need to take out any predictor
model_SMD_ess.sei_s1_scaled <- NA
for (i in 1:length(s1_ess.sei_file_scaled)) {
  model_SMD_ess.sei_s1_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ ess.sei_zscore + year_pub.l, method = "REML", test = "t", data = SMD[s1_ess.sei_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## replace sei by var
model_SMD_ess.var_s1_scaled <- NA
for (i in 1:length(s1_ess.sei_file_scaled)) {
  model_SMD_ess.var_s1_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ ess.var_zscore + year_pub.l, method = "REML", test = "t", data = SMD[s1_ess.sei_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model model coefficients and their significance test results 'see.sei' for scenario 2 
model_est_SMD_ess.sei_s1_scaled <- data.frame(case = s1_ess.sei_file_scaled,
                             es_type = rep("SMD", length(s1_ess.sei_file_scaled)),
                             beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s1_ess.sei_file_scaled, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s1_ess.sei_file_scaled, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s1_ess.sei_file_scaled, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_SMD_ess.sei_s1_scaled, function(x) x$beta[1]), # beta0_c in equation 6 -  bias corrected overall mean
                             se_beta0_c = sapply(model_SMD_ess.sei_s1_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_SMD_ess.sei_s1_scaled, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_SMD_ess.sei_s1_scaled, function(x) x$beta[2]), # beta1 in Equation 6 - slope of sampling error 
                             se_beta1 = sapply(model_SMD_ess.sei_s1_scaled, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_SMD_ess.sei_s1_scaled, function(x) x$pval[2]), # p value of beta1
                             beta2 = sapply(model_SMD_ess.sei_s1_scaled, function(x) x$beta[3]), # beta2 in Equation 6 - slope of year 
                             se_beta2 = sapply(model_SMD_ess.sei_s1_scaled, function(x) x$se[3]), # standard error of beta2
                             pval_beta2 = sapply(model_SMD_ess.sei_s1_scaled, function(x) x$pval[3]), # p value of beta2
                             beta0_c2 = sapply(model_SMD_ess.var_s1_scaled, function(x) x$beta[1]), # beta0_c in equation 6 -  bias corrected overall mean
                             se_beta0_c2 = sapply(model_SMD_ess.var_s1_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_SMD_ess.var_s1_scaled, function(x) x$pval[1]) # p valuer of beta0_c
                            )


#*****************************scenario 2****************************#
## beta1 has a wrong direction, while beta2 has a correct direction
beta1w_beta2c_SMD_scaled <- model_est_SMD_scaled %>% subset(model_est_SMD_scaled$beta0Tbeta1 < 0 & model_est_SMD_scaled$beta0Tbeta2 < 0) 
beta1w_beta2c_SMD_scaled$case 

## reduced model based on scenario 2 - beta1 has a wrong direction, while beta2 has a correct direction
## make a data list which only contains scenario2's data
s2_file_scaled <-  beta1w_beta2c_SMD_scaled$case
## model fitting -  take out beta1-related predictor (sei or esss.sei) and keep beta2-related predictor (year_pub.l)
## beta1-related predictor is removed, so no need to fit ess.sei where possible
model_SMD_s2_scaled <- NA
for (i in 1:length(s2_file_scaled)) {
  model_SMD_s2_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ year_pub.l, method = "REML", test = "t", data = SMD[s2_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model model coefficients and their significance test results scenario 2
model_est_SMD_s2_scaled <- data.frame(case = s2_file_scaled,
                             es_type = rep("SMD", length(s2_file_scaled)),
                             beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s2_file_scaled, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s2_file_scaled, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s2_file_scaled, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_SMD_s2_scaled, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c = sapply(model_SMD_s2_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_SMD_s2_scaled, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = 0, # beta1 in Equation 5 - slope of sampling error: no error term
                             se_beta1 = 0, # standard error of beta1
                             pval_beta1 = 0, # p value of beta1
                             beta2 = sapply(model_SMD_s2_scaled, function(x) x$beta[2]), # beta2 in Equation 5 - slope of year 
                             se_beta2 = sapply(model_SMD_s2_scaled, function(x) x$se[2]), # standard error of beta2
                             pval_beta2 = sapply(model_SMD_s2_scaled, function(x) x$pval[2]), # p value of beta2
                             beta0_c2 = sapply(model_SMD_s2_scaled, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c2 = sapply(model_SMD_s2_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_SMD_s2_scaled, function(x) x$pval[1]) # p valuer of beta0_c
                            )


#*****************************scenario 3****************************#
## beta1 has a correct direction, while beta2 has a wrong direction
beta1c_beta2w_SMD_scaled <- model_est_SMD_scaled %>% subset(model_est_SMD_scaled$beta0Tbeta1 > 0 & model_est_SMD_scaled$beta0Tbeta2 > 0) 
beta1c_beta2w_SMD_scaled$case 

## reduced model based on scenario 3 - beta1 has a correct direction, while beta2 has a wrong direction
## make a data list which only contains scenario3's data
s3_file_scaled <-  beta1c_beta2w_SMD_scaled$case
## fit ess.sei where possible
## subset of sei
s3_sei_file_scaled <- SMD_filenames[SMD_filenames %in% s3_file_scaled] # this subset should fit sei as a predictor and belong to scenario 3
## model fitting - keep beta1-related predictor (sei) and take out beta2-related predictor (year_pub.l)
model_SMD_sei_s3_scaled <- NA
for (i in 1:length(s3_sei_file_scaled)) {
  model_SMD_sei_s3_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ sei_zscore, method = "REML", test = "t", data = SMD[s3_sei_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## replace sei by vari=
model_SMD_var_s3_scaled <- NA
for (i in 1:length(s3_sei_file_scaled)) {
  model_SMD_var_s3_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ var_zscore, method = "REML", test = "t", data = SMD[s3_sei_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## subset of ess.sei
s3_ess.sei_file_scaled <- SMD_des_filenames[SMD_des_filenames %in% s3_file_scaled] # this subset should fit ess.sei as a predictor and belong to scenario 3
## model fitting - keep beta1-related predictor (sei) and take out beta2-related predictor (year_pub.l)
model_SMD_ess.sei_s3_scaled <- NA
for (i in 1:length(s3_ess.sei_file_scaled)) {
  model_SMD_ess.sei_s3_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ ess.sei_zscore, method = "REML", test = "t", data = SMD[s3_ess.sei_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

# replace sei by var
model_SMD_ess.var_s3_scaled <- NA
for (i in 1:length(s3_ess.sei_file_scaled)) {
  model_SMD_ess.var_s3_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ ess.var_zscore, method = "REML", test = "t", data = SMD[s3_ess.sei_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model model coefficients and their significance test results for 'sei' in scenario 3
model_est_SMD_sei_s3_scaled <- data.frame(case = s3_sei_file_scaled,
                             es_type = rep("SMD", length(s3_sei_file_scaled)),
                             beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s3_sei_file_scaled, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s3_sei_file_scaled, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s3_sei_file_scaled, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_SMD_sei_s3_scaled, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c = sapply(model_SMD_sei_s3_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_SMD_sei_s3_scaled, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_SMD_sei_s3_scaled, function(x) x$beta[2]), # beta1 in Equation 5 - slope of sampling error 
                             se_beta1 = sapply(model_SMD_sei_s3_scaled, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_SMD_sei_s3_scaled, function(x) x$pval[2]), # p value of beta1
                             beta2 = 0, # beta2 in Equation 5 - slope of year: no year term 
                             se_beta2 = 0, # standard error of beta2
                             pval_beta2 = 0, # p value of beta2
                             beta0_c2 = sapply(model_SMD_var_s3_scaled, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c2 = sapply(model_SMD_var_s3_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_SMD_var_s3_scaled, function(x) x$pval[1]) # p valuer of beta0_c
                            )

## extract model model coefficients and their significance test results for 'ess.sei' in scenario 3
model_est_SMD_ess.sei_s3_scaled <- data.frame(case = s3_ess.sei_file_scaled,
                             es_type = rep("SMD", length(s3_ess.sei_file_scaled)),
                             beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s3_ess.sei_file_scaled, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s3_ess.sei_file_scaled, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s3_ess.sei_file_scaled, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_SMD_ess.sei_s3_scaled, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c = sapply(model_SMD_ess.sei_s3_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_SMD_ess.sei_s3_scaled, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_SMD_ess.sei_s3_scaled, function(x) x$beta[2]), # beta1 in Equation 5 - slope of sampling error 
                             se_beta1 = sapply(model_SMD_ess.sei_s3_scaled, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_SMD_ess.sei_s3_scaled, function(x) x$pval[2]), # p value of beta1
                             beta2 = 0, # beta2 in Equation 5 - slope of year: no year term 
                             se_beta2 = 0, # standard error of beta2
                             pval_beta2 = 0, # p value of beta2
                             beta0_c2 = sapply(model_SMD_ess.var_s3_scaled, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c2 = sapply(model_SMD_ess.var_s3_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_SMD_ess.var_s3_scaled, function(x) x$pval[1]) # p valuer of beta0_c
                            )


#*****************************scenario 4****************************#
## both beta1 and beta2 has a wrong direction
beta1w_beta2w_SMD_scaled <- model_est_SMD_scaled %>% subset(model_est_SMD_scaled$beta0Tbeta1 < 0 & model_est_SMD_scaled$beta0Tbeta2 > 0) 
beta1w_beta2w_SMD_scaled$case   

## reduced model based on scenario 4 - beta1 has a wrong direction and beta2 has a wrong direction 
## this reduced model needs to take out both of the two predictors (sei and pub_year.l). This is equivalent to a null model (intercept-only model), which is used to estimate (uncorrected) meta-analytic overall mean

## make a data list which only contains scenario4's data
s4_file_scaled <-  beta1w_beta2w_SMD_scaled$case
## no need to subset sei and ess.sei because scenario4 fits a null model (without any predictor)
## model fitting - take out both beta1-related predictor (sei or ess.sei) and  beta2-related predictor (year_pub.l)
model_SMD_s4_scaled <- NA
for (i in 1:length(s4_file_scaled)) {
  model_SMD_s4_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), method = "REML", test = "t", data = SMD[s4_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model model coefficients and their significance test results scenario 4
model_est_SMD_s4_scaled <- data.frame(case = s4_file_scaled,
                             es_type = rep("SMD", length(s4_file_scaled)),
                             beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s4_file_scaled, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean # or sapply(model_SMD_s4_scaled, function(x) x$beta[1])
                             se_beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s4_file_scaled, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s4_file_scaled, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_SMD_s4_scaled, function(x) x$beta[1]), # beta0_c in equation 1 -  bias corrected overall mean: this is equal to uncorrected overall mean (i.e., beta0)
                             se_beta0_c = sapply(model_SMD_s4_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_SMD_s4_scaled, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = 0, # beta1 in Equation 1 - slope of sampling error: no error term
                             se_beta1 = 0, # standard error of beta1
                             pval_beta1 = 0, # p value of beta1
                             beta2 = 0, # beta2 in Equation 1 - slope of year 
                             se_beta2 = 0, # standard error of beta2
                             pval_beta2 = 0, # p value of beta2
                             beta0_c2 = sapply(model_SMD_s4_scaled, function(x) x$beta[1]), # beta0_c in equation 1 -  bias corrected overall mean: this is equal to uncorrected overall mean (i.e., beta0)
                             se_beta0_c2 = sapply(model_SMD_s4_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_SMD_s4_scaled, function(x) x$pval[1]) # p valuer of beta0_c
                            )

```


#### Zr
```{r}
#*************************************************************************#
#             Identify the presence of publication bias 
#*************************************************************************#

## check the significance and direction of model regressions from each meta-analysis to identify whether it presents a small-study effect or decline effect

## first to create a dataframe containing full-model's parameter estimates
### combine two types full-models (with sei and ess.sei as a predictor, respectively)
### Zr
model_Zr_pb_scaled <- model_Zr_sei.year_scaled
### extract model model coefficients and their significance test results
model_est_Zr_scaled <- data.frame(case = names(Zr),
                             es_type = rep("Zr", length(Zr)),
                             beta0 = sapply(model_Zr_scaled, function(x) x$beta), # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = sapply(model_Zr_scaled, function(x) x$se), # standard error of beta0
                             pval_beta0 = sapply(model_Zr_scaled, function(x) x$pval), # p value of beta0
                             beta0_c = sapply(model_Zr_pb_scaled, function(x) x$beta[1]), # beta0_c in equation 2 -  bias corrected overall mean
                             se_beta0_c = sapply(model_Zr_pb_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_Zr_pb_scaled, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_Zr_pb_scaled, function(x) x$beta[2]), # beta1 in Equation 2 - slope of sampling error 
                             se_beta1 = sapply(model_Zr_pb_scaled, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_Zr_pb_scaled, function(x) x$pval[2]), # p value of beta1
                             beta2 = sapply(model_Zr_pb_scaled, function(x) x$beta[3]), # beta2 in Equation 2 - slope of year 
                             se_beta2 = sapply(model_Zr_pb_scaled, function(x) x$se[3]), # standard error of beta2
                             pval_beta2 = sapply(model_Zr_pb_scaled, function(x) x$pval[3]) # p value of beta2
                            )

## we next aim to identify the presence of the small-study effect and decline effect for each meta-analysis. We also figure out the wrong directions of slope, which will be used to inform the parameterization of reduced models

## our rational is: for an effect that is expected to be positive, a small study effect and decline effect would be expressed in a positive value of beta1 and negative value of beta2, respectively. In such a case, a slope (beta1] or beta2)) with opposing direction (unexpected sign) indicates no detectable publication bias and subsequently does not require correction for such a bias

## we use the product of beta0 and beta1 (i.e., beta*beta1) as the signal, that is, if beta0*beta1 is positive, it indicates the examined meta-analysis has a small-study effect (beta1 is in a correct direction)

## of relevance, when the value of beta0*beta2 is negative, the examined meta-analysis has a decline effect (beta 2 is in a correct direction)

## so we first to create two new columns to contain the two products:  beta0*beta1 and beta0*beta2

## Zr
model_est_Zr_scaled[15:16] <- data.frame(beta0Tbeta1 = model_est_Zr_scaled$beta0 * model_est_Zr_scaled$beta1, beta0Tbeta2 = model_est_Zr_scaled$beta0 * model_est_Zr_scaled$beta2) # model_est_Zr has 14 column (ncol(model_est_Zr)), so we add columns 15 and 16

## visual check
model_est_Zr_scaled

## identify the small-study effect - significant beta1 with correct sign
sse_Zr_scaled <- model_est_Zr_scaled %>% subset(model_est_Zr_scaled$pval_beta1 < 0.05 & model_est_Zr_scaled$beta0Tbeta1 > 0)
## check which meta-analyses have small-study effects
sse_Zr_scaled$case 



## identify the decline effect - significant beta2 with correct sign 
de_Zr_scaled <- model_est_Zr_scaled %>% subset(model_est_Zr_scaled$pval_beta2 < 0.05 & model_est_Zr_scaled$beta0Tbeta2 < 0)
## check which meta-analyses have decline effects
de_Zr_scaled$case 

## identify the concurrence of the small-study effect and decline effect
sse_de_Zr_scaled <- model_est_Zr_scaled %>% subset(model_est_Zr_scaled$pval_beta1 < 0.05 & model_est_Zr_scaled$beta0Tbeta1 > 0 & model_est_Zr_scaled$pval_beta2 < 0.05 & model_est_Zr_scaled$beta0Tbeta2 < 0) 
sse_de_Zr_scaled$case



#*************************************************************************#
#        Multilevel models to estimate bias-corrected effect
#*************************************************************************#

#*****************************scenario 1****************************#
## both beta1 and beta2 has a correct direction
beta1c_beta2c_Zr_scaled <- model_est_Zr_scaled %>% subset(model_est_Zr_scaled$beta0Tbeta1 > 0 & model_est_Zr_scaled$beta0Tbeta2 < 0) 
beta1c_beta2c_Zr_scaled$case 

## if a model slope (beta1 and beta2) has a wrong direction, we need to take out it when fitting model to estimate the bias-corrected mean
## in scenario 1, both of the two slopes have a correct direction, we use can use full model directly, no need to use take out any predictor

## full model based on scenario 1 - both beta1 and beta2 has a correct direction
## make a data list which only contains scenario1's data
s1_file_scaled <- beta1c_beta2c_Zr_scaled$case
## fit ess.sei where possible
## subset of sei
s1_file_scaled <- Zr_filenames[Zr_filenames %in% s1_file_scaled] # for Zr, do not need to subset sei and ess.sei
## model fitting - fit a full model
model_Zr_s1_scaled <- NA
for (i in 1:length(s1_file_scaled)) {
  model_Zr_s1_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ sei_zscore + year_pub.l, method = "REML", test = "t", data = Zr[s1_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

# replace sei by var
model_Zr_var.s1_scaled <- NA
for (i in 1:length(s1_file_scaled)) {
  model_Zr_var.s1_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ var_zscore + year_pub.l, method = "REML", test = "t", data = Zr[s1_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model coefficients and their significance test results for 'sei' in scenario 1
model_est_Zr_s1_scaled <- data.frame(case = s1_file_scaled,
                             es_type = rep("Zr", length(s1_file_scaled)),
                             beta0 = model_est_Zr_scaled[model_est_Zr_scaled$case %in% s1_file_scaled, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_Zr_scaled[model_est_Zr_scaled$case %in% s1_file_scaled, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_Zr_scaled[model_est_Zr_scaled$case %in% s1_file_scaled, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_Zr_s1_scaled, function(x) x$beta[1]), # beta0_c in equation 6 -  bias corrected overall mean
                             se_beta0_c = sapply(model_Zr_s1_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_Zr_s1_scaled, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_Zr_s1_scaled, function(x) x$beta[2]), # beta1 in Equation 6 - slope of sampling error 
                             se_beta1 = sapply(model_Zr_s1_scaled, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_Zr_s1_scaled, function(x) x$pval[2]), # p value of beta1
                             beta2 = sapply(model_Zr_s1_scaled, function(x) x$beta[3]), # beta2 in Equation 6 - slope of year 
                             se_beta2 = sapply(model_Zr_s1_scaled, function(x) x$se[3]), # standard error of beta2
                             pval_beta2 = sapply(model_Zr_s1_scaled, function(x) x$pval[3]), # p value of beta2
                             beta0_c2 = sapply(model_Zr_var.s1_scaled, function(x) x$beta[1]), # beta0_c in equation 6 -  bias corrected overall mean
                             se_beta0_c2 = sapply(model_Zr_var.s1_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_Zr_var.s1_scaled, function(x) x$pval[1]) # p valuer of beta0_c
                            )


#*****************************scenario 2****************************#
## beta1 has a wrong direction, while beta2 has a correct direction
beta1w_beta2c_Zr_scaled <- model_est_Zr_scaled %>% subset(model_est_Zr_scaled$beta0Tbeta1 < 0 & model_est_Zr_scaled$beta0Tbeta2 < 0) 
beta1w_beta2c_Zr_scaled$case 

## reduced model based on scenario 2 - beta1 has a wrong direction, while beta2 has a correct direction
## make a data list which only contains scenario2's data
s2_file_scaled <-  beta1w_beta2c_Zr_scaled$case
## model fitting -  take out beta1-related predictor (sei or esss.sei) and keep beta2-related predictor (year_pub.l)
## beta1-related predictor is removed, so no need to fit ess.sei where possible
model_Zr_s2_scaled <- NA
for (i in 1:length(s2_file_scaled)) {
  model_Zr_s2_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ year_pub.l, method = "REML", test = "t", data = Zr[s2_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model model coefficients and their significance test results scenario 2
model_est_Zr_s2_scaled <- data.frame(case = s2_file_scaled,
                             es_type = rep("Zr", length(s2_file_scaled)),
                             beta0 = model_est_Zr_scaled[model_est_Zr_scaled$case %in% s2_file_scaled, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_Zr_scaled[model_est_Zr_scaled$case %in% s2_file_scaled, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_Zr_scaled[model_est_Zr_scaled$case %in% s2_file_scaled, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_Zr_s2_scaled, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c = sapply(model_Zr_s2_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_Zr_s2_scaled, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = 0, # beta1 in Equation 5 - slope of sampling error: no error term
                             se_beta1 = 0, # standard error of beta1
                             pval_beta1 = 0, # p value of beta1
                             beta2 = sapply(model_Zr_s2_scaled, function(x) x$beta[2]), # beta2 in Equation 5 - slope of year 
                             se_beta2 = sapply(model_Zr_s2_scaled, function(x) x$se[2]), # standard error of beta2
                             pval_beta2 = sapply(model_Zr_s2_scaled, function(x) x$pval[2]), # p value of beta2
                             beta0_c2 = sapply(model_Zr_s2_scaled, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c2 = sapply(model_Zr_s2_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_Zr_s2_scaled, function(x) x$pval[1]) # p valuer of beta0_c
                            )


#*****************************scenario 3****************************#
## beta1 has a correct direction, while beta2 has a wrong direction
beta1c_beta2w_Zr_scaled <- model_est_Zr_scaled %>% subset(model_est_Zr_scaled$beta0Tbeta1 > 0 & model_est_Zr_scaled$beta0Tbeta2 > 0) 
beta1c_beta2w_Zr_scaled$case 

## reduced model based on scenario 3 - beta1 has a correct direction, while beta2 has a wrong direction
## make a data list which only contains scenario3's data
s3_file_scaled <- beta1c_beta2w_Zr_scaled$case
## Zr does not need to subset sei and ess.sei
s3_file_scaled <- Zr_filenames[Zr_filenames %in% s3_file_scaled] 
## model fitting - keep beta1-related predictor (sei) and take out beta2-related predictor (year_pub.l)
model_Zr_s3_scaled <- NA
for (i in 1:length(s3_file_scaled)) {
  model_Zr_s3_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ sei_zscore, method = "REML", test = "t", data = Zr[s3_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

# replace sei by var
model_Zr_var.s3_scaled <- NA
for (i in 1:length(s3_file_scaled)) {
  model_Zr_var.s3_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ var_zscore, method = "REML", test = "t", data = Zr[s3_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model coefficients and their significance test results for 'sei' in scenario 3
model_est_Zr_s3_scaled <- data.frame(case = s3_file_scaled,
                             es_type = rep("Zr", length(s3_file_scaled)),
                             beta0 = model_est_Zr_scaled[model_est_Zr_scaled$case %in% s3_file_scaled, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_Zr_scaled[model_est_Zr_scaled$case %in% s3_file_scaled, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_Zr_scaled[model_est_Zr_scaled$case %in% s3_file_scaled, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_Zr_s3_scaled, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c = sapply(model_Zr_s3_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_Zr_s3_scaled, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_Zr_s3_scaled, function(x) x$beta[2]), # beta1 in Equation 5 - slope of sampling error 
                             se_beta1 = sapply(model_Zr_s3_scaled, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_Zr_s3_scaled, function(x) x$pval[2]), # p value of beta1
                             beta2 = 0, # beta2 in Equation 5 - slope of year: no year term 
                             se_beta2 = 0, # standard error of beta2
                             pval_beta2 = 0, # p value of beta2
                             beta0_c2 = sapply(model_Zr_var.s3_scaled, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c2 = sapply(model_Zr_var.s3_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_Zr_var.s3_scaled, function(x) x$pval[1]) # p valuer of beta0_c
                            )



#*****************************scenario 4****************************#
## both beta1 and beta2 has a wrong direction
beta1w_beta2w_Zr_scaled <- model_est_Zr_scaled %>% subset(model_est_Zr_scaled$beta0Tbeta1 < 0 & model_est_Zr_scaled$beta0Tbeta2 > 0) 
beta1w_beta2w_Zr_scaled$case 

## reduced model based on scenario 4 - beta1 has a wrong direction and beta2 has a wrong direction 
## this reduced model needs to take out both of the two predictors (sei and pub_year.l). This is equivalent to a null model (intercept-only model), which is used to estimate (uncorrected) meta-analytic overall mean

## make a data list which only contains scenario4's data
s4_file_scaled <-  beta1w_beta2w_Zr_scaled$case
## no need to subset sei and ess.sei because scenario4 fits a null model (without any predictor)
## model fitting - take out both beta1-related predictor (sei or ess.sei) and  beta2-related predictor (year_pub.l)
model_Zr_s4_scaled <- NA
for (i in 1:length(s4_file_scaled)) {
  model_Zr_s4_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), method = "REML", test = "t", data = Zr[s4_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model coefficients and their significance test results for scenario 4
model_est_Zr_s4_scaled <- data.frame(case = s4_file_scaled,
                             es_type = rep("Zr", length(s4_file_scaled)),
                             beta0 = model_est_Zr_scaled[model_est_Zr_scaled$case %in% s4_file_scaled, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean # or sapply(model_Zr_s4_scaled, function(x) x$beta[1])
                             se_beta0 = model_est_Zr_scaled[model_est_Zr_scaled$case %in% s4_file_scaled, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_Zr_scaled[model_est_Zr_scaled$case %in% s4_file_scaled, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_Zr_s4_scaled, function(x) x$beta[1]), # beta0_c in equation 1 -  bias corrected overall mean: this is equal to uncorrected overall mean (i.e., beta0)
                             se_beta0_c = sapply(model_Zr_s4_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_Zr_s4_scaled, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = 0, # beta1 in Equation 1 - slope of sampling error: no error term
                             se_beta1 = 0, # standard error of beta1
                             pval_beta1 = 0, # p value of beta1
                             beta2 = 0, # beta2 in Equation 1 - slope of year 
                             se_beta2 = 0, # standard error of beta2
                             pval_beta2 = 0, # p value of beta2
                             beta0_c2 = sapply(model_Zr_s4_scaled, function(x) x$beta[1]), # beta0_c in equation 1 -  bias corrected overall mean: this is equal to uncorrected overall mean (i.e., beta0)
                             se_beta0_c2 = sapply(model_Zr_s4_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c2 = sapply(model_Zr_s4_scaled, function(x) x$pval[1]) # p valuer of beta0_c
                            )

```


### (c) correct for publication bias

Compare the absolute values of beta0 and beta0_c2 to decide whether the publication bias is correctly adjusted. The principle is that when the bias-corrected meta-analytic overall mean (beta0_c2) is smaller than the original meta-analytic overall mean (beta0), the publication bias is correctly adjusted. Otherwise, the publication bias is not correctly adjusted because the slope of sampling variance (beta1) and publication year (beta2) are not in a expected directionality (which is caused by the un-accounted heterogeneity).

```{r}
# combine reduced model estimates from lnRR, SMD, and Zr
model_est_all_corrected_scaled <- rbind(model_est_lnRR_sei_s1_scaled,
                                        model_est_lnRR_ess.sei_s1_scaled,
                                        model_est_lnRR_s2_scaled,
                                        model_est_lnRR_sei_s3_scaled,
                                        model_est_lnRR_ess.sei_s3_scaled,
                                        model_est_lnRR_s4_scaled,
                                        model_est_SMD_sei_s1_scaled,
                                        model_est_SMD_ess.sei_s1_scaled,
                                        model_est_SMD_s2_scaled,
                                        model_est_SMD_sei_s3_scaled,
                                        model_est_SMD_ess.sei_s3_scaled,
                                        model_est_SMD_s4_scaled,
                                        model_est_Zr_s1_scaled,
                                        model_est_Zr_s2_scaled,
                                        model_est_Zr_s3_scaled,
                                        model_est_Zr_s4_scaled)

# create a variable to decide whether beta0 is larger than beta0_c2
## model_est_all_corrected_scaled %>% mutate(beta0Mbeta0_c2 = abs(beta0) - abs(beta0_c2))
model_est_all_corrected_scaled$beta0Mbeta0_c2 <- abs(model_est_all_corrected_scaled$beta0) - abs(model_est_all_corrected_scaled$beta0_c2)

# create a variable to contain the 'real' bias-corrected meta-analytic mean
model_est_all_corrected_scaled$beta0_c3 <- ifelse(model_est_all_corrected_scaled$beta0Mbeta0_c2 > 0, model_est_all_corrected_scaled$beta0_c2, model_est_all_corrected_scaled$beta0)

```


# meta-meta-analysis

we use random-effect meta-analytic models to aggregate three coefficients obtained from the above meta-analytic models:

3.1 D - the mean difference between overall mean (beta0) and bias-corrected overall mean (beta0_c); see formula in the main text

3.2 beta1 - the slope of uncertainty (sampling error or effective sample size analogue)

3.3 beta2 - the slope of year

note that for a given meta-analysis, if beta0 is negative, we need to flip beta1 and beta2 prior to model fitting

## D
quantify the overall decline in the magniude of effect size after adjusting for publication bias
```{r}
#*************************************************************************#
#      Z-transform slopes and effect sizes in full models
#*************************************************************************#

# combine model estimates from lnRR, SMD, and Zr
model_est_all_scaled <- rbind(model_est_lnRR_scaled, model_est_SMD_scaled, model_est_Zr_scaled) 
# add unique identifier to each meta-analysis paper
model_est_all_scaled$MA <- c("ft053","ft053","ft053","ft077","ft084","ft089","ft125","ft127","ft181","ft181","ft027","ft030","ft078","ft095","ft095","ft095","ft109","ft144","ft155","ft166","ft056","ft056","ft068","ft096","ft096","ft101","ft107","ft107","ft112","ft117","ft126","ft133","ft134","ft140","ft140","ft151","ft167","ft179","ft037","ft037","ft037","ft037","ft040","ft045","ft067","ft087","ft092","ft093","ft098","ft105","ft115","ft120","ft139","ft143","ft173","ft174","ft003","ft012","ft017","ft017","ft018","ft025","ft028","ft028","ft028","ft028","ft028","ft044","ft047","ft054","ft061","ft061","ft061","ft061","ft061","ft061","ft061","ft065","ft081","ft081","ft082","ft091","ft122","ft135",  
"ft138","ft177","ft187")

# differences in beta0
model_est_all_scaled$D <- abs(model_est_all_scaled$beta0 - model_est_all_scaled$beta0_c)
# differences in beta0' variance
model_est_all_scaled$D_var <- model_est_all_scaled$se_beta0^2 + model_est_all_scaled$se_beta0_c^2 + 2*model_est_all_scaled$se_beta0*model_est_all_scaled$se_beta0_c
# differences in beta0' SE
model_est_all_scaled$D_sei <- sqrt(model_est_all_scaled$D_var)

# get folded mean and variance
model_est_all_scaled$D_folded <- folded_es(mean = model_est_all_scaled$D, variance = model_est_all_scaled$D_var)
model_est_all_scaled$D_var_folded <- folded_error(mean = model_est_all_scaled$D, variance = model_est_all_scaled$D_var)
model_est_all_scaled$D_sei_folded <- sqrt(model_est_all_scaled$D_var_folded)

# overall decline in effect size magnitude
MMA_D_all_scaled <- rma.mv(yi = D_folded, V = D_var_folded, random = list(~1 | MA, ~1 | case), method = "REML", data = model_est_all_scaled) # rma(yi = D_folded, sei = D_sei_folded, method = "REML", data = model_est_all_scaled)


# orchard for overall decline in effect size magnitude
png(filename = "./orchard_MMA_D_all.png", width = 6, height = 3.5, units = "in", res = 400, type = "windows")
orchard_MMA_D_all <- orchard_plot(MMA_D_all_scaled, 
             data = model_est_all_scaled,
             mod = "1", group = "case", 
             k = FALSE, g = FALSE,
             trunk.size = 1, branch.size = 1, twig.size = 0.5,
             xlab = "D statistics",
             transfm = "none", 
             angle = 90) +
# scale_y_continuous(limits = c(-0.5, 1), breaks = seq(-0.5, 1, 0.25)) + 
scale_x_discrete(labels = c("Decline in effect size magnitude")) + 
scale_fill_manual(values = c("#88CCEE")) +
scale_color_manual(values = c("#88CCEE")) +
theme(axis.text.x = element_text(size = 10, colour = "black"),
      axis.text.y = element_text(size = 10, colour = "black"),
      axis.title.x = element_text(size = 10, colour = "black"),
      plot.title = element_text(size = 10, colour = "black"))
orchard_MMA_D_all
dev.off()



# overall decline in different effect size measures
# reorder the level of effect size types
model_est_all_scaled$es_type <- factor(model_est_all_scaled$es_type, levels = c("Zr", "SMD", "lnRR"))
MMA_D_all_scaled_es_type <- rma.mv(yi = D_folded, V = D_var_folded, random = list(~1 | MA, ~1 | case), mods = ~ es_type - 1, method = "REML", test = "t", data = model_est_all_scaled)

# orchard for overall decline in effect size magnitude for effect size measures
png(filename = "./orchard_MMA_D_all_es_type.png", width = 6, height = 3.5, units = "in", res = 400, type = "windows")
orchard_MMA_D_all_es_type <- orchard_plot(MMA_D_all_scaled_es_type, 
             data = model_est_all_scaled,
             mod = "es_type", group = "case", 
             k = FALSE, g = FALSE,
             trunk.size = 1, branch.size = 1, twig.size = 0.5,
             xlab = "Decline in effect size magnitude",
             transfm = "none", 
             angle = 90) +
# scale_y_continuous(limits = c(-0.5, 1), breaks = seq(-0.5, 1, 0.25)) + 
scale_x_discrete(labels = c("Zr", "SMD", "lnRR")) + 
scale_fill_manual(values = c("#CC6677", "#DDCC77", "#117733")) +
scale_color_manual(values = c("#CC6677", "#DDCC77", "#117733")) +
theme(axis.text.x = element_text(size = 10, colour = "black"),
      axis.text.y = element_text(size = 10, colour = "black"),
      axis.title.x = element_text(size = 10, colour = "black"),
      plot.title = element_text(size = 10, colour = "black"))
orchard_MMA_D_all_es_type
dev.off()


# assemble panels
png(filename = "./orchard_MMA_D_assemble.png", width = 5, height = 5, units = "in", res = 400, type = "windows")
plot_grid(orchard_MMA_D_all, orchard_MMA_D_all_es_type, labels = c("A","B"), nrow = 2)
dev.off()


# paired plot
dummy <- rep("Dummy", nrow(model_est_all_scaled))
wide.data <- 
  tibble::tibble(
    `Overall mean` = abs(model_est_all_scaled$beta0),
    `Bias-corrected mean` = abs(model_est_all_scaled$beta0_c),
     Dummy = dummy, ID = 1:length(dummy), case = model_est_all_scaled$case,es_type = model_est_all_scaled$es_type)

## find correct direction of slope
include.point <- which((wide.data$`Overall mean` - wide.data$`Bias-corrected mean`)>0)
## table only includes correct data
wide.data2 <- wide.data[include.point,]


# use ggpaired() in ggpubr to show the pairwise comparisons 
## lnRR
png(filename = "./paired.plot_lnRR.png", width = 5, height = 5, units = "in", type = "windows", res = 400)
paired.plot_lnRR <- ggpaired(data = wide.data2 %>% filter(es_type=="lnRR"), cond1 = "Overall mean", cond2 = "Bias-corrected mean",fill = "condition", palette = "jco") + 
  scale_y_continuous(limits = c(0,1)) +
  scale_fill_manual(values = c("#E69F00", "#56B4E9")) + 
  guides(fill = "none") + labs(x = "", y = "Effect size estimate of lnRR")
paired.plot_lnRR
dev.off()

## SMD
png(filename = "./paired.plot_SMD.png", width = 5, height = 5, units = "in", type = "windows", res = 400)
paired.plot_SMD <- ggpaired(data = wide.data2 %>% filter(es_type=="SMD"), cond1 = "Overall mean", cond2 = "Bias-corrected mean",fill = "condition", palette = "jco") + 
  scale_y_continuous(limits = c(0,1)) +
  scale_fill_manual(values = c("#009E73", "#CC79A7")) + 
  guides(fill = "none") + labs(x = "", y = "Effect size estimate of SMD")
paired.plot_SMD
dev.off()

## Zr
png(filename = "./paired.plot_Zr.png", width = 5, height = 5, units = "in", type = "windows", res = 400)
paired.plot_Zr <- ggpaired(data = wide.data2 %>% filter(es_type=="Zr"), cond1 = "Overall mean", cond2 = "Bias-corrected mean",fill = "condition", palette = "jco") + 
  scale_y_continuous(limits = c(0,1)) +
  scale_fill_manual(values = c("#0072B2", "#D55E00")) + 
  guides(fill = "none") + labs(x = "", y = "Effect size estimate of Zr")
paired.plot_Zr
dev.off()

## put together
png(filename = "./paired.plot3.png", width = 4, height = 8, units = "in", type = "windows", res = 400)
plot_grid(paired.plot_lnRR,paired.plot_SMD,paired.plot_Zr,ncol = 1)
dev.off()


#*************************************************************************#
#           Z-transform in reduced models (with correct directions) 
#*************************************************************************#


# add unique identifier to each meta-analysis paper
model_est_all_corrected_scaled$MA <- c("ft053v","ft053","ft089","ft181","ft027","ft030","ft095","ft144","ft053","ft181","ft078","ft095","ft077","ft109","ft084","ft125","ft127","ft095","ft155","ft166","ft068","ft096","ft101","ft107","ft117","ft126","ft133","ft140","ft167","ft037","ft037","ft098","ft056","ft096","ft107","ft140","ft037","ft037","ft067","ft087","ft092","ft093","ft105","ft120","ft173","ft056","ft112","ft134","ft151","ft179","ft040","ft045","ft139","ft143","ft174","ft115","ft003","ft017","ft018","ft028","ft028","ft047","ft061","ft061","ft065","ft122","ft187","ft012","ft017","ft061","ft135","ft138","ft177","ft025","ft028","ft044","ft054","ft061","ft061","ft061","ft061","ft081","ft082","ft091","ft028","ft028","ft081")


# differences in beta0
model_est_all_corrected_scaled$D <- abs(model_est_all_corrected_scaled$beta0 - model_est_all_corrected_scaled$beta0_c)
# differences in beta0' variance
model_est_all_corrected_scaled$D_var <- model_est_all_corrected_scaled$se_beta0^2 + model_est_all_corrected_scaled$se_beta0_c^2 + 2*model_est_all_corrected_scaled$se_beta0*model_est_all_corrected_scaled$se_beta0_c
# differences in beta0' SE
model_est_all_corrected_scaled$D_sei <- sqrt(model_est_all_corrected_scaled$D_var)

# get folded mean and variance
model_est_all_corrected_scaled$D_folded <- folded_es(mean = model_est_all_corrected_scaled$D, variance = model_est_all_corrected_scaled$D_var)
model_est_all_corrected_scaled$D_var_folded <- folded_error(mean = model_est_all_corrected_scaled$D, variance = model_est_all_corrected_scaled$D_var)
model_est_all_corrected_scaled$D_sei_folded <- sqrt(model_est_all_corrected_scaled$D_var_folded)

# overall decline in effect size magnitude
MMA_D_all_corrected_scaled <- rma.mv(yi = D_folded, V = D_var_folded, random = list(~1 | MA, ~1 | case), method = "REML", data = model_est_all_corrected_scaled) # rma(yi = D_folded, sei = D_sei_folded, method = "REML", data = model_est_all_corrected_scaled)


# orchard for overall decline in effect size magnitude
png(filename = "./orchard_MMA_D_all_corrected.png", width = 6, height = 3.5, units = "in", res = 400, type = "windows")
orchard_MMA_D_all_corrected <- orchard_plot(MMA_D_all_corrected_scaled, 
             data = model_est_all_scaled,
             mod = "1", group = "case", 
             k = FALSE, g = FALSE,
             trunk.size = 1, branch.size = 1, twig.size = 0.5,
             xlab = "D statistics",
             transfm = "none", 
             angle = 90) +
# scale_y_continuous(limits = c(-0.5, 1), breaks = seq(-0.5, 1, 0.25)) + 
scale_x_discrete(labels = c("Decline in effect size magnitude")) + 
scale_fill_manual(values = c("#88CCEE")) +
scale_color_manual(values = c("#88CCEE")) +
theme(axis.text.x = element_text(size = 10, colour = "black"),
      axis.text.y = element_text(size = 10, colour = "black"),
      axis.title.x = element_text(size = 10, colour = "black"),
      plot.title = element_text(size = 10, colour = "black"))
orchard_MMA_D_all_corrected
dev.off()

# overall decline in different effect size measures
# reorder the level of effect size types
model_est_all_corrected_scaled$es_type <- factor(model_est_all_corrected_scaled$es_type, levels = c("Zr", "SMD", "lnRR"))
MMA_D_all_corrected_scaled_es_type <- rma.mv(yi = D_folded, V = D_var_folded, random = ~1 | case, mods = ~ es_type - 1, method = "REML", test = "t", data = model_est_all_corrected_scaled)

# orchard for overall decline in effect size magnitude for effect size measures
png(filename = "./orchard_MMA_D_all_corrected_es_type.png", width = 6, height = 3.5, units = "in", res = 400, type = "windows")
orchard_MMA_D_all_corrected_es_type <- orchard_plot(MMA_D_all_corrected_scaled_es_type, 
             data = model_est_all_scaled,
             mod = "es_type", group = "case", 
             k = FALSE, g = FALSE,
             trunk.size = 1, branch.size = 1, twig.size = 0.5,
             xlab = "Decline in effect size magnitude",
             transfm = "none", 
             angle = 90) +
# scale_y_continuous(limits = c(-0.5, 1), breaks = seq(-0.5, 1, 0.25)) + 
scale_x_discrete(labels = c("Zr", "SMD", "lnRR")) + 
scale_fill_manual(values = c("#CC6677", "#DDCC77", "#117733")) +
scale_color_manual(values = c("#CC6677", "#DDCC77", "#117733")) +
labs(title = "B") +
theme(axis.text.x = element_text(size = 10, colour = "black"),
      axis.text.y = element_text(size = 10, colour = "black"),
      axis.title.x = element_text(size = 10, colour = "black"),
      plot.title = element_text(size = 10, colour = "black"))
orchard_MMA_D_all_corrected_es_type
dev.off()


# assemble panels
png(filename = "./orchard_MMA_D_corrected_assemble.png", width = 5, height = 5, units = "in", res = 400, type = "windows")
plot_grid(orchard_MMA_D_all_corrected, orchard_MMA_D_all_corrected_es_type, labels = c("A","B"), nrow = 2)
dev.off()


# paired plot
dummy <- rep("Dummy", nrow(model_est_all_corrected_scaled))
# folded mean
wide.data <- 
  tibble::tibble(
    `Original` =  folded_es(mean=model_est_all_corrected_scaled$beta0,variance=model_est_all_corrected_scaled$se_beta0^2)*c(sd_lnRR,sd_SMD,sd_Zr),
    `Bias-corrected` = folded_es(mean=model_est_all_corrected_scaled$beta0_c,variance=model_est_all_corrected_scaled$se_beta0_c^2)*c(sd_lnRR,sd_SMD,sd_Zr),
     Dummy = dummy, ID = 1:length(dummy), case = model_est_all_corrected_scaled$case,es_type = model_est_all_corrected_scaled$es_type)

## find correct direction of slope
include.point <- which((wide.data$`Original` - wide.data$`Bias-corrected`)>0)
## table only includes correct data
wide.data2 <- wide.data[include.point,]


# use ggpaired() in ggpubr to show the pairwise comparisons 
# order
wide.data2$es_type <- factor(wide.data2$es_type,levels = c("lnRR", "SMD", "Zr"))
# facet by es_type
png(filename = "./paired.plot.png", width = 8, height = 3, units = "in", type = "windows", res = 400)
ggpaired(data = wide.data2, facet.by = "es_type", cond1 = "Original", cond2 = "Bias-corrected",fill = "condition", palette = "jco") + 
  scale_y_continuous(breaks = seq(0,1.2,0.3), limits = c(0,1.2)) +
  # scale_fill_manual(values = c("#E69F00", "#56B4E9")) + 
  guides(fill = "none") + labs(x = "", y = "Effect size estimate")
dev.off()


## lnRR
png(filename = "./paired.plot_lnRR.png", width = 5, height = 5, units = "in", type = "windows", res = 400)
paired.plot_lnRR <- ggpaired(data = wide.data2 %>% filter(es_type=="lnRR"), cond1 = "Overall mean", cond2 = "Bias-corrected mean",fill = "condition", palette = "jco") + 
  scale_y_continuous(limits = c(0,1)) +
  scale_fill_manual(values = c("#E69F00", "#56B4E9")) + 
  guides(fill = "none") + labs(x = "", y = "Effect size estimate of lnRR")
paired.plot_lnRR
dev.off()


## SMD
png(filename = "./paired.plot_SMD.png", width = 5, height = 5, units = "in", type = "windows", res = 400)
paired.plot_SMD <- ggpaired(data = wide.data2 %>% filter(es_type=="SMD"), cond1 = "Overall mean", cond2 = "Bias-corrected mean",fill = "condition", palette = "jco") + 
  scale_y_continuous(limits = c(0,1)) +
  scale_fill_manual(values = c("#009E73", "#CC79A7")) + 
  guides(fill = "none") + labs(x = "", y = "Effect size estimate of SMD")
paired.plot_SMD
dev.off()

## Zr
png(filename = "./paired.plot_Zr.png", width = 5, height = 5, units = "in", type = "windows", res = 400)
paired.plot_Zr <- ggpaired(data = wide.data2 %>% filter(es_type=="Zr"), cond1 = "Overall mean", cond2 = "Bias-corrected mean",fill = "condition", palette = "jco") + 
  scale_y_continuous(limits = c(0,1)) +
  scale_fill_manual(values = c("#0072B2", "#D55E00")) + 
  guides(fill = "none") + labs(x = "", y = "Effect size estimate of Zr")
paired.plot_Zr
dev.off()

## put together
png(filename = "./paired.plot3.png", width = 4, height = 8, units = "in", type = "windows", res = 400)
plot_grid(paired.plot_lnRR,paired.plot_SMD,paired.plot_Zr,ncol = 1)
dev.off()

```


## beta1

```{r}
#*************************************************************************#
#      Z-transform slopes and effect sizes in full models
#*************************************************************************#

# overall evidence of small-study effect - beta1
# aggregation of beta1 from full model
beta1_flip_all <- (model_est_all_scaled %>% subset(model_est_all_scaled$beta0 < 0))$case
# first use beta as flipped beta1
model_est_all_scaled$beta1_flip <- model_est_all_scaled$beta1
# then replace those with negative overall mean
model_est_all_scaled[model_est_all_scaled$case %in% beta1_flip_all, ]$beta1_flip <- model_est_all_scaled[model_est_all_scaled$case %in% beta1_flip_all, ]$beta1*(-1)

# overall beta1
MMA_beta1_all_scaled <- rma.mv(yi = beta1_flip, V = se_beta1^2, mods = ~ 1, random = list(~1 | MA, ~1 | case), method = "REML", test = "t", control = list(optimizer = "optim"), data = model_est_all_scaled) 


# orchard for overall beta1
png(filename = "./orchard_MMA_beta1_all.png", width = 6, height = 3.5, units = "in", res = 400, type = "windows")
orchard_MMA_beta1_all <- orchard_plot(MMA_beta1_all_scaled, 
             data = model_est_all_scaled,
             mod = "1", group = "case", 
             k = FALSE, g = FALSE,
             trunk.size = 4, branch.size = 1, twig.size = 0.5,
             xlab = " ",
             transfm = "none", 
             angle = 90) +
scale_y_continuous(limits = c(-0.5, 1), breaks = seq(-0.5, 1, 0.25)) + 
scale_x_discrete(labels = c("Pattern of small-study effects")) + 
scale_fill_manual(values = c("#88CCEE")) +
scale_color_manual(values = c("#88CCEE")) +
theme(#axis.text.x = element_text(size = 10, colour = "black"),
      axis.text.x = element_blank(),
      axis.text.y = element_text(size = 10, colour = "black"),
      #axis.title.x = element_text(size = 10, colour = "black"),
      axis.title.x = element_blank(),
      axis.ticks.x = element_blank(),
      plot.title = element_text(size = 10, colour = "black"))
orchard_MMA_beta1_all
dev.off()



# overall beta1 for different effect size measures
# reorder the level of effect size types
model_est_all_scaled$es_type <- factor(model_est_all_scaled$es_type, levels = c("Zr", "SMD", "lnRR"))

MMA_beta1_all_scaled_es_type <- rma.mv(yi = beta1_flip, V = se_beta1^2, random = list(~1 | MA, ~1 | case), method = "REML", mods = ~ es_type - 1, test = "t", data = model_est_all_scaled)

# orchard for overall beta1 for different effect size measures
png(filename = "./orchard_MMA_beta1_all_es_type.png", width = 6, height = 3.5, units = "in", res = 400, type = "windows")
orchard_MMA_beta1_all_es_type <- orchard_plot(MMA_beta1_all_scaled_es_type, 
             data = model_est_all_scaled,
             mod = "es_type", group = "case", 
             k = FALSE, g = FALSE,
             trunk.size = 4, branch.size = 1, twig.size = 0.5,
             xlab = "Standardised regression slope of sampling error",
             transfm = "none", 
             angle = 90) +
scale_y_continuous(limits = c(-0.5, 1), breaks = seq(-0.5, 1, 0.25)) + 
scale_x_discrete(labels = c(expression(italic("Zr")), "SMD", "lnRR")) + 
scale_fill_manual(values = c("#CC6677", "#DDCC77", "#117733")) +
scale_color_manual(values = c("#CC6677", "#DDCC77", "#117733")) +
theme(axis.text.x = element_text(size = 10, colour = "black"),
      axis.text.y = element_text(size = 10, colour = "black"),
      axis.title.x = element_text(size = 10, colour = "black"),
      plot.title = element_text(size = 10, colour = "black"),
      legend.position = "none")
orchard_MMA_beta1_all_es_type
dev.off()


# assemble panels
png(filename = "./orchard_MMA_beta1_assemble.png", width = 5, height = 5, units = "in", res = 400, type = "windows")
plot_grid(orchard_MMA_beta1_all, orchard_MMA_beta1_all_es_type, labels = c("A","B"), nrow = 2)
dev.off()

#*************************************************************************#
#           Z-transform in reduced models (with correct directions) 
#*************************************************************************#
# overall_corrected evidence of small_corrected-study effect - beta1
# aggregation of beta1 from full model
beta1_flip_all_corrected <- (model_est_all_corrected_scaled %>% subset(model_est_all_corrected_scaled$beta0 < 0))$case
# first use beta as flipped beta1
model_est_all_corrected_scaled$beta1_flip <- model_est_all_corrected_scaled$beta1
# then replace those with negative overall_corrected mean
model_est_all_corrected_scaled[model_est_all_corrected_scaled$case %in% beta1_flip_all_corrected, ]$beta1_flip <- model_est_all_corrected_scaled[model_est_all_corrected_scaled$case %in% beta1_flip_all_corrected, ]$beta1*(-1)

# overall beta1
# only keep non-zero beta1, because zero beta1 has a wrong direction (when extracting beta1 from fitted models, we specify a value of zero to a wrong direction of beta2) 
model_est_all_corrected_scaled2 <- model_est_all_corrected_scaled[model_est_all_corrected_scaled$beta1 != 0, ]

# note that for some datasets, when fitting reduced model with only sei as a predictor (scenario 3 where year is removed due to wrong direction), the sign of beta1 (sei's coefficient) changed from positive to negative (which is wrong direction), so we need to remove these datasets when aggregating beta1 with correct directions.
# find negative flipped beta1
model_est_all_corrected_scaled2 %>% filter(beta1_flip < 0) # ft112.csv
# remove it
model_est_all_corrected_scaled2 <- model_est_all_corrected_scaled2[model_est_all_corrected_scaled2$case != "ft112.csv", ]

# obtain overall beta1
MMA_beta1_all_corrected_scaled <- rma.mv(yi = beta1_flip, V = se_beta1^2, random = list(~1 | MA, ~1 | case), method = "REML", test = "t", data = model_est_all_corrected_scaled2) 

# overall_corrected beta1 for different effect size measures
MMA_beta1_all_corrected_scaled_es_type <- rma.mv(yi = beta1_flip, V = se_beta1^2, random = list(~1 | MA, ~1 | case), method = "REML", mods = ~ es_type - 1, test = "t", data = model_est_all_corrected_scaled2)

# orchard
png(filename = "./orchard_MMA_beta1_all_corrected.png", width = 6, height = 3.5, units = "in", res = 400, type = "windows")
orchard_plot(MMA_beta1_all_corrected_scaled, 
             data = model_est_all_corrected_scaled2,
             mod = "1", group = "case", 
             k = FALSE, g = FALSE,
             trunk.size = 1, branch.size = 1, twig.size = 0.5,
             xlab = "Standardised regression slope of uncertainty",
             transfm = "none", 
             angle = 90) +
scale_y_continuous(limits = c(-0.15, 1), breaks = seq(0, 1, 0.25)) + 
scale_x_discrete(labels = c("Small-study effect pattern in ecology and evolution")) + 
theme(axis.text.x = element_text(size = 10, colour = "black"),
      axis.text.y = element_text(size = 10, colour = "black"),
      axis.title.x = element_text(size = 10, colour = "black"),
      plot.title = element_text(size = 10, colour = "black"))
dev.off()


```



## beta2
```{r}
#*************************************************************************#
#      Z-transform slopes and effect sizes in full models
#*************************************************************************#

# overall evidence of time lag bias - beta2
# aggregation of beta2 from full model
beta2_flip_all <- (model_est_all_scaled %>% subset(model_est_all_scaled$beta0 < 0))$case
# first use beta as flipped beta2
model_est_all_scaled$beta2_flip <- model_est_all_scaled$beta2
# then replace those with negative overall mean
model_est_all_scaled[model_est_all_scaled$case %in% beta2_flip_all, ]$beta2_flip <- model_est_all_scaled[model_est_all_scaled$case %in% beta2_flip_all, ]$beta2*(-1)

# overall beta2
MMA_beta2_all_scaled <- rma.mv(yi = beta2_flip, V = se_beta2^2, mods = ~ 1, random = list(~1 | MA, ~1 | case), method = "REML", test = "t", control = list(optimizer = "optim"), data = model_est_all_scaled) 

# i2_ml(MMA_beta2_all_scaled, data = model_est_all_scaled, boot = 500)
# orchard
png(filename = "./orchard_MMA_beta2_all.png", width = 6, height = 3.5, units = "in", res = 400, type = "windows")
orchard_MMA_beta2_all <- orchard_plot(MMA_beta2_all_scaled, 
             data = model_est_all_scaled,
             mod = "1", group = "case", 
             k = FALSE, g = FALSE,
             trunk.size = 4, branch.size = 1, twig.size = 0.5,
             xlab = " ",
             transfm = "none", 
             angle = 90) +
scale_y_continuous(limits = c(-0.1, 0.1), breaks = seq(-0.1, 0.1, 0.05)) + 
scale_x_discrete(labels = c("Pattern of decline effect")) + 
scale_fill_manual(values = c("#332288")) +
scale_color_manual(values = c("#332288")) +
theme(#axis.text.x = element_text(size = 10, colour = "black"),
      axis.text.x = element_blank(),
      axis.text.y = element_text(size = 10, colour = "black"),
      #axis.title.x = element_text(size = 10, colour = "black"),
      axis.title.x = element_blank(),
      axis.ticks.x = element_blank(),
      plot.title = element_text(size = 10, colour = "black"))
orchard_MMA_beta2_all
dev.off()


# overall beta2 for different effect size measures
# reorder the level of effect size types
model_est_all_scaled$es_type <- factor(model_est_all_scaled$es_type, levels = c("Zr", "SMD", "lnRR"))

MMA_beta2_all_scaled_es_type <- rma.mv(yi = beta2_flip, V = se_beta2^2, random = list(~1 | MA, ~1 | case), method = "REML", mods = ~ es_type - 1, test = "t", data = model_est_all_scaled)

# orchard for overall beta2 for different effect size measures
png(filename = "./orchard_MMA_beta2_all_es_type.png", width = 6, height = 3.5, units = "in", res = 400, type = "windows")
orchard_MMA_beta2_all_es_type <- orchard_plot(MMA_beta2_all_scaled_es_type, 
             data = model_est_all_scaled,
             mod = "es_type", group = "case", 
             k = FALSE, g = FALSE,
             trunk.size = 4, branch.size = 1, twig.size = 0.5,
             xlab = "Standardised regression slope of year",
             transfm = "none", 
             angle = 90) +
scale_y_continuous(limits = c(-0.1, 0.1), breaks = seq(-0.1, 0.1, 0.05)) + 
scale_x_discrete(labels = c(expression(italic("Zr")), "SMD", "lnRR")) + 
scale_fill_manual(values = c("#AA4499", "#44AA99", "#999933")) +
scale_color_manual(values = c("#AA4499", "#44AA99", "#999933")) +
theme(axis.text.x = element_text(size = 10, colour = "black"),
      axis.text.y = element_text(size = 10, colour = "black"),
      axis.title.x = element_text(size = 10, colour = "black"),
      plot.title = element_text(size = 10, colour = "black"),
      legend.position = "none")
orchard_MMA_beta2_all_es_type
dev.off()

# put together
png(filename = "./orchard_MMA_beta2_assemble.png", width = 5, height = 5, units = "in", res = 400, type = "windows")
plot_grid(orchard_MMA_beta2_all, orchard_MMA_beta2_all_es_type, labels = c("A","B"), nrow = 2)
dev.off()

#*************************************************************************#
#           Z-transform in reduced models (with correct directions) 
#*************************************************************************#
# overall_corrected evidence of time lag bias - beta2
# aggregation of beta2 from full model
beta2_flip_all_corrected <- (model_est_all_corrected_scaled %>% subset(model_est_all_corrected_scaled$beta0 < 0))$case
# first use beta as flipped beta2
model_est_all_corrected_scaled$beta2_flip <- model_est_all_corrected_scaled$beta2
# then replace those with negative overall_corrected mean
model_est_all_corrected_scaled[model_est_all_corrected_scaled$case %in% beta2_flip_all_corrected, ]$beta2_flip <- model_est_all_corrected_scaled[model_est_all_corrected_scaled$case %in% beta2_flip_all_corrected, ]$beta2*(-1)

# overall_corrected beta2
# only keep non-zero beta2, because zero beta2 has a wrong direction (when extracting beta2 from fitted models, we specify a value of zero to a wrong direction of beta2) 
model_est_all_corrected_scaled3 <- model_est_all_corrected_scaled[model_est_all_corrected_scaled$beta2 != 0, ]


# note that for some datasets, when fitting reduced model with only year as a predictor (scenario 2 where sei is removed due to wrong direction), the sign of beta2 (year's coefficient) changed from negative to positive (which is wrong direction), so we need to remove these datasets when aggregating beta2 with correct directions.
# find positive flipped beta2
model_est_all_corrected_scaled3 %>% filter(beta2_flip > 0) # ft037_1.csv, ft037_2.csv

# remove them
model_est_all_corrected_scaled3 <- model_est_all_corrected_scaled3[model_est_all_corrected_scaled3$case != "ft037_1.csv", ]
model_est_all_corrected_scaled3 <- model_est_all_corrected_scaled3[model_est_all_corrected_scaled3$case != "ft037_2.csv", ]

MMA_beta2_all_corrected_scaled <- rma.mv(yi = beta2_flip, V = se_beta2^2, random = list(~1 | MA, ~1 | case), method = "REML", test = "t", data = model_est_all_corrected_scaled3) 

# overall_corrected beta2 for different effect size measures
MMA_beta2_all_corrected_scaled_es_type <- rma.mv(yi = beta2_flip, V = se_beta2^2, random = list(~1 | MA, ~1 | case), method = "REML", mods = ~ es_type - 1, test = "t", data = model_est_all_corrected_scaled3)

# orchard
png(filename = "./orchard_MMA_beta2_all_corrected.png", width = 6, height = 3.5, units = "in", res = 400, type = "windows")
orchard_plot(MMA_beta2_all_corrected_scaled, 
             data = model_est_all_corrected_scaled3,
             mod = "1", group = "case", 
             k = FALSE, g = FALSE,
             trunk.size = 1, branch.size = 1, twig.size = 0.5,
             xlab = "Standardised regression slope of year",
             transfm = "none", 
             angle = 90) +
# ylim(-0.1, 0.01) +
scale_y_continuous(limits = c(-0.1, 0.01), breaks = seq(-0.1, 0.0, 0.05)) + 
scale_x_discrete(labels = c("Decline effect pattern in ecology and evolution")) + 
theme(axis.text.x = element_text(size = 10, colour = "black"),
      axis.text.y = element_text(size = 10, colour = "black"),
      axis.title.x = element_text(size = 10, colour = "black"),
      plot.title = element_text(size = 10, colour = "black"))
dev.off()
  


```


## interpretation of standardised coefficients

```{r}


# lnRR
# sd for each dataset
sd_lnRR <- NA 
for (i in 1:length(lnRR)) {
  sd_lnRR[i] <- sd(lnRR[[i]]$es)
}
# average sd
mean(sd_lnRR)


# overall standardised beta0
MMA_D_all_corrected_scaled_es_type$beta[3]
# convert standardised beta0 into original scale
MMA_D_all_corrected_scaled_es_type$beta[3]*mean(sd_lnRR) # 0.1163103


# SMD
# sd for each dataset
sd_SMD <- NA
for (i in 1:length(SMD)) {
  sd_SMD[i] <- sd(SMD[[i]]$es)
}
# average sd
mean(sd_SMD) # 2.06368
mean(sd_SMD[sd_SMD<2]) # 1.147149
# overall standardized beta0
MMA_D_all_es_type$beta[2]
# convert standardised beta0 into original scale
MMA_D_all_corrected_scaled_es_type$beta[2]*mean(sd_SMD) # 0.3908448
MMA_D_all_corrected_scaled_es_type$beta[2]*mean(sd_SMD[sd_SMD<2]) # 0.217261

# MMA_D_SMD$beta[1]*mean(unlist(w.sd_es_SMD)) # weighted variance


# Zr
# sd for each dataset
sd_Zr <- NA
for (i in 1:length(Zr)) {
  sd_Zr[i] <- sd(Zr[[i]]$es)
}
# average sd
mean(sd_Zr)
# overall standardised beta0
MMA_D_all_corrected_scaled_es_type$beta[1]
# convert standardised beta0 into original scale
MMA_D_all_corrected_scaled_es_type$beta[1]*mean(sd_Zr) # 0.1276679




# alternative approach
es_lnRR <- sapply(lnRR, function(x) x$es) %>% unlist
sd(es_lnRR)
es_SMD <- sapply(SMD, function(x) x$es) %>% unlist
sd(es_SMD)
es_Zr <- sapply(Zr, function(x) x$es) %>% unlist
sd(es_Zr)

### calculate weighted sd 
var_lnRR <- sapply(lnRR, function(x) x$var) %>% unlist
weighted.var(es_lnRR, w = 1/var_lnRR) %>% sqrt() # 0.7209148
MMA_D_lnRR$beta[1]*0.7209148

var_SMD <- sapply(SMD, function(x) x$var) %>% unlist
weighted.var(es_SMD, w = 1/var_SMD) %>% sqrt() # 0.01278843
MMA_D_SMD$beta[1]*0.01278843

var_Zr <- sapply(Zr, function(x) x$var) %>% unlist
weighted.var(es_Zr, w = 1/var_Zr) %>% sqrt() # 0.05958888
MMA_D_Zr$beta[1]*0.05958888


```


# Firepower plot - power

```{r}
# lnRR
MA_case_lnRR <- paste("lnRR", sep = " ", 1:20) # nrow(model_est_lnRR)

intercept_lnRR <- data.frame(MA_case=MA_case_lnRR,
                             es=model_est_lnRR$beta0,
                             power=power_summary_lnRR$Median,
                             effect=rep(c("Sampling"),20),
                             es_cat=rep(c("a.Intercept_es"),20))


intercept_adjusted_lnRR <- data.frame(MA_case=MA_case_lnRR,
                                      es=model_est_lnRR$beta0_c3,
                                      power=power_c_summary_lnRR$Median,
                                      effect=rep(c("cSampling"),20),
                                      es_cat=rep(c("a.Intercept_es"),20))

MA.power_lnRR <- data.frame(MA_case=MA_case_lnRR,
                            es=model_est_lnRR$beta0,
                            power=model_est_lnRR$MA.power,
                            effect=rep(c("Meta-analysis"),20),
                            es_cat=rep(c("b.MA.power"),20))

MA.power_adjusted_lnRR <- data.frame(MA_case=MA_case_lnRR,
                                     es=model_est_lnRR$beta0_c3,
                                     power=model_est_lnRR$MA.power_c,
                                     effect=rep(c("cMeta-analysis"),20),
                                     es_cat=rep(c("b.MA.power"),20))


power_firepower_lnRR <- rbind(intercept_adjusted_lnRR,
                              intercept_lnRR,
                              MA.power_adjusted_lnRR,
                              MA.power_lnRR)

# convert MA_case to factor with desired ordering of levels
power_firepower_lnRR$MA_case <- factor(power_firepower_lnRR$MA_case, levels=rev(MA_case_lnRR))


firepower_plot_lnRR <- ggplot(data = power_firepower_lnRR) +
  geom_tile(aes(x = effect, y = MA_case, fill = power, width = 0.95, height = 0.25)) +
  theme(aspect.ratio = 0.3) +
  scale_fill_gradient(
    name = "Power",
    low = "white", # #56B4E9
    #mid = "white",
    high = "#CC79A7", 
    #midpoint = 0.5, # if using scale_fill_gradient2, it needs mid color and midpoint
    limits = c(0,1),
    guide = "colourbar",
  ) + 
  facet_grid( ~ es_cat, scale = 'free_x', space = "free_x") +
  theme_tufte(base_family = "Helvetica") +
  theme(strip.text.x = element_blank()) + 
  labs(x ="lnRR", y = "") + 
  theme(axis.text.x = element_text(size = 12, colour = "black", angle = -60, margin = margin(t = 0, r = 0, b = 5, l = 0)),
        axis.text.y = element_text(size = 12, colour = "black"),
        axis.title.x = element_text(size = 12, colour = "black", face = "bold"),
        legend.text = element_text(size=12),
        legend.title = element_text(size=12),
        legend.key.width = unit(0.3,"in"),
        legend.key.height = unit(0.4,"in"), legend.position = "bottom" ) +
  scale_x_discrete(position = "top") + labs(title = "(A)")


png(filename = "./firepower_plot_lnRR.png", width = 5, height = 5, units = "in", type = "windows", res = 400)
firepower_plot_lnRR 
dev.off()


pdf(file = "./Figures/figure3_colour_bar.pdf", width = 8, height = 10)
firepower_plot_RR 
dev.off()



# SMD
MA_case_SMD <- paste("SMD", sep = " ", 1:36) # nrow(model_est_SMD)

intercept_SMD <- data.frame(MA_case=MA_case_SMD,
                             es=model_est_SMD$beta0,
                             power=power_summary_SMD$Median,
                             effect=rep(c("Sampling"),36),
                             es_cat=rep(c("a.Intercept_es"),36))


intercept_adjusted_SMD <- data.frame(MA_case=MA_case_SMD,
                                      es=model_est_SMD$beta0_c3,
                                      power=power_c_summary_SMD$Median,
                                      effect=rep(c("cSampling"),36),
                                      es_cat=rep(c("a.Intercept_es"),36))

MA.power_SMD <- data.frame(MA_case=MA_case_SMD,
                            es=model_est_SMD$beta0,
                            power=model_est_SMD$MA.power,
                            effect=rep(c("Meta-analysis"),36),
                            es_cat=rep(c("b.MA.power"),36))

MA.power_adjusted_SMD <- data.frame(MA_case=MA_case_SMD,
                                     es=model_est_SMD$beta0_c3,
                                     power=model_est_SMD$MA.power_c,
                                     effect=rep(c("cMeta-analysis"),36),
                                     es_cat=rep(c("b.MA.power"),36))


power_firepower_SMD <- rbind(intercept_adjusted_SMD,
                              intercept_SMD,
                              MA.power_adjusted_SMD,
                              MA.power_SMD)

# convert MA_case to factor with desired ordering of levels
power_firepower_SMD$MA_case <- factor(power_firepower_SMD$MA_case, levels=rev(MA_case_SMD))



firepower_plot_SMD <- ggplot(data = power_firepower_SMD) +
  geom_tile(aes(x = effect, y = MA_case, fill = power, width = 0.95, height = 0.5)) +
  theme(aspect.ratio = 0.3) +
  scale_fill_gradient(
    name = "Power",
    low = "white", # #56B4E9
    #mid = "white",
    high = "#CC79A7", 
    #midpoint = 0.5, # if using scale_fill_gradient2, it needs mid color and midpoint
    limits = c(0,1),
    guide = "colourbar",
  ) + 
  facet_grid( ~ es_cat, scale = 'free_x', space = "free_x") +
  theme_tufte(base_family = "Helvetica") +
  theme(strip.text.x = element_blank()) + 
  labs(x ="SMD", y = "") + 
  theme(axis.text.x = element_text(size = 12, colour = "black", angle = -60, margin = margin(t = 0, r = 0, b = 5, l = 0)),
        axis.text.y = element_text(size = 12, colour = "black"),
        axis.title.x = element_text(size = 12, colour = "black", face = "bold"),
        legend.text = element_text(size=12),
        legend.title = element_text(size=12),
        legend.key.width = unit(0.5,"in"),
        legend.key.height = unit(0.4,"in"), legend.position = "none" ) +
  scale_x_discrete(position = "top") + labs(title = "(B)")


png(filename = "./firepower_plot_SMD.png", width = 5, height = 5, units = "in", type = "windows", res = 400)
firepower_plot_SMD 
dev.off()


# Zr
MA_case_Zr <- paste("Zr", sep = " ", 1:31) # nrow(model_est_Zr)

intercept_Zr <- data.frame(MA_case=MA_case_Zr,
                             es=model_est_Zr$beta0,
                             power=power_summary_Zr$Median,
                             effect=rep(c("Sampling"),31),
                             es_cat=rep(c("a.Intercept_es"),31))


intercept_adjusted_Zr <- data.frame(MA_case=MA_case_Zr,
                                      es=model_est_Zr$beta0_c3,
                                      power=power_c_summary_Zr$Median,
                                      effect=rep(c("cSampling"),31),
                                      es_cat=rep(c("a.Intercept_es"),31))

MA.power_Zr <- data.frame(MA_case=MA_case_Zr,
                            es=model_est_Zr$beta0,
                            power=model_est_Zr$MA.power,
                            effect=rep(c("Meta-analysis"),31),
                            es_cat=rep(c("b.MA.power"),31))

MA.power_adjusted_Zr <- data.frame(MA_case=MA_case_Zr,
                                     es=model_est_Zr$beta0_c3,
                                     power=model_est_Zr$MA.power_c,
                                     effect=rep(c("cMeta-analysis"),31),
                                     es_cat=rep(c("b.MA.power"),31))


power_firepower_Zr <- rbind(intercept_adjusted_Zr,
                              intercept_Zr,
                              MA.power_adjusted_Zr,
                              MA.power_Zr)

# convert MA_case to factor with desired ordering of levels
power_firepower_Zr$MA_case <- factor(power_firepower_Zr$MA_case, levels=rev(MA_case_Zr))




firepower_plot_Zr <- ggplot(data = power_firepower_Zr) +
  geom_tile(aes(x = effect, y = MA_case, fill = power, width = 0.95, height = 0.5)) +
  theme(aspect.ratio = 0.3) +
  scale_fill_gradient(
    name = "Power",
    low = "white", # #56B4E9
    #mid = "white",
    high = "#CC79A7", 
    #midpoint = 0.5, # if using scale_fill_gradient2, it needs mid color and midpoint
    limits = c(0,1),
    guide = "colourbar",
  ) + 
  facet_grid( ~ es_cat, scale = 'free_x', space = "free_x") +
  theme_tufte(base_family = "Helvetica") +
  theme(strip.text.x = element_blank()) + 
  labs(x =expression(italic("Zr")), y = "") + 
  theme(axis.text.x = element_text(size = 12, colour = "black", angle = -60, margin = margin(t = 0, r = 0, b = 5, l = 0)),
        axis.text.y = element_text(size = 12, colour = "black"),
        axis.title.x = element_text(size = 12, colour = "black", face = "bold"),
        legend.text = element_text(size=12),
        legend.title = element_text(size=12),
        legend.key.width = unit(0.5,"in"),
        legend.key.height = unit(0.4,"in"), legend.position = "none" ) +
  scale_x_discrete(position = "top") + labs(title = "(C)")


png(filename = "./firepower_plot_Zr.png", width = 5, height = 5, units = "in", type = "windows", res = 400)
firepower_plot_Zr 
dev.off()

# assemble
png(filename = "./firepower_plot_assemble.png", width = 10, height = 8.5, units = "in", type = "windows", res = 400)
plot_grid(firepower_plot_lnRR,firepower_plot_SMD,firepower_plot_Zr,ncol=3)
dev.off()


# put all
power_firepower_all <- rbind(power_firepower_lnRR,power_firepower_SMD,power_firepower_Zr)

firepower_plot_all <- ggplot(data = power_firepower_all) +
  geom_tile(aes(x = effect, y = MA_case, fill = power, width = 0.95, height = 0.5)) +
  theme(aspect.ratio = 0.3) +
  scale_fill_gradient(
    name = "Power",
    low = "white", # #56B4E9
    #mid = "white",
    high = "#CC79A7", 
    #midpoint = 0.5, # if using scale_fill_gradient2, it needs mid color and midpoint
    limits = c(0,1),
    guide = "colourbar",
  ) + 
  facet_grid( ~ es_cat, scale = 'free_x', space = "free_x") +
  theme_tufte(base_family = "Helvetica") +
  theme(strip.text.x = element_blank()) + 
  labs(x ="all", y = "") + 
  theme(axis.text.x = element_text(size = 12, colour = "black", angle = 0, margin = margin(t = 0, r = 0, b = 5, l = 0)),
        axis.text.y = element_text(size = 12, colour = "black"),
        axis.title.x = element_text(size = 12, colour = "black", face = "bold"),
        legend.text = element_text(size=12),
        legend.title = element_text(size=12),
        legend.key.width = unit(0.5,"in"),
        legend.key.height = unit(0.4,"in"), legend.position = "none" ) +
  scale_x_discrete(position = "top") + labs(title = "(A)")


png(filename = "./firepower_plot_all.png", width = 5, height = 15, units = "in", type = "windows", res = 400)
firepower_plot_all 
dev.off()

```


# Firepower plot - S error

```{r}
# lnRR
intercept_lnRR <- data.frame(MA_case=MA_case_lnRR,
                             es=model_est_lnRR$beta0,
                             power=power.S_summary_lnRR$Median,
                             effect=rep(c("Sampling"),20),
                             es_cat=rep(c("a.Intercept_es"),20))


intercept_adjusted_lnRR <- data.frame(MA_case=MA_case_lnRR,
                                      es=model_est_lnRR$beta0_c3,
                                      power=power.S_c_summary_lnRR$Median,
                                      effect=rep(c("cSampling"),20),
                                      es_cat=rep(c("a.Intercept_es"),20))

MA.power_lnRR <- data.frame(MA_case=MA_case_lnRR,
                            es=model_est_lnRR$beta0,
                            power=model_est_lnRR$MA.power.S,
                            effect=rep(c("Meta-analysis"),20),
                            es_cat=rep(c("b.MA.power"),20))

MA.power_adjusted_lnRR <- data.frame(MA_case=MA_case_lnRR,
                                     es=model_est_lnRR$beta0_c3,
                                     power=model_est_lnRR$MA.power.S_c,
                                     effect=rep(c("cMeta-analysis"),20),
                                     es_cat=rep(c("b.MA.power"),20))


power.S_firepower_lnRR <- rbind(intercept_adjusted_lnRR,
                              intercept_lnRR,
                              MA.power_adjusted_lnRR,
                              MA.power_lnRR)

# convert MA_case to factor with desired ordering of levels
power.S_firepower_lnRR$MA_case <- factor(power_firepower_lnRR$MA_case, levels=rev(MA_case_lnRR))



firepower.S_plot_lnRR <- ggplot(data = power.S_firepower_lnRR) +
  geom_tile(aes(x = effect, y = MA_case, fill = power, width = 0.95, height = 0.25)) +
  theme(aspect.ratio = 0.3) +
  scale_fill_gradient(
    name = "Power",
    low = "white", # #56B4E9
    #mid = "white",
    high = "#E69F00", 
    #midpoint = 0.5, # if using scale_fill_gradient2, it needs mid color and midpoint
    limits = c(0,0.5),
    guide = "colourbar",
  ) + 
  facet_grid( ~ es_cat, scale = 'free_x', space = "free_x") +
  theme_tufte(base_family = "Helvetica") +
  theme(strip.text.x = element_blank()) + 
  labs(x ="lnRR", y = "") + 
  theme(axis.text.x = element_text(size = 12, colour = "black", angle = -60, margin = margin(t = 0, r = 0, b = 5, l = 0)),
        axis.text.y = element_text(size = 12, colour = "black"),
        axis.title.x = element_text(size = 12, colour = "black", face = "bold"),
        legend.text = element_text(size=12),
        legend.title = element_text(size=12),
        legend.key.width = unit(0.3,"in"),
        legend.key.height = unit(0.4,"in"), legend.position = "bottom" ) +
  scale_x_discrete(position = "top") + labs(title = "(A)")


png(filename = "./firepower.S_plot_lnRR.png", width = 5, height = 5, units = "in", type = "windows", res = 400)
firepower.S_plot_lnRR 
dev.off()


pdf(file = "./Figures/figure3_colour_bar.pdf", width = 8, height = 10)
firepower_plot_RR 
dev.off()



# SMD
MA_case_SMD <- paste("SMD", sep = " ", 1:36) # nrow(model_est_SMD)

intercept_SMD <- data.frame(MA_case=MA_case_SMD,
                             es=model_est_SMD$beta0,
                             power=power.S_summary_SMD$Median,
                             effect=rep(c("Sampling"),36),
                             es_cat=rep(c("a.Intercept_es"),36))


intercept_adjusted_SMD <- data.frame(MA_case=MA_case_SMD,
                                      es=model_est_SMD$beta0_c3,
                                      power=power.S_c_summary_SMD$Median,
                                      effect=rep(c("cSampling"),36),
                                      es_cat=rep(c("a.Intercept_es"),36))

MA.power_SMD <- data.frame(MA_case=MA_case_SMD,
                            es=model_est_SMD$beta0,
                            power=model_est_SMD$MA.power.S,
                            effect=rep(c("Meta-analysis"),36),
                            es_cat=rep(c("b.MA.power"),36))

MA.power_adjusted_SMD <- data.frame(MA_case=MA_case_SMD,
                                     es=model_est_SMD$beta0_c3,
                                     power=model_est_SMD$MA.power.S_c,
                                     effect=rep(c("cMeta-analysis"),36),
                                     es_cat=rep(c("b.MA.power"),36))


power.S_firepower_SMD <- rbind(intercept_adjusted_SMD,
                              intercept_SMD,
                              MA.power_adjusted_SMD,
                              MA.power_SMD)

# convert MA_case to factor with desired ordering of levels
power.S_firepower_SMD$MA_case <- factor(power_firepower_SMD$MA_case, levels=rev(MA_case_SMD))


firepower.S_plot_SMD <- ggplot(data = power.S_firepower_SMD) +
  geom_tile(aes(x = effect, y = MA_case, fill = power, width = 0.95, height = 0.5)) +
  theme(aspect.ratio = 0.3) +
  scale_fill_gradient(
    name = "Power",
    low = "white", # #56B4E9
    #mid = "white",
    high = "#E69F00", 
    #midpoint = 0.5, # if using scale_fill_gradient2, it needs mid color and midpoint
    limits = c(0,0.5),
    guide = "colourbar",
  ) + 
  facet_grid( ~ es_cat, scale = 'free_x', space = "free_x") +
  theme_tufte(base_family = "Helvetica") +
  theme(strip.text.x = element_blank()) + 
  labs(x ="SMD", y = "") + 
  theme(axis.text.x = element_text(size = 12, colour = "black", angle = -40, margin = margin(t = 0, r = 0, b = 5, l = 0)),
        axis.text.y = element_text(size = 12, colour = "black"),
        axis.title.x = element_text(size = 12, colour = "black", face = "bold"),
        legend.text = element_text(size=12),
        legend.title = element_text(size=12),
        legend.key.width = unit(0.5,"in"),
        legend.key.height = unit(0.4,"in"), legend.position = "none" ) +
  scale_x_discrete(position = "top") + labs(title = "(B)")


png(filename = "./firepower_plot_SMD.png", width = 5, height = 5, units = "in", type = "windows", res = 400)
firepower_plot_SMD 
dev.off()


# Zr
MA_case_Zr <- paste("Zr", sep = " ", 1:31) # nrow(model_est_Zr)

intercept_Zr <- data.frame(MA_case=MA_case_Zr,
                             es=model_est_Zr$beta0,
                             power=power.S_summary_Zr$Median,
                             effect=rep(c("Sampling"),31),
                             es_cat=rep(c("a.Intercept_es"),31))


intercept_adjusted_Zr <- data.frame(MA_case=MA_case_Zr,
                                      es=model_est_Zr$beta0_c3,
                                      power=power.S_c_summary_Zr$Median,
                                      effect=rep(c("cSampling"),31),
                                      es_cat=rep(c("a.Intercept_es"),31))

MA.power_Zr <- data.frame(MA_case=MA_case_Zr,
                            es=model_est_Zr$beta0,
                            power=model_est_Zr$MA.power.S,
                            effect=rep(c("Meta-analysis"),31),
                            es_cat=rep(c("b.MA.power"),31))

MA.power_adjusted_Zr <- data.frame(MA_case=MA_case_Zr,
                                     es=model_est_Zr$beta0_c3,
                                     power=model_est_Zr$MA.power.S_c,
                                     effect=rep(c("cMeta-analysis"),31),
                                     es_cat=rep(c("b.MA.power"),31))


power.S_firepower_Zr <- rbind(intercept_adjusted_Zr,
                              intercept_Zr,
                              MA.power_adjusted_Zr,
                              MA.power_Zr)

# convert MA_case to factor with desired ordering of levels
power.S_firepower_Zr$MA_case <- factor(power_firepower_Zr$MA_case, levels=rev(MA_case_Zr))




firepower.S_plot_Zr <- ggplot(data = power.S_firepower_Zr) +
  geom_tile(aes(x = effect, y = MA_case, fill = power, width = 0.95, height = 0.5)) +
  theme(aspect.ratio = 0.3) +
  scale_fill_gradient(
    name = "Power",
    low = "white", # #56B4E9
    #mid = "white",
    high = "#E69F00", 
    #midpoint = 0.5, # if using scale_fill_gradient2, it needs mid color and midpoint
    limits = c(0,0.5),
    guide = "colourbar",
  ) + 
  facet_grid( ~ es_cat, scale = 'free_x', space = "free_x") +
  theme_tufte(base_family = "Helvetica") +
  theme(strip.text.x = element_blank()) + 
  labs(x =expression(italic("Zr")), y = "") + 
  theme(axis.text.x = element_text(size = 12, colour = "black", angle = -60, margin = margin(t = 0, r = 0, b = 5, l = 0)),
        axis.text.y = element_text(size = 12, colour = "black"),
        axis.title.x = element_text(size = 12, colour = "black", face = "bold"),
        legend.text = element_text(size=12),
        legend.title = element_text(size=12),
        legend.key.width = unit(0.5,"in"),
        legend.key.height = unit(0.4,"in"), legend.position = "none" ) +
  scale_x_discrete(position = "top") + labs(title = "(C)")


png(filename = "./firepower_plot_Zr.png", width = 5, height = 5, units = "in", type = "windows", res = 400)
firepower_plot_Zr 
dev.off()

# assemble
png(filename = "./firepower.S_plot_assemble.png", width = 9.5, height = 8.5, units = "in", type = "windows", res = 400)
plot_grid(firepower.S_plot_lnRR,firepower.S_plot_SMD,firepower.S_plot_Zr,ncol=3)
dev.off()

```


# Firepower plot - M error

```{r}
# lnRR
intercept_lnRR <- data.frame(MA_case=MA_case_lnRR,
                             es=model_est_lnRR$beta0,
                             power=power.M_summary_lnRR$Median,
                             effect=rep(c("Sampling"),20),
                             es_cat=rep(c("a.Intercept_es"),20))


intercept_adjusted_lnRR <- data.frame(MA_case=MA_case_lnRR,
                                      es=model_est_lnRR$beta0_c3,
                                      power=power.M_c_summary_lnRR$Median,
                                      effect=rep(c("cSampling"),20),
                                      es_cat=rep(c("a.Intercept_es"),20))

MA.power_lnRR <- data.frame(MA_case=MA_case_lnRR,
                            es=model_est_lnRR$beta0,
                            power=model_est_lnRR$MA.power.M,
                            effect=rep(c("Meta-analysis"),20),
                            es_cat=rep(c("b.MA.power"),20))

MA.power_adjusted_lnRR <- data.frame(MA_case=MA_case_lnRR,
                                     es=model_est_lnRR$beta0_c3,
                                     power=model_est_lnRR$MA.power.M_c,
                                     effect=rep(c("cMeta-analysis"),20),
                                     es_cat=rep(c("b.MA.power"),20))


power.M_firepower_lnRR <- rbind(intercept_adjusted_lnRR,
                              intercept_lnRR,
                              MA.power_adjusted_lnRR,
                              MA.power_lnRR)

# convert MA_case to factor with desired ordering of levels
power.M_firepower_lnRR$MA_case <- factor(power_firepower_lnRR$MA_case, levels=rev(MA_case_lnRR))



firepower.M_plot_lnRR <- ggplot(data = power.M_firepower_lnRR) +
  geom_tile(aes(x = effect, y = MA_case, fill = power, width = 0.95, height = 0.25)) +
  theme(aspect.ratio = 0.3) +
  scale_fill_gradient(
    name = "Power",
    low = "white", # #56B4E9
    #mid = "white",
    high = "#56B4E9", 
    #midpoint = 0.5, # if using scale_fill_gradient2, it needs mid color and midpoint
    limits = c(0,10),
    guide = "colourbar",
  ) + 
  facet_grid( ~ es_cat, scale = 'free_x', space = "free_x") +
  theme_tufte(base_family = "Helvetica") +
  theme(strip.text.x = element_blank()) + 
  labs(x ="lnRR", y = "") + 
  theme(axis.text.x = element_text(size = 12, colour = "black", angle = -60, margin = margin(t = 0, r = 0, b = 5, l = 0)),
        axis.text.y = element_text(size = 12, colour = "black"),
        axis.title.x = element_text(size = 12, colour = "black", face = "bold"),
        legend.text = element_text(size=12),
        legend.title = element_text(size=12),
        legend.key.width = unit(0.3,"in"),
        legend.key.height = unit(0.4,"in"), legend.position = "bottom" ) +
  scale_x_discrete(position = "top") + labs(title = "(A)")


png(filename = "./firepower.M_plot_lnRR.png", width = 5, height = 5, units = "in", type = "windows", res = 400)
firepower.M_plot_lnRR 
dev.off()


pdf(file = "./Figures/figure3_colour_bar.pdf", width = 8, height = 10)
firepower_plot_RR 
dev.off()



# SMD
MA_case_SMD <- paste("SMD", sep = " ", 1:36) # nrow(model_est_SMD)

intercept_SMD <- data.frame(MA_case=MA_case_SMD,
                             es=model_est_SMD$beta0,
                             power=power.M_summary_SMD$Median,
                             effect=rep(c("Sampling"),36),
                             es_cat=rep(c("a.Intercept_es"),36))


intercept_adjusted_SMD <- data.frame(MA_case=MA_case_SMD,
                                      es=model_est_SMD$beta0_c3,
                                      power=power.M_c_summary_SMD$Median,
                                      effect=rep(c("cSampling"),36),
                                      es_cat=rep(c("a.Intercept_es"),36))

MA.power_SMD <- data.frame(MA_case=MA_case_SMD,
                            es=model_est_SMD$beta0,
                            power=model_est_SMD$MA.power.M,
                            effect=rep(c("Meta-analysis"),36),
                            es_cat=rep(c("b.MA.power"),36))

MA.power_adjusted_SMD <- data.frame(MA_case=MA_case_SMD,
                                     es=model_est_SMD$beta0_c3,
                                     power=model_est_SMD$MA.power.M_c,
                                     effect=rep(c("cMeta-analysis"),36),
                                     es_cat=rep(c("b.MA.power"),36))


power.M_firepower_SMD <- rbind(intercept_adjusted_SMD,
                              intercept_SMD,
                              MA.power_adjusted_SMD,
                              MA.power_SMD)

# convert MA_case to factor with desired ordering of levels
power.M_firepower_SMD$MA_case <- factor(power.M_firepower_SMD$MA_case, levels=rev(MA_case_SMD))


firepower.M_plot_SMD <- ggplot(data = power.S_firepower_SMD) +
  geom_tile(aes(x = effect, y = MA_case, fill = power, width = 0.95, height = 0.5)) +
  theme(aspect.ratio = 0.3) +
  scale_fill_gradient(
    name = "Power",
    low = "white", # #56B4E9
    #mid = "white",
    high = "#56B4E9", 
    #midpoint = 0.5, # if using scale_fill_gradient2, it needs mid color and midpoint
    limits = c(0,10),
    guide = "colourbar",
  ) + 
  facet_grid( ~ es_cat, scale = 'free_x', space = "free_x") +
  theme_tufte(base_family = "Helvetica") +
  theme(strip.text.x = element_blank()) + 
  labs(x ="SMD", y = "") + 
  theme(axis.text.x = element_text(size = 12, colour = "black", angle = -40, margin = margin(t = 0, r = 0, b = 5, l = 0)),
        axis.text.y = element_text(size = 12, colour = "black"),
        axis.title.x = element_text(size = 12, colour = "black", face = "bold"),
        legend.text = element_text(size=12),
        legend.title = element_text(size=12),
        legend.key.width = unit(0.5,"in"),
        legend.key.height = unit(0.4,"in"), legend.position = "none" ) +
  scale_x_discrete(position = "top") + labs(title = "(B)")


png(filename = "./firepower_plot_SMD.png", width = 5, height = 5, units = "in", type = "windows", res = 400)
firepower_plot_SMD 
dev.off()


# Zr
MA_case_Zr <- paste("Zr", sep = " ", 1:31) # nrow(model_est_Zr)

intercept_Zr <- data.frame(MA_case=MA_case_Zr,
                             es=model_est_Zr$beta0,
                             power=power.M_summary_Zr$Median,
                             effect=rep(c("Sampling"),31),
                             es_cat=rep(c("a.Intercept_es"),31))


intercept_adjusted_Zr <- data.frame(MA_case=MA_case_Zr,
                                      es=model_est_Zr$beta0_c3,
                                      power=power.M_c_summary_Zr$Median,
                                      effect=rep(c("cSampling"),31),
                                      es_cat=rep(c("a.Intercept_es"),31))

MA.power_Zr <- data.frame(MA_case=MA_case_Zr,
                            es=model_est_Zr$beta0,
                            power=model_est_Zr$MA.power.M,
                            effect=rep(c("Meta-analysis"),31),
                            es_cat=rep(c("b.MA.power"),31))

MA.power_adjusted_Zr <- data.frame(MA_case=MA_case_Zr,
                                     es=model_est_Zr$beta0_c3,
                                     power=model_est_Zr$MA.power.M_c,
                                     effect=rep(c("cMeta-analysis"),31),
                                     es_cat=rep(c("b.MA.power"),31))


power.M_firepower_Zr <- rbind(intercept_adjusted_Zr,
                              intercept_Zr,
                              MA.power_adjusted_Zr,
                              MA.power_Zr)

# convert MA_case to factor with desired ordering of levels
power.M_firepower_Zr$MA_case <- factor(power.M_firepower_Zr$MA_case, levels=rev(MA_case_Zr))




firepower.M_plot_Zr <- ggplot(data = power.M_firepower_Zr) +
  geom_tile(aes(x = effect, y = MA_case, fill = power, width = 0.95, height = 0.5)) +
  theme(aspect.ratio = 0.3) +
  scale_fill_gradient(
    name = "Power",
    low = "white", # #56B4E9
    #mid = "white",
    high = "#56B4E9", 
    #midpoint = 0.5, # if using scale_fill_gradient2, it needs mid color and midpoint
    limits = c(0,10),
    guide = "colourbar",
  ) + 
  facet_grid( ~ es_cat, scale = 'free_x', space = "free_x") +
  theme_tufte(base_family = "Helvetica") +
  theme(strip.text.x = element_blank()) + 
  labs(x =expression(italic("Zr")), y = "") + 
  theme(axis.text.x = element_text(size = 12, colour = "black", angle = -60, margin = margin(t = 0, r = 0, b = 5, l = 0)),
        axis.text.y = element_text(size = 12, colour = "black"),
        axis.title.x = element_text(size = 12, colour = "black", face = "bold"),
        legend.text = element_text(size=12),
        legend.title = element_text(size=12),
        legend.key.width = unit(0.5,"in"),
        legend.key.height = unit(0.4,"in"), legend.position = "none" ) +
  scale_x_discrete(position = "top") + labs(title = "(C)")


png(filename = "./firepower_plot_Zr.png", width = 5, height = 5, units = "in", type = "windows", res = 400)
firepower_plot_Zr 
dev.off()

# assemble
png(filename = "./firepower.M_plot_assemble.png", width = 9.5, height = 8.5, units = "in", type = "windows", res = 400)
plot_grid(firepower.M_plot_lnRR,firepower.M_plot_SMD,firepower.M_plot_Zr,ncol=3)
dev.off()

```




# Manhattan plot

```{r}
# use package qqman to make a Manhattan figure show power for each individual decline effect test
## lnRR
individual_est_lnRR2 <- individual_est_lnRR %>% mutate(es_group = rep("lnRR",nrow(individual_est_lnRR)))
colnames(individual_est_lnRR2) <- c("study_ID","power","power.S","power.M","es_group")

## SMD
individual_est_SMD2 <- individual_est_SMD %>% mutate(es_group = rep("SMD",nrow(individual_est_SMD)))
colnames(individual_est_SMD2) <- c("study_ID","power","power.S","power.M","es_group")

## Zr
individual_est_Zr2 <- individual_est_Zr %>% mutate(es_group = rep("Zr",nrow(individual_est_Zr)))
colnames(individual_est_Zr2) <- c("study_ID","power","power.S","power.M","es_group")

## combine together
Manhattan_dat <- rbind(individual_est_lnRR2, individual_est_SMD2, individual_est_Zr2) 

## make a Manhattan figure
Manhattan_dat$BP <- 1:nrow(Manhattan_dat)
# Manhattan_dat$case <- row.names(Manhattan_dat)
Manhattan_dat$es_group <- factor(Manhattan_dat$es_group, levels = c("lnRR", "SMD", "Zr"))

Manhattan_dat$CHR <- Manhattan_dat$es_group %>% as.numeric()






# statistical power
# png(filename = "./Manhattan_power.jpg", width = 6, height = 4, units = "in", res = 400, type = "windows")
pdf(NULL)
dev.control(displaylist="enable")
manhattan(Manhattan_dat, chr="CHR", bp="BP", snp="study_ID", p="power", logp=FALSE,
          col = c("#88CCEE", "#332288", "#DDCC77"),
          xlab = " ",
          ylab = "Statistical power",
          cex = 0.9,
          cex.axis = 0.9,
          ylim = c(0, 1)) # reduce the point size to 60% (cex=), and reduce the font size of the axis labels to 90% (cex.axis=).
abline(h=0.8128048, lwd=2, lty=1, col="#88CCEE") # add averaged meta-analysis level power to detect lnRR - MMA_MA.power_lnRR$coefficients%>%exp()
abline(h=0.4078456, lwd=2, lty=1, col="#332288") # add averaged meta-analysis level power to detect SMD - MMA_MA.power_SMD$coefficients%>%exp()
abline(h=0.8097946, lwd=2, lty=1, col="#DDCC77") # add averaged meta-analysis level power to detect Zr - MMA_MA.power_Zr$coefficients%>%exp()

abline(h=0.238011, lwd=2, lty=2, col="#88CCEE") # add averaged individual level power to detect lnRR - summary(MMA_EXP.power_lnRR)$coefficients[1] %>% exp()
abline(h=0.1918616, lwd=2, lty=2, col="#88CCEE") # add averaged individual level power to detect SMD - summary(MMA_EXP.power_SMD)$coefficients[1] %>% exp()
abline(h=0.281797, lwd=2, lty=2, col="#DDCC77") # add averaged individual level power to detect Zr - summary(MMA_EXP.power_Zr)$coefficients[1] %>% exp()
#  https://cran.r-project.org/web/packages/qqman/vignettes/qqman.html - The chromosome column must be numeric. If you have “X,” “Y,” or “MT” chromosomes, you'll need to rename these 23, 24, 25, etc. You can modify the source code (e.g., fix(manhattan)) to change the line designating the axis tick labels (labs <- unique(d$CHR)) to set this to whatever you'd like it to be - c("lnRR", "SMD", "Zr").
#dev.off()
Manhattan_power <- recordPlot() # turuning the base plot into ggplot plot <- ggdraw(plot)
invisible(dev.off())


# type S error
# png(filename = "./Manhattan_typeS.jpg", width = 6, height = 4, units = "in", res = 400, type = "windows")
pdf(NULL)
dev.control(displaylist="enable")
manhattan(Manhattan_dat, chr="CHR", bp="BP", snp="study_ID", p="power.S", logp=FALSE, genomewideline = FALSE,
          col = c("#88CCEE", "#332288", "#DDCC77"),
          xlab = " ",
          ylab = "Type S errors",
          cex = 0.9,
          cex.axis = 0.9,
          ylim = c(0, 0.5))
abline(h=0.02649693, lwd=2, lty=1, col="#88CCEE") # add averaged meta-analysis level typeS to detect lnRR - MMA_MA.power.S_lnRR$coefficients%>%exp()
abline(h=0.04255963, lwd=2, lty=1, col="#332288") # add averaged meta-analysis level typeS to detect SMD - MMA_MA.power.S_SMD$coefficients%>%exp()
abline(h= 0.02705107, lwd=2, lty=1, col="#DDCC77") # add averaged meta-analysis level typeS to detect Zr - MMA_MA.power.S_Zr$coefficients%>%exp()

abline(h=0.04759515, lwd=2, lty=2, col="#332288") # add averaged individual level typeS to detect lnRR - summary(MMA_EXP.power.S_lnRR)$coefficients[1] %>% exp()
abline(h=0.06363831, lwd=2, lty=2, col="#88CCEE") # add averaged individual level typeS to detect SMD - summary(MMA_EXP.power.S_SMD)$coefficients[1] %>% exp()
abline(h=0.03882332, lwd=2, lty=2, col="#DDCC77") # add averaged individual level typeS to detect Zr - summary(MMA_EXP.power.S_Zr)$coefficients[1] %>% exp()
# dev.off()
Manhattan_typeS <- recordPlot()
invisible(dev.off())


# type M error
#png(filename = "./Manhattan_typeM.jpg", width = 6, height = 4, units = "in", res = 400, type = "windows")
pdf(NULL)
dev.control(displaylist="enable")
manhattan(Manhattan_dat[Manhattan_dat$power.M<20,] , chr="CHR", bp="BP", snp="study_ID", p="power.M", logp=FALSE, genomewideline = FALSE,
                 col = c("#88CCEE", "#332288", "#DDCC77"),
                 xlab = " ",
                 ylab = "Type M errors",
                 cex = 0.9,
                 cex.axis = 0.9
                 )
abline(h=1.130857, lwd=2, lty=1, col="#88CCEE") # add averaged meta-analysis level typeM to detect lnRR - MMA_MA.power.M_lnRR$coefficients%>%exp()
abline(h=1.86473, lwd=2, lty=1, col="#332288") # add averaged meta-analysis level typeM to detect SMD - MMA_MA.power.M_SMD$coefficients%>%exp()
abline(h=1.142515, lwd=2, lty=1, col="#DDCC77") # add averaged meta-analysis level typeM to detect Zr - MMA_MA.power.M_Zr$coefficients%>%exp()

abline(h=2.499265, lwd=2, lty=2, col="#88CCEE") # add averaged individual level typeM to detect lnRR - summary(MMA_EXP.power.M_lnRR)$coefficients[1] %>% exp()
abline(h=3.475951, lwd=2, lty=2, col="#332288") # add averaged individual level typeM to detect SMD - summary(MMA_EXP.power.M_SMD)$coefficients[1] %>% exp()
abline(h=2.100849, lwd=2, lty=2, col="#DDCC77") # add averaged individual level typeM to detect Zr - summary(MMA_EXP.power.M_Zr)$coefficients[1] %>% exp()
#dev.off()
Manhattan_typeM <- recordPlot()
invisible(dev.off())


# assemble panels
png(filename = "./Manhattan_assemble.png", width = 5, height = 10, units = "in", res = 400, type = "windows")
plot_grid(Manhattan_power, Manhattan_typeS, Manhattan_typeM, labels = c("A","B","C"), nrow = 3)
dev.off()
```


# Paired plot

```{r}
# paired plot
dummy <- rep("Dummy", nrow(model_est_all_corrected_scaled))
# folded mean
wide.data <- 
  tibble::tibble(
    `Original` =  folded_es(mean=model_est_all_corrected_scaled$beta0,variance=model_est_all_corrected_scaled$se_beta0^2)*c(sd_lnRR,sd_SMD,sd_Zr),
    `Bias-corrected` = folded_es(mean=model_est_all_corrected_scaled$beta0_c,variance=model_est_all_corrected_scaled$se_beta0_c^2)*c(sd_lnRR,sd_SMD,sd_Zr),
     Dummy = dummy, ID = 1:length(dummy), case = model_est_all_corrected_scaled$case,es_type = model_est_all_corrected_scaled$es_type)

## find correct direction of slope
include.point <- which((wide.data$`Original` - wide.data$`Bias-corrected`)>0)
## table only includes correct data
wide.data2 <- wide.data[include.point,]


# use ggpaired() in ggpubr to show the pairwise comparisons 
# order
wide.data2$es_type <- factor(wide.data2$es_type,levels = c("lnRR", "SMD", "Zr"))
# facet by es_type
png(filename = "./paired.plot.png", width = 8, height = 3, units = "in", type = "windows", res = 400)
ggpaired(data = wide.data2, facet.by = "es_type", cond1 = "Original", cond2 = "Bias-corrected",fill = "condition", palette = "jco") + 
  scale_y_continuous(breaks = seq(0,1.2,0.3), limits = c(0,1.2)) +
  # scale_fill_manual(values = c("#E69F00", "#56B4E9")) + 
  guides(fill = "none") + labs(x = "", y = "Effect size estimate")
dev.off()
```



# ES VS. power vs. M vs. M
```{r}
library(retrodesign)
library(gridExtra)
library(ggplot2)

# creat different levels of effect sizes
possible_effects <- seq(0.01,1.01,by=0.01)

# calculate power, Type S and M with alpha 0.05
effect_power_s_m_0.05 <- retrodesign::retro_design(A = possible_effects, s = 0.25, alpha = 0.05)

# creat a dataframe
effect_pairs_0.05 <- data.frame(es = possible_effects, power = effect_power_s_m_0.05$power, types = effect_power_s_m_0.05$typeS, typem = effect_power_s_m_0.05$typeM, alpha = rep(c("0.05"),length(possible_effects)))


# es vs. power
power_plot <- ggplot(effect_pairs_0.05) + geom_line(aes(x=es,y=power),show.legend=F) + 
  scale_y_continuous(breaks = seq(0, 1, 0.2), limits = c(0, 1)) +
  #geom_vline(xintercept=0.25) +
  #geom_hline(yintercept=retrodesign::retro_design(A = 0.25, s = 0.2, alpha = 0.05)$power) + 
  geom_segment(aes(x = 0.15, y = 0, xend = 0.15, yend = retrodesign::retro_design(A = 0.15, s = 0.25, alpha = 0.05)$power), colour = "red", arrow = arrow(length = unit(0.15, "cm"), ends="first", type = "closed")) +  
  geom_segment(aes(x = 0, y = retrodesign::retro_design(A = 0.15, s = 0.25, alpha = 0.05)$power, xend = 0.15, yend = retrodesign::retro_design(A = 0.15, s = 0.25, alpha = 0.05)$power), colour = "red", arrow = arrow(length = unit(0.15, "cm"), ends="first", type = "closed")) +
  geom_point(aes(x = 0.15, y = retrodesign::retro_design(A = 0.15, s = 0.25, alpha = 0.05)$power), colour = "red", size = 3) +
  
  geom_segment(aes(x = 0.75, y = 0, xend = 0.75, yend = retrodesign::retro_design(A = 0.75, s = 0.25, alpha = 0.05)$power), colour = "blue", arrow = arrow(length = unit(0.15, "cm"), ends="first", type = "closed")) +  
  geom_segment(aes(x = 0, y = retrodesign::retro_design(A = 0.75, s = 0.25, alpha = 0.05)$power, xend = 0.75, yend = retrodesign::retro_design(A = 0.75, s = 0.25, alpha = 0.05)$power), colour = "blue", arrow = arrow(length = unit(0.15, "cm"), ends="first", type = "closed")) +
  geom_point(aes(x = 0.75, y = retrodesign::retro_design(A = 0.75, s = 0.25, alpha = 0.05)$power), colour = "blue", size = 3) +
  
  annotate(geom = "text", x = 0.33,  y = 0.06, label = "small effect", size = 3, colour = "red") +
  annotate(geom = "text", x = 0.1,  y = 0.18, label = "low power", size = 3, colour = "red") +
  annotate(geom = "text", x = 0.91,  y = 0.5, label = "large effect", size = 3, colour = "blue") +
  annotate(geom = "text", x = 0.38,  y = 0.9, label = "high power", size = 3, colour = "blue") +

  labs(x="Effect size", y="Statistical power") + theme_bw()

  
png(filename = "./power_plot.png", width=3, height=3, units="in", type="window", res=400)
power_plot
dev.off()


# es vs. typeS
s_plot <- ggplot(effect_pairs_0.05) + geom_line(aes(x=es,y=types),show.legend=F) + 
    geom_segment(aes(x = 0.1, y = 0, xend = 0.1, yend = retrodesign::retro_design(A = 0.1, s = 0.25, alpha = 0.05)$typeS), colour = "red", arrow = arrow(length = unit(0.15, "cm"), ends="first", type = "closed")) +  
  geom_segment(aes(x = 0, y = retrodesign::retro_design(A = 0.1, s = 0.25, alpha = 0.05)$typeS, xend = 0.1, yend = retrodesign::retro_design(A = 0.1, s = 0.25, alpha = 0.05)$typeS), colour = "red", arrow = arrow(length = unit(0.15, "cm"), ends="first", type = "closed")) +
  geom_point(aes(x = 0.1, y = retrodesign::retro_design(A = 0.1, s = 0.25, alpha = 0.05)$typeS), colour = "red", size = 3) +
  
  geom_segment(aes(x = 0.75, y = 0.15, xend = 0.75, yend = retrodesign::retro_design(A = 0.75, s = 0.25, alpha = 0.05)$typeS), colour = "blue", arrow = arrow(length = unit(0.15, "cm"), ends="first", type = "closed")) +  
  geom_segment(aes(x = 0, y = retrodesign::retro_design(A = 0.75, s = 0.25, alpha = 0.05)$typeS, xend = 0.75, yend = retrodesign::retro_design(A = 0.75, s = 0.25, alpha = 0.05)$typeS), colour = "blue", arrow = arrow(length = unit(0.15, "cm"), ends="first", type = "closed")) +
  geom_point(aes(x = 0.75, y = retrodesign::retro_design(A = 0.75, s = 0.25, alpha = 0.05)$typeS), colour = "blue", size = 3) +
  
  annotate(geom = "text", x = 0.28,  y = 0.07, label = "small effect", size = 3, colour = "red") +
  annotate(geom = "text", x = 0.185,  y = 0.17, label = "high Type S error", size = 3, colour = "red") +
  annotate(geom = "text", x = 0.75,  y = 0.17, label = "large effect", size = 3, colour = "blue") +
  annotate(geom = "text", x = 0.5,  y = 0.02, label = "low Type S error", size = 3, colour = "blue") +
  labs(x="Effect size", y="Type S error") + theme_bw()


png(filename = "./s_plot.png", width=3, height=3, units="in", type="window", res=400)
s_plot
dev.off()



# es vs. Type M
m_plot <- ggplot(effect_pairs_0.05) + geom_line(aes(x=es,y=typem),show.legend=F) + 
    geom_segment(aes(x = 0.1, y = 13, xend = 0.1, yend = retrodesign::retro_design(A = 0.1, s = 0.25, alpha = 0.05)$typeM), colour = "red", arrow = arrow(length = unit(0.15, "cm"), ends="first", type = "closed")) +  
  geom_segment(aes(x = 0, y = retrodesign::retro_design(A = 0.1, s = 0.25, alpha = 0.05)$typeM, xend = 0.1, yend = retrodesign::retro_design(A = 0.1, s = 0.25, alpha = 0.05)$typeM), colour = "red", arrow = arrow(length = unit(0.15, "cm"), ends="first", type = "closed")) +
  geom_point(aes(x = 0.1, y = retrodesign::retro_design(A = 0.1, s = 0.25, alpha = 0.05)$typeM), colour = "red", size = 3) +
  
  geom_segment(aes(x = 0.75, y = 10, xend = 0.75, yend = retrodesign::retro_design(A = 0.75, s = 0.25, alpha = 0.05)$typeM), colour = "blue", arrow = arrow(length = unit(0.15, "cm"), ends="first", type = "closed")) +  
  geom_segment(aes(x = 0, y = retrodesign::retro_design(A = 0.75, s = 0.25, alpha = 0.05)$typeM, xend = 0.75, yend = retrodesign::retro_design(A = 0.75, s = 0.25, alpha = 0.05)$typeM), colour = "blue", arrow = arrow(length = unit(0.15, "cm"), ends="first", type = "closed")) +
  geom_point(aes(x = 0.75, y = retrodesign::retro_design(A = 0.75, s = 0.25, alpha = 0.05)$typeM), colour = "blue", size = 3) +
  
  annotate(geom = "text", x = 0.1,  y = 16, label = "small effect", size = 3, colour = "red") +
  annotate(geom = "text", x = 0.29,  y = 8, label = "high M error", size = 3, colour = "red") +
  annotate(geom = "text", x = 0.75,  y = 12, label = "large effect", size = 3, colour = "blue") +
  annotate(geom = "text", x = 0.53,  y = 4, label = "low Type M error", size = 3, colour = "blue") +
    labs(x="Effect size", y="Type M error") + theme_bw()

png(filename = "./m_plot.png", width = 3, height = 3, units = "in", type = "windows", res = 400)
m_plot
dev.off()




png(filename = "./power_s_m.png", width = 9, height = 3, units = "in", type = "windows", res = 400)
power_plot + s_plot + m_plot + plot_layout(ncol = 3, nrow = 1)
dev.off()




# power vs. Type S
ggplot(effect_pairs_0.05) + geom_line(aes(x=power,y=types),show.legend=F) + labs(x="Statistical power", y="Type S error") + theme_bw() -> power_s_plot

# power vs. Type M
ggplot(effect_pairs_0.05) + geom_line(aes(x=power,y=typem),show.legend=F) + geom_vline(xintercept=0.8) + labs(x="Statistical power", y="Type M error") + theme_bw() -> power_m_plot

# Type S vs. Type M
ggplot(effect_pairs_0.05) + geom_line(aes(x=types,y=typem),show.l=F) + labs(x="Type S error", y="Type M error") + theme_bw() -> s_m_plot


png(filename = "./power_m_s.png", width=8, height=8, units="in", type="windows",res=400)
gridExtra::grid.arrange(power_plot, s_plot, 
                        m_plot, power_s_plot,
                        power_m_plot, s_m_plot,
                        nrow = 3, ncol = 2)
dev.off()


```


# Post-hoc analyses

#### Aggregation
```{r}
#***************************************************************#
#      estimate overall power for meta-analysis level power     #
#***************************************************************#

# divide dataset according to beta0's significance
## less than 0.05
model_est_all_corrected_original2_less0.05 <- filter(model_est_all_corrected_original2, pval_beta0 < 0.05)
## greater than 0.05
model_est_all_corrected_original2_greater0.05 <- filter(model_est_all_corrected_original2, pval_beta0 > 0.05)

#--------------------- (1) two tailed power ---------------------#
# meta-analytic overall mean
MMA_MA.power_all_less0.05 <- lm(log(MA.power) ~ 1, weights = k, data = model_est_all_corrected_original2_less0.05)
MMA_MA.power_all_greater0.05 <- lm(log(MA.power) ~ 1, weights = k, data = model_est_all_corrected_original2_greater0.05)

# this is median
MMA_MA.power_all_less0.05$coefficients %>% exp() 
MMA_MA.power_all_greater0.05$coefficients %>% exp() 

#confidence interval of median
confint(MMA_MA.power_all_less0.05) %>% exp()
confint(MMA_MA.power_all_greater0.05) %>% exp()

# bias-corrected version
MMA_MA.power_c_all_less0.05 <- lm(log(MA.power_c) ~ 1, weights = k, data = model_est_all_corrected_original2_less0.05)
MMA_MA.power_c_all_greater0.05 <- lm(log(MA.power_c) ~ 1, weights = k, data = model_est_all_corrected_original2_greater0.05)

# this is median
MMA_MA.power_c_all_less0.05$coefficients %>% exp()
MMA_MA.power_c_all_greater0.05$coefficients %>% exp()

#confidence interval of median
confint(MMA_MA.power_c_all_less0.05) %>% exp()
confint(MMA_MA.power_c_all_greater0.05) %>% exp()

#---------------------- (2) type S error -----------------------#
# meta-analytic overall mean
MMA_MA.power.S_all_less0.05 <- lm(log(MA.power.S+0.025) ~ 1, weights = k, data = model_est_all_corrected_original2_less0.05) # add an offset of 0.025(25%) to avoid ln(0) = infinity 
MMA_MA.power.S_all_greater0.05 <- lm(log(MA.power.S+0.025) ~ 1, weights = k, data = model_est_all_corrected_original2_greater0.05)

# this is median
MMA_MA.power.S_all_less0.05$coefficients %>% exp() - 0.025
MMA_MA.power.S_all_greater0.05$coefficients %>% exp() - 0.025

#confidence interval of median
confint(MMA_MA.power.S_all_less0.05) %>% exp() - 0.025 # if the lower boundary was negative (probably caused by the negative variance), we used 0 to replace it
confint(MMA_MA.power.S_all_greater0.05) %>% exp() - 0.025

# bias-corrected version
MMA_MA.power.S_c_all_less0.05 <- lm(log(MA.power.S_c+0.025) ~ 1, weights = k, data = model_est_all_corrected_original2_less0.05) # add an offset of 0.025(25%) to avoid ln(0) = infinity
MMA_MA.power.S_c_all_greater0.05 <- lm(log(MA.power.S_c+0.025) ~ 1, weights = k, data = model_est_all_corrected_original2_greater0.05)
# this is median
MMA_MA.power.S_c_all_less0.05$coefficients %>% exp() - 0.025
MMA_MA.power.S_c_all_greater0.05$coefficients %>% exp() - 0.025

#confidence interval of median
confint(MMA_MA.power.S_c_all_less0.05) %>% exp() - 0.025 # if the lower boundary was negative (probably caused by the negative variance), we used 0 to replace it
confint(MMA_MA.power.S_c_all_greater0.05) %>% exp() - 0.025

#-------------- (3) type M error (overestimate ratio) -------------#
# meta-analytic overall mean
MMA_MA.power.M_all_less0.05 <- lm(log(MA.power.M) ~ 1, weights = k, data = model_est_all_corrected_original2_less0.05)
MMA_MA.power.M_all_greater0.05 <- lm(log(MA.power.M) ~ 1, weights = k, data = model_est_all_corrected_original2_greater0.05)

# this is median
MMA_MA.power.M_all_less0.05$coefficients %>% exp() 
MMA_MA.power.M_all_greater0.05$coefficients %>% exp()  

#confidence interval of median
confint(MMA_MA.power.M_all_less0.05) %>% exp()
confint(MMA_MA.power.M_all_greater0.05) %>% exp()

# bias-corrected version
MMA_MA.power.M_c_all_less0.05 <- lm(log(MA.power.M_c) ~ 1, weights = k, data = model_est_all_corrected_original2_less0.05)
MMA_MA.power.M_c_all_greater0.05 <- lm(log(MA.power.M_c) ~ 1, weights = k, data = model_est_all_corrected_original2_greater0.05)
# this is median
MMA_MA.power.M_c_all_less0.05$coefficients %>% exp() 
MMA_MA.power.M_c_all_greater0.05$coefficients %>% exp() 
#confidence interval of median
confint(MMA_MA.power.M_c_all_less0.05) %>% exp()
confint(MMA_MA.power.M_c_all_greater0.05) %>% exp()

#***************************************************************#
#      estimate overall power for experimental level power     #
#***************************************************************#

# add p value of beta0 to dataset
individual_est_all2 <- individual_est_all
individual_est_all2$pval_beta0 <- c(rep(model_est_lnRR$pval_beta0[1], model_est_lnRR$k[1]),
                                    rep(model_est_lnRR$pval_beta0[2], model_est_lnRR$k[2]),
                                    rep(model_est_lnRR$pval_beta0[3], model_est_lnRR$k[3]),
                                    rep(model_est_lnRR$pval_beta0[4], model_est_lnRR$k[4]),
                                    rep(model_est_lnRR$pval_beta0[5], model_est_lnRR$k[5]),
                                    rep(model_est_lnRR$pval_beta0[6], model_est_lnRR$k[6]),
                                    rep(model_est_lnRR$pval_beta0[7], model_est_lnRR$k[7]),
                                    rep(model_est_lnRR$pval_beta0[8], model_est_lnRR$k[8]),
                                    rep(model_est_lnRR$pval_beta0[9], model_est_lnRR$k[9]),
                                    rep(model_est_lnRR$pval_beta0[10], model_est_lnRR$k[10]),
                                    rep(model_est_lnRR$pval_beta0[11], model_est_lnRR$k[11]),
                                    rep(model_est_lnRR$pval_beta0[12], model_est_lnRR$k[12]),
                                    rep(model_est_lnRR$pval_beta0[13], model_est_lnRR$k[13]),
                                    rep(model_est_lnRR$pval_beta0[14], model_est_lnRR$k[14]),
                                    rep(model_est_lnRR$pval_beta0[15], model_est_lnRR$k[15]),
                                    rep(model_est_lnRR$pval_beta0[16], model_est_lnRR$k[16]),
                                    rep(model_est_lnRR$pval_beta0[17], model_est_lnRR$k[17]),
                                    rep(model_est_lnRR$pval_beta0[18], model_est_lnRR$k[18]),
                                    rep(model_est_lnRR$pval_beta0[19], model_est_lnRR$k[19]),
                                    rep(model_est_lnRR$pval_beta0[20], model_est_lnRR$k[20]),
                                    rep(model_est_SMD$pval_beta0[1], model_est_SMD$k[1]),
                                    rep(model_est_SMD$pval_beta0[2], model_est_SMD$k[2]),
                                    rep(model_est_SMD$pval_beta0[3], model_est_SMD$k[3]),
                                    rep(model_est_SMD$pval_beta0[4], model_est_SMD$k[4]),
                                    rep(model_est_SMD$pval_beta0[5], model_est_SMD$k[5]),
                                    rep(model_est_SMD$pval_beta0[6], model_est_SMD$k[6]),
                                    rep(model_est_SMD$pval_beta0[7], model_est_SMD$k[7]),
                                    rep(model_est_SMD$pval_beta0[8], model_est_SMD$k[8]),
                                    rep(model_est_SMD$pval_beta0[9], model_est_SMD$k[9]),
                                    rep(model_est_SMD$pval_beta0[10], model_est_SMD$k[10]),
                                    rep(model_est_SMD$pval_beta0[11], model_est_SMD$k[11]),
                                    rep(model_est_SMD$pval_beta0[12], model_est_SMD$k[12]),
                                    rep(model_est_SMD$pval_beta0[13], model_est_SMD$k[13]),
                                    rep(model_est_SMD$pval_beta0[14], model_est_SMD$k[14]),
                                    rep(model_est_SMD$pval_beta0[15], model_est_SMD$k[15]),
                                    rep(model_est_SMD$pval_beta0[16], model_est_SMD$k[16]),
                                    rep(model_est_SMD$pval_beta0[17], model_est_SMD$k[17]),
                                    rep(model_est_SMD$pval_beta0[18], model_est_SMD$k[18]),
                                    rep(model_est_SMD$pval_beta0[19], model_est_SMD$k[19]),
                                    rep(model_est_SMD$pval_beta0[20], model_est_SMD$k[20]),
                                    rep(model_est_SMD$pval_beta0[21], model_est_SMD$k[21]),
                                    rep(model_est_SMD$pval_beta0[22], model_est_SMD$k[22]),
                                    rep(model_est_SMD$pval_beta0[23], model_est_SMD$k[23]),
                                    rep(model_est_SMD$pval_beta0[24], model_est_SMD$k[24]),
                                    rep(model_est_SMD$pval_beta0[25], model_est_SMD$k[25]),
                                    rep(model_est_SMD$pval_beta0[26], model_est_SMD$k[26]),
                                    rep(model_est_SMD$pval_beta0[27], model_est_SMD$k[27]),
                                    rep(model_est_SMD$pval_beta0[28], model_est_SMD$k[28]),
                                    rep(model_est_SMD$pval_beta0[29], model_est_SMD$k[29]),
                                    rep(model_est_SMD$pval_beta0[30], model_est_SMD$k[30]),
                                    rep(model_est_SMD$pval_beta0[31], model_est_SMD$k[31]),
                                    rep(model_est_SMD$pval_beta0[32], model_est_SMD$k[32]),
                                    rep(model_est_SMD$pval_beta0[33], model_est_SMD$k[33]),
                                    rep(model_est_SMD$pval_beta0[34], model_est_SMD$k[34]),
                                    rep(model_est_SMD$pval_beta0[35], model_est_SMD$k[35]),
                                    rep(model_est_SMD$pval_beta0[36], model_est_SMD$k[36]),
                                    rep(model_est_Zr$pval_beta0[1], model_est_Zr$k[1]),
                                    rep(model_est_Zr$pval_beta0[2], model_est_Zr$k[2]),
                                    rep(model_est_Zr$pval_beta0[3], model_est_Zr$k[3]),
                                    rep(model_est_Zr$pval_beta0[4], model_est_Zr$k[4]),
                                    rep(model_est_Zr$pval_beta0[5], model_est_Zr$k[5]),
                                    rep(model_est_Zr$pval_beta0[6], model_est_Zr$k[6]),
                                    rep(model_est_Zr$pval_beta0[7], model_est_Zr$k[7]),
                                    rep(model_est_Zr$pval_beta0[8], model_est_Zr$k[8]),
                                    rep(model_est_Zr$pval_beta0[9], model_est_Zr$k[9]),
                                    rep(model_est_Zr$pval_beta0[10], model_est_Zr$k[10]),
                                    rep(model_est_Zr$pval_beta0[11], model_est_Zr$k[11]),
                                    rep(model_est_Zr$pval_beta0[12], model_est_Zr$k[12]),
                                    rep(model_est_Zr$pval_beta0[13], model_est_Zr$k[13]),
                                    rep(model_est_Zr$pval_beta0[14], model_est_Zr$k[14]),
                                    rep(model_est_Zr$pval_beta0[15], model_est_Zr$k[15]),
                                    rep(model_est_Zr$pval_beta0[16], model_est_Zr$k[16]),
                                    rep(model_est_Zr$pval_beta0[17], model_est_Zr$k[17]),
                                    rep(model_est_Zr$pval_beta0[18], model_est_Zr$k[18]),
                                    rep(model_est_Zr$pval_beta0[19], model_est_Zr$k[19]),
                                    rep(model_est_Zr$pval_beta0[20], model_est_Zr$k[20]),
                                    rep(model_est_Zr$pval_beta0[21], model_est_Zr$k[21]),
                                    rep(model_est_Zr$pval_beta0[22], model_est_Zr$k[22]),
                                    rep(model_est_Zr$pval_beta0[23], model_est_Zr$k[23]),
                                    rep(model_est_Zr$pval_beta0[24], model_est_Zr$k[24]),
                                    rep(model_est_Zr$pval_beta0[25], model_est_Zr$k[25]),
                                    rep(model_est_Zr$pval_beta0[26], model_est_Zr$k[26]),
                                    rep(model_est_Zr$pval_beta0[27], model_est_Zr$k[27]),
                                    rep(model_est_Zr$pval_beta0[28], model_est_Zr$k[28]),
                                    rep(model_est_Zr$pval_beta0[29], model_est_Zr$k[29]),
                                    rep(model_est_Zr$pval_beta0[30], model_est_Zr$k[30]),
                                    rep(model_est_Zr$pval_beta0[31], model_est_Zr$k[31]))


# divide dataset according to beta0's significance
## less than 0.05
individual_est_all2_less0.05 <- filter(individual_est_all2, pval_beta0 < 0.05)
## greater than 0.05
individual_est_all2_greater0.05 <- filter(individual_est_all2, pval_beta0 > 0.05)

#--------------------- (1) two tailed power ---------------------#
# meta-analytic overall mean
MMA_EXP.power_all_less0.05 <- lmer(log(power_all) ~ 1 + (1 | study_ID_all), data = individual_est_all2_less0.05)
MMA_EXP.power_all_greater0.05 <- lmer(log(power_all) ~ 1 + (1 | study_ID_all), data = individual_est_all2_greater0.05)

# this is median 
summary(MMA_EXP.power_all_less0.05)$coefficients[1] %>% exp()
summary(MMA_EXP.power_all_greater0.05)$coefficients[1] %>% exp()
# confidence interval of median
confint(MMA_EXP.power_all_less0.05) %>% exp()
confint(MMA_EXP.power_all_greater0.05) %>% exp()

# bias-corrected version
MMA_EXP.power_c_all_less0.05 <- lmer(log(power_c_all) ~ 1 + (1 | study_ID_all), data = individual_est_all2_less0.05)
MMA_EXP.power_c_all_greater0.05 <- lmer(log(power_c_all) ~ 1 + (1 | study_ID_all), data = individual_est_all2_greater0.05)

# this is median 
summary(MMA_EXP.power_c_all_less0.05)$coefficients[1] %>% exp()
summary(MMA_EXP.power_c_all_greater0.05)$coefficients[1] %>% exp()

# confidence interval of median
confint(MMA_EXP.power_c_all_less0.05) %>% exp()
confint(MMA_EXP.power_c_all_greater0.05) %>% exp()

#---------------------- (2) type S error -----------------------#
# meta-analytic overall mean
MMA_EXP.power.S_all_less0.05 <- lmer( log(power.S_all + 0.025) ~ 1 + (1 | study_ID_all), data = individual_est_all2_less0.05) # add an offset of 0.025 to avoid log(0) = inf
MMA_EXP.power.S_all_greater0.05 <- lmer( log(power.S_all + 0.025) ~ 1 + (1 | study_ID_all), data = individual_est_all2_greater0.05)
# this is median 
summary(MMA_EXP.power.S_all_less0.05)$coefficients[1] %>% exp() - 0.025 # - 0.025 is important
summary(MMA_EXP.power.S_all_greater0.05)$coefficients[1] %>% exp() - 0.025

#confidence interval of median
confint(MMA_EXP.power.S_all_less0.05) %>% exp() - 0.025 # if the lower boundary was negative (probably caused by the negative variance), we used 0 to replace it
confint(MMA_EXP.power.S_all_greater0.05) %>% exp() - 0.025

# bias-corrected version
MMA_EXP.power.S_c_all_less0.05 <- lmer( log(power.S_c_all + 0.025) ~ 1 + (1 | study_ID_all), data = individual_est_all2_less0.05) # add an offset of 0.025 to avoid log(0) = inf
MMA_EXP.power.S_c_all_greater0.05 <- lmer( log(power.S_c_all + 0.025) ~ 1 + (1 | study_ID_all), data = individual_est_all2_greater0.05)

# this is median 
summary(MMA_EXP.power.S_c_all_less0.05)$coefficients[1] %>% exp() - 0.025 # - 0.025 is important
summary(MMA_EXP.power.S_c_all_greater0.05)$coefficients[1] %>% exp() - 0.025 # - 0.025 is important

#confidence interval of median
confint(MMA_EXP.power.S_c_all_less0.05) %>% exp() - 0.025 # if the lower boundary was negative (probably caused by the negative variance), we used 0 to replace it
confint(MMA_EXP.power.S_c_all_greater0.05) %>% exp() - 0.025

#--------------------- (3) type M error ---------------------#
MMA_EXP.power.M_all_less0.05 <- lmer(log(power.M_all) ~ 1 + (1 | study_ID_all), data = individual_est_all2_less0.05)
MMA_EXP.power.M_all_greater0.05 <- lmer(log(power.M_all) ~ 1 + (1 | study_ID_all), data = individual_est_all2_greater0.05)

# this is median 
summary(MMA_EXP.power.M_all_less0.05)$coefficients[1] %>% exp()
summary(MMA_EXP.power.M_all_greater0.05)$coefficients[1] %>% exp()

# confidence interval of median
confint(MMA_EXP.power.M_all_less0.05) %>% exp()
confint(MMA_EXP.power.M_all_greater0.05) %>% exp()

# bias-corrected version
MMA_EXP.power.M_c_all_less0.05 <- lmer(log(power.M_c_all) ~ 1 + (1 | study_ID_all), data = individual_est_all2_less0.05)
MMA_EXP.power.M_c_all_greater0.05 <- lmer(log(power.M_c_all) ~ 1 + (1 | study_ID_all), data = individual_est_all2_greater0.05)

# this is median 
summary(MMA_EXP.power.M_c_all_less0.05)$coefficients[1] %>% exp()
summary(MMA_EXP.power.M_c_all_greater0.05)$coefficients[1] %>% exp()
# confidence interval of median
confint(MMA_EXP.power.M_c_all_less0.05) %>% exp()
confint(MMA_EXP.power.M_c_all_greater0.05) %>% exp()
```


