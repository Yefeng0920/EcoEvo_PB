---
title: "Untitled"
author: "Yefeng Yang"
date: "2022/4/4"
output: html_document
---

# setup

```{r setup, echo = FALSE}
# Tidy
 # rm(list=ls())
 # graphics.off()

# Preparing workspace
knitr::opts_chunk$set(echo = TRUE, include = TRUE)

# Loading packages
pacman::p_load(knitr, # knit markdown
               readxl, 
               readr, 
               metafor, 
               dplyr, 
               tidyverse, 
               janitor, # generate 1-, 2-way table
               patchwork, # layout of plots
               cowplot, 
               ggpubr,
               gridExtra,
               orchaRd, # v2.0, a new version
               gridGraphics, # Redraw Base Graphics Using 'grid' Graphics. `gridGraphics` is required to handle base-R plots.
               dabestr,
               here,
               retrodesign,
               lme4,
               car, # logit transformation, car::logit()
               boot, # Bootstrap Resampling
               lmerTest, # get p-valus from lme4 model
               ggthemes,
               modi # weighted variance
               )


# Function to calculate power (two-tail) for meta-analysis
power.ma_Shinichi <- function(mu, SE, alpha = 0.05) {
  2-pnorm(qnorm(1-alpha/2)-abs(mu)/SE)-pnorm(qnorm(1-alpha/2)+abs(mu)/SE)
  } # or power.ma_Shinichi1 <- function(mu,SE){1 - pnorm(qnorm(1-0.05/2)-abs(mu)/SE) + pnorm(-qnorm(1-0.05/2)-abs(mu)/SE)}


# Function for power analysis for empirical data point
power.individual_Shinichi <- function(mu, se, alpha = 0.05) {
  2-pnorm(qnorm(1-alpha/2)-abs(mu)/se)-pnorm(qnorm(1-alpha/2)+abs(mu)/se)} # two-tailed power


# Function for Type S error for empirical data point
error_S <- function(mu, se, alpha = 0.05){
  #z <- qnorm(1 - alpha/2) # Z-score or quantile
  p.u <- 1 - pnorm(qnorm(1 - alpha/2) - abs(mu)/se) # upper-tail probability
  p.l <- pnorm(-qnorm(1 - alpha/2) - abs(mu)/se) # lower-tail probability
  power <- p.u + p.l # upper + lower
  errorS <- p.l/power # percentage of the opposite direction
  return(errorS)
} 

# Function for Type M error for empirical data point
error_M <- function(mu, se, alpha = 0.05, N = 10000) {
    est.random <- rnorm(n=N, mean = mu, sd = se)
    # est.random <- mu + se*rnorm(n=N, mean=0, sd=1)
    sig.index <- abs(est.random) > se*qnorm(1 - alpha/2)
    overestimate <- mean(abs(est.random)[sig.index])/abs(mu) # ratio is regardnesss of sign, so we need absolute value
    absolute_error <- overestimate*abs(mu) - abs(mu)
    relative_error <- absolute_error/(overestimate*abs(mu))
  return(abs(overestimate) %>% round(3))
}


error_M2 <- function(mu, se, alpha = 0.05, N = 10000) {
    est.random <- rnorm(n=N, mean = mu, sd = se)
    # est.random <- mu + se*rnorm(n=N, mean=0, sd=1)
    sig.index <- abs(est.random) > se*qnorm(1 - alpha/2)
    overestimate <- mean(abs(est.random)[sig.index])/abs(mu) # ratio is regardnesss of sign, so we need absolute value
    absolute_error <- overestimate*abs(mu) - abs(mu)
    relative_error <- absolute_error/(overestimate*abs(mu))
  return(abs(relative_error) %>% round(3))
} # relative error: (M - 1) / M



# meta-analysis of magnitude
## folded effect size
folded_es <-function(mean, variance){ # the sampling variance of magnitude   
  mu <- mean
  sigma <- sqrt(variance)
  fold_mu <- sigma*sqrt(2/pi)*exp((-mu^2)/(2*sigma^2)) + mu*(1 - 2*pnorm(-mu/sigma))
  fold_mu
}
## folded error
folded_error <- function(mean, variance){ # the sampling variance of magnitude   
  mu <- mean
  sigma <- sqrt(variance)
  fold_mu <- sigma*sqrt(2/pi)*exp((-mu^2)/(2*sigma^2)) + mu*(1 - 2*pnorm(-mu/sigma))
  fold_se <- sqrt(mu^2 + sigma^2 - fold_mu^2)
  # adding se to make bigger mean
  fold_v <- fold_se^2
  fold_v
}


# custom function for extracting mean and CI from each metafor model
estimates.CI <- function(model){
  db.mf <- data.frame(model$b,row.names = 1:nrow(model$b))
  db.mf <- cbind(db.mf,model$ci.lb,model$ci.ub,row.names(model$b))
  names(db.mf) <- c("mean","lower","upper","estimate")
  return(db.mf[,c("estimate","mean","lower","upper")])
}
```


# 1. import data and pre-process

(1) import data
we have two subsets: pre-calculated effect sizes and descriptive statistics

(2) compute effect size and sampling variance for subset with descriptive statistics

(3) calculate effective sample size related variables for for subset with descriptive statistics

(4) scale/center continuous variables
```{r}
#*************************************************************************#
#       import datasets with pre-calculated effect sizes                    
#*************************************************************************#

## import SMD datasets with pre-calculated effect size (es) and sampling variance (var)
SMD_csv <- list.files(path = "./SMD", pattern = "*.csv", full.names = TRUE) %>% lapply(read_csv)

### get names of each .csv file
SMD_filenames <- list.files(path = "./SMD", pattern = "*.csv", full.names = FALSE)

### rename the elements of the list
names(SMD_csv) <- SMD_filenames


#*************************************************************************#
#                      import datasets with raw data                    
#*************************************************************************#

## import SMD datasets with descriptive statistics (mean, sd and sample size)
SMD_des_csv <- list.files(path = "./SMD/des_stat", pattern = "*.csv", full.names = TRUE) %>% lapply(read_csv)
SMD_des_filenames <- list.files(path = "./SMD/des_stat", pattern = "*.csv", full.names = FALSE) # extract file names, which will be used later

## for datasets with descriptive statistics, we also need to create a variable named 'effective sample size' for each of them 
## effective sample size makes the examination of the small-study effect more statistically sound (see below or the main text for explanations)

## function to calculate effective sample size
ess_cal <- function(dat){(4*dat$C_n*dat$T_n) / (dat$C_n + dat$T_n)}

## calculate effective sample size for SMD with descriptive statistics
ess <- NA
for (i in 1:length(SMD_des_csv)) {
  ess[i] <- ess_cal(SMD_des_csv[[i]]) %>% list()}

### allocate each set of effective sample size into corresponding dataset
for (i in 1:length(SMD_des_csv)) {
  SMD_des_csv[[i]]$ess <- ess[[i]]
}
 
### create inverse effective sample size - "effective sample size" based "sampling variance" 
### function to calculate inverse of effective sample size 
ess.var_cal <- function(dat){1/dat$C_n + 1/dat$T_n}

### calculations for SMD
ess.var <- NA
for (i in 1:length(SMD_des_csv)) {
  ess.var[i] <- ess.var_cal(SMD_des_csv[[i]]) %>% list()}

### allocate each set of effective sample size into corresponding dataset
for (i in 1:length(SMD_des_csv)) {
  SMD_des_csv[[i]]$ess.var <- ess.var[[i]]
}

### calculate inverse sqrt of effective sample size - "effective sample size" based "sampling error" 
for (i in 1:length(SMD_des_csv)) {
  SMD_des_csv[[i]]$ess.sei <- sqrt(SMD_des_csv[[i]]$ess.var)
}


## calculate effect size for datasets with descriptive statistics
SMD_es <- NA
for (i in 1:length(SMD_des_csv)) {
  SMD_es[i] <- escalc(measure = "SMD",
                    m1i = T_mean,
                    m2i = C_mean,
                    sd1i = T_sd,
                    sd2i = C_sd,
                    n1i = T_n,
                    n2i = C_n,
                    data = SMD_des_csv[[i]]) %>% list()
}

### rename the elements of the list
names(SMD_es) <- SMD_des_filenames

## to keep consistent, rename effect size and sampling variance
for (i in 1:length(SMD_es)) {
  names(SMD_es[[i]])[names(SMD_es[[i]]) == "yi"] <- "es"
  names(SMD_es[[i]])[names(SMD_es[[i]]) == "vi"] <- "var"
}

## combine two sets of dataset for SMD (
SMD <- append(SMD_csv, SMD_es)

## remove NAs, zero variance, and +-Inf
### delete NAs, zero variance
for (i in 1:length(SMD)) {
  SMD[[i]] <- SMD[[i]][!is.na(SMD[[i]]$es) & !is.na(SMD[[i]]$var) & SMD[[i]]$var != 0 & !is.na(SMD[[i]]$year_pub), ]
}
### delete +-Inf
for (i in 1:length(SMD)) {
  SMD[[i]] <- SMD[[i]] %>% na.omit()
}



## create the variable of latest-year-centring publication year, which is used as a predictor to test time-lag bias (decline effect). The reason why creating this variable is to set the intercept conditional on the latest year rather than zero year (details see the main text)

## SMD
for (i in 1:length(SMD)) {
  SMD[[i]]$year_pub.l <- as.vector(SMD[[i]]$year_pub - max(SMD[[i]]$year_pub))
}


## create the variable of sampling error, which was used as a predictor to test small-study effect
for (i in 1:length(SMD)) {
  SMD[[i]]$sei <- sqrt(SMD[[i]]$var)
}


## transform effect size, sei, and year_pub.l prior to model fitting. This  is to eliminate scale-dependency and to allow for aggregations of model coefficients over different effect size metrics in subsequent second-order meta-analysis

## scale data using respective standard deviation
for (i in 1:length(SMD)) {
  # scale effect size
  SMD[[i]]$es_zscore <- scale(SMD[[i]]$es, center = F, scale = TRUE) # without centering, which is used to estimate intercept; otherwise, the intercept will be zero
  # scale sampling variance
  SMD[[i]]$var_zscore <- scale(SMD[[i]]$var, scale = TRUE) - ( (0-mean(SMD[[i]]$var))/sd(SMD[[i]]$var) ) # see Equation 7 for explanations
  # scale sampling error
  SMD[[i]]$sei_zscore <- scale(SMD[[i]]$sei, scale = TRUE) - ( (0-mean(SMD[[i]]$sei))/sd(SMD[[i]]$sei) ) # see Equation 7 for explanations; should we use sqrt(var_zscore)?
  # scale year
  SMD[[i]]$year_pub.l_zscore <- scale(SMD[[i]]$year_pub.l, scale = TRUE)
}

## also need to scale effective-sample-size-related variables
## note that only SMD_des_filenames contains descriptive statistics, so we only need to scale this subset

for (i in 1:length(SMD[SMD_des_filenames])) {
  # scale effective-sample-size based sampling variance 
  SMD[SMD_des_filenames][[i]]$ess.var_zscore <- scale(SMD[SMD_des_filenames][[i]]$ess.var, scale = TRUE) - ( (0-mean(SMD[SMD_des_filenames][[i]]$ess.var))/sd(SMD[SMD_des_filenames][[i]]$ess.var) ) # see Equation 7 for explanations
  # scale effective-sample-size based sampling error
  SMD[SMD_des_filenames][[i]]$ess.sei_zscore <- scale(SMD[SMD_des_filenames][[i]]$ess.sei, scale = TRUE) - ( (0-mean(SMD[SMD_des_filenames][[i]]$ess.sei))/sd(SMD[SMD_des_filenames][[i]]$ess.sei) )
}

```



# 2. multilevel modelling 

we will use multilevel meta-analytic model to fit two types data:
2.1 original scale data
2.2 scaled data

For each type of data, we:
(i) estimate the meta-analytic overall mean, model intercept (beta0)
(ii) detect potential publication bias - test for small-study (sign & significance of beta1) and decline effects (sign & significance of beta1)
(iii) correct for publication bias and estimate bias-corrected overall mean (beta0_c)


# 2.1 original scale data

## (i) estimates of beta0
fit intercept-only multilevel model to each dataset
```{r}

#*************************************************************************#
#                      meta-analytic overall mean, beta0                    
#*************************************************************************#

model_SMD <- NA
for (i in 1:length(SMD)) {
  model_SMD[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), method = "REML", test = "t", data = SMD[[i]], sparse = TRUE, control = list(optimizer = "optim")) %>% list()
}
```


## (ii) detect publication bias

we aim for detecting two forms of publication bias: small-study effect  and decline effect.

we use a full model with sampling error (sei) and publication year (year_pub.l) as moderators to detect publication bias.

of relevance, sei's slope (beta1) and year_pub.l's slope (beta2) can be used to indicate the occurrence of small-study effect and decline effect, respectively.

note that for SMD we need to model 'effective sample size' based sampling error analogue (ees.sei), where possible

### (a) fit full models with samping error and year as predictors
```{r}

#*************************************************************************#
#       Full model with error and latest year as predictors
#*************************************************************************#

## use sampling error (sei) and latest year (year_pub.l) as predictors for those with pre-calculated effect sizes and sampling variance

model_SMD_sei.year <- NA
for (i in 1:length(SMD[SMD_filenames])) {
  model_SMD_sei.year[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ sei + year_pub.l, method = "REML", test = "t", data = SMD[SMD_filenames][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
} # note that here only fit the subset of SMD[SMD_filenames] - SMD with pre-calculated effect sizes and sampling variance

## use effective-sample-size based sampling error (ess.sei) where possible (SMD with descriptive statistics)
model_SMD_ess.sei.year <- NA
for (i in 1:length(SMD[SMD_des_filenames])) {
  model_SMD_ess.sei.year[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ ess.sei + year_pub.l, method = "REML", test = "t", data = SMD[SMD_des_filenames][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
} 

```

### (b) identify the presence of publication bias

we next aim for identify the presence of the small-study effect and decline effect for each meta-analysis. 

our rational is: for an effect that is expected to be positive, a small study effect and decline effect would be expressed in a positive value of beta1 and negative value of beta2, respectively. In this respect, a slope (beta1 or beta2)) with opposing direction (unexpected sign) indicates no detectable publication bias and subsequently does not require correction for such a bias.

we use the product of beta0 and beta1 (i.e., beta×beta1) as the signal, that is, if beta0×beta1 is positive, it indicates the examined meta-analysis has a small-study effect (beta1 is in a correct direction).

similarly, when the product of beta0×beta2 is negative, the examined meta-analysis has a decline effect (beta 2 is in a correct direction)
```{r}
## combine two types of models: one is fitted with sampling error (sei), the other is fitted with effective sample size (ess.sei)
model_SMD_pb <- append(model_SMD_sei.year, model_SMD_ess.sei.year)

## extract model model coefficients and their significance test results
model_est_SMD <- data.frame(case = names(SMD),
                             es_type = rep("SMD", length(SMD)),
                             beta0 = sapply(model_SMD, function(x) x$beta), # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = sapply(model_SMD, function(x) x$se), # standard error of beta0
                             pval_beta0 = sapply(model_SMD, function(x) x$pval), # p value of beta0
                             beta0_c = sapply(model_SMD_pb, function(x) x$beta[1]), # beta0_c in equation 2 -  bias corrected overall mean
                             se_beta0_c = sapply(model_SMD_pb, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_SMD_pb, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_SMD_pb, function(x) x$beta[2]), # beta1 in Equation 2 - slope of sampling error 
                             se_beta1 = sapply(model_SMD_pb, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_SMD_pb, function(x) x$pval[2]), # p value of beta1
                             beta2 = sapply(model_SMD_pb, function(x) x$beta[3]), # beta2 in Equation 2 - slope of year 
                             se_beta2 = sapply(model_SMD_pb, function(x) x$se[3]), # standard error of beta2
                             pval_beta2 = sapply(model_SMD_pb, function(x) x$pval[3]) # p value of beta2
                            )

## we first to create two new columns to contain the two products:  beta0*beta1 and beta0*beta2

model_est_SMD[15:16] <- data.frame(beta0Tbeta1 = model_est_SMD$beta0 * model_est_SMD$beta1, beta0Tbeta2 = model_est_SMD$beta0 * model_est_SMD$beta2) # model_est_SMD has 14 column (ncol(model_est_SMD)), so we add columns 15 and 16

## visual check
model_est_SMD

## identify the small-study effect - significant beta1 with correct sign
sse_SMD <- model_est_SMD %>% subset(model_est_SMD$pval_beta1 < 0.05 & model_est_SMD$beta0Tbeta1 > 0)
## check which meta-analyses have small-study effects
sse_SMD$case 

## identify the decline effect - significant beta2 with correct sign 
de_SMD <- model_est_SMD %>% subset(model_est_SMD$pval_beta2 < 0.05 & model_est_SMD$beta0Tbeta2 < 0)
## check which meta-analyses have decline effects
de_SMD$case # 

## identify the concurrence of the small-study effect and decline effect
sse_de_SMD <- model_est_SMD %>% subset(model_est_SMD$pval_beta1 < 0.05 & model_est_SMD$beta0Tbeta1 > 0 & model_est_SMD$pval_beta2 < 0.05 & model_est_SMD$beta0Tbeta2 < 0) 
sse_de_SMD$case # no meta-analysis surfers from both a small-study effect and decline effect
```


## (iii) correct for publication bias  

we aim for estimate bias-corrected beta0 according to 4 scenarios as outlined in the main text (permutation of the signs of beta1 and beta2)

```{r}

#*************************************************************************#
#        Multilevel models to estimate bias-corrected effect
#*************************************************************************#

#*****************************scenario 1****************************#
## both beta1 and beta2 has a correct direction
beta1c_beta2c_SMD <- model_est_SMD %>% subset(model_est_SMD$beta0Tbeta1 > 0 & model_est_SMD$beta0Tbeta2 < 0) 
beta1c_beta2c_SMD$case 

## if a model slope (beta1 and beta2) has a wrong direction, we need to take out it when fitting model to estimate the bias-corrected mean
## in scenario 1, both of the two slopes have a correct direction, we  can use full model directly - no need to take out any predictor

## full model based on scenario 1 - both beta1 and beta2 has a correct direction
## make a data list which only contains scenario1's data
s1_file <- beta1c_beta2c_SMD$case
## note to fit ess.sei where possible
## subset of sei
s1_sei_file <- SMD_filenames[SMD_filenames %in% s1_file] # this subset should use sei as a predictor and belong to scenario 1
## model fitting - keep beta1-related predictor (ess.sei) and beta2-related predictor (year_pub.l)
model_SMD_sei_s1 <- NA
for (i in 1:length(s1_sei_file)) {
  model_SMD_sei_s1[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ sei + year_pub.l, method = "REML", test = "t", data = SMD[s1_sei_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model coefficients and their significance test results for 'sei' in scenario 1
model_est_SMD_sei_s1 <- data.frame(case = s1_sei_file,
                             es_type = rep("SMD", length(s1_sei_file)),
                             beta0 = model_est_SMD[model_est_SMD$case %in% s1_sei_file, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_SMD[model_est_SMD$case %in% s1_sei_file, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_SMD[model_est_SMD$case %in% s1_sei_file, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_SMD_sei_s1, function(x) x$beta[1]), # beta0_c in equation 6 -  bias corrected overall mean
                             se_beta0_c = sapply(model_SMD_sei_s1, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_SMD_sei_s1, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_SMD_sei_s1, function(x) x$beta[2]), # beta1 in Equation 6 - slope of sampling error 
                             se_beta1 = sapply(model_SMD_sei_s1, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_SMD_sei_s1, function(x) x$pval[2]), # p value of beta1
                             beta2 = sapply(model_SMD_sei_s1, function(x) x$beta[3]), # beta2 in Equation 6 - slope of year 
                             se_beta2 = sapply(model_SMD_sei_s1, function(x) x$se[3]), # standard error of beta2
                             pval_beta2 = sapply(model_SMD_sei_s1, function(x) x$pval[3]) # p value of beta2
                            )

## subset of ess.sei
s1_ess.sei_file <- SMD_des_filenames[SMD_des_filenames %in% s1_file] # this subset should fit ess.sei as a predictor and belong to scenario 1
## model fitting - keep beta1-related predictor (ess.sei) and beta2-related predictor (year_pub.l)
model_SMD_ess.sei_s1 <- NA
for (i in 1:length(s1_ess.sei_file)) {
  model_SMD_ess.sei_s1[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ ess.sei + year_pub.l, method = "REML", test = "t", data = SMD[s1_ess.sei_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model model coefficients and their significance test results 'see.sei' for scenario 1 
model_est_SMD_ess.sei_s1 <- data.frame(case = s1_ess.sei_file,
                             es_type = rep("SMD", length(s1_ess.sei_file)),
                             beta0 = model_est_SMD[model_est_SMD$case %in% s1_ess.sei_file, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_SMD[model_est_SMD$case %in% s1_ess.sei_file, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_SMD[model_est_SMD$case %in% s1_ess.sei_file, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_SMD_ess.sei_s1, function(x) x$beta[1]), # beta0_c in equation 6 -  bias corrected overall mean
                             se_beta0_c = sapply(model_SMD_ess.sei_s1, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_SMD_ess.sei_s1, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_SMD_ess.sei_s1, function(x) x$beta[2]), # beta1 in Equation 6 - slope of sampling error 
                             se_beta1 = sapply(model_SMD_ess.sei_s1, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_SMD_ess.sei_s1, function(x) x$pval[2]), # p value of beta1
                             beta2 = sapply(model_SMD_ess.sei_s1, function(x) x$beta[3]), # beta2 in Equation 6 - slope of year 
                             se_beta2 = sapply(model_SMD_ess.sei_s1, function(x) x$se[3]), # standard error of beta2
                             pval_beta2 = sapply(model_SMD_ess.sei_s1, function(x) x$pval[3]) # p value of beta2
                            )


#*****************************scenario 2****************************#
## beta1 has a wrong direction, while beta2 has a correct direction
beta1w_beta2c_SMD <- model_est_SMD %>% subset(model_est_SMD$beta0Tbeta1 < 0 & model_est_SMD$beta0Tbeta2 < 0) 
beta1w_beta2c_SMD$case 

## reduced model based on scenario 2 - beta1 has a wrong direction, while beta2 has a correct direction
## make a data list which only contains scenario2's data
s2_file <-  beta1w_beta2c_SMD$case
## model fitting -  take out beta1-related predictor (sei or esss.sei) and keep beta2-related predictor (year_pub.l)
## beta1-related predictor is removed, so no need to fit ess.sei where possible
model_SMD_s2 <- NA
for (i in 1:length(s2_file)) {
  model_SMD_s2[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ year_pub.l, method = "REML", test = "t", data = SMD[s2_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model coefficients and their significance test results scenario 2
model_est_SMD_s2 <- data.frame(case = s2_file,
                             es_type = rep("SMD", length(s2_file)),
                             beta0 = model_est_SMD[model_est_SMD$case %in% s2_file, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_SMD[model_est_SMD$case %in% s2_file, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_SMD[model_est_SMD$case %in% s2_file, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_SMD_s2, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c = sapply(model_SMD_s2, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_SMD_s2, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = 0, # beta1 in Equation 5 - slope of sampling error: no error term
                             se_beta1 = 0, # standard error of beta1
                             pval_beta1 = 0, # p value of beta1
                             beta2 = sapply(model_SMD_s2, function(x) x$beta[2]), # beta2 in Equation 5 - slope of year 
                             se_beta2 = sapply(model_SMD_s2, function(x) x$se[2]), # standard error of beta2
                             pval_beta2 = sapply(model_SMD_s2, function(x) x$pval[2]) # p value of beta2
                            )


#*****************************scenario 3****************************#
## beta1 has a correct direction, while beta2 has a wrong direction
beta1c_beta2w_SMD <- model_est_SMD %>% subset(model_est_SMD$beta0Tbeta1 > 0 & model_est_SMD$beta0Tbeta2 > 0) 
beta1c_beta2w_SMD$case 

## reduced model based on scenario 3 - beta1 has a correct direction, while beta2 has a wrong direction
## make a data list which only contains scenario3's data
s3_file <-  beta1c_beta2w_SMD$case
## fit ess.sei where possible
## subset of sei
s3_sei_file <- SMD_filenames[SMD_filenames %in% s3_file] # this subset should fit sei as a predictor and belong to scenario 3
## model fitting - keep beta1-related predictor (sei) and take out beta2-related predictor (year_pub.l)
model_SMD_sei_s3 <- NA
for (i in 1:length(s3_sei_file)) {
  model_SMD_sei_s3[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ sei, method = "REML", test = "t", data = SMD[s3_sei_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## subset of ess.sei
s3_ess.sei_file <- SMD_des_filenames[SMD_des_filenames %in% s3_file] # this subset should fit ess.sei as a predictor and belong to scenario 3
## model fitting - keep beta1-related predictor (sei) and take out beta2-related predictor (year_pub.l)
model_SMD_ess.sei_s3 <- NA
for (i in 1:length(s3_ess.sei_file)) {
  model_SMD_ess.sei_s3[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), mods = ~ ess.sei, method = "REML", test = "t", data = SMD[s3_ess.sei_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model model coefficients and their significance test results for 'sei' in scenario 3
model_est_SMD_sei_s3 <- data.frame(case = s3_sei_file,
                             es_type = rep("SMD", length(s3_sei_file)),
                             beta0 = model_est_SMD[model_est_SMD$case %in% s3_sei_file, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_SMD[model_est_SMD$case %in% s3_sei_file, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_SMD[model_est_SMD$case %in% s3_sei_file, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_SMD_sei_s3, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c = sapply(model_SMD_sei_s3, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_SMD_sei_s3, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_SMD_sei_s3, function(x) x$beta[2]), # beta1 in Equation 5 - slope of sampling error 
                             se_beta1 = sapply(model_SMD_sei_s3, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_SMD_sei_s3, function(x) x$pval[2]), # p value of beta1
                             beta2 = 0, # beta2 in Equation 5 - slope of year: no year term 
                             se_beta2 = 0, # standard error of beta2
                             pval_beta2 = 0 # p value of beta2
                            )

## extract model model coefficients and their significance test results for 'ess.sei' in scenario 3
model_est_SMD_ess.sei_s3 <- data.frame(case = s3_ess.sei_file,
                             es_type = rep("SMD", length(s3_ess.sei_file)),
                             beta0 = model_est_SMD[model_est_SMD$case %in% s3_ess.sei_file, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_SMD[model_est_SMD$case %in% s3_ess.sei_file, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_SMD[model_est_SMD$case %in% s3_ess.sei_file, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_SMD_ess.sei_s3, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c = sapply(model_SMD_ess.sei_s3, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_SMD_ess.sei_s3, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_SMD_ess.sei_s3, function(x) x$beta[2]), # beta1 in Equation 5 - slope of sampling error 
                             se_beta1 = sapply(model_SMD_ess.sei_s3, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_SMD_ess.sei_s3, function(x) x$pval[2]), # p value of beta1
                             beta2 = 0, # beta2 in Equation 5 - slope of year: no year term 
                             se_beta2 = 0, # standard error of beta2
                             pval_beta2 = 0 # p value of beta2
                            )


#*****************************scenario 4****************************#
## both beta1 and beta2 has a wrong direction
beta1w_beta2w_SMD <- model_est_SMD %>% subset(model_est_SMD$beta0Tbeta1 < 0 & model_est_SMD$beta0Tbeta2 > 0) 
beta1w_beta2w_SMD$case 

## reduced model based on scenario 4 - beta1 has a wrong direction and beta2 has a wrong direction 
## this reduced model needs to take out both of the two predictors (sei and pub_year.l). This is equivalent to a null model (intercept-only model), which is used to estimate (uncorrected) meta-analytic overall mean

## make a data list which only contains scenario4's data
s4_file <-  beta1w_beta2w_SMD$case
## no need to subset sei and ess.sei because scenario4 fits a null model (without any predictor)
## model fitting - take out both beta1-related predictor (sei or ess.sei) and  beta2-related predictor (year_pub.l)
model_SMD_s4 <- NA
for (i in 1:length(s4_file)) {
  model_SMD_s4[i] <- rma.mv(yi = es, V = var, random = list(~1|study_ID/obs_ID), method = "REML", test = "t", data = SMD[s4_file][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model model coefficients and their significance test results scenario 4
model_est_SMD_s4 <- data.frame(case = s4_file,
                             es_type = rep("SMD", length(s4_file)),
                             beta0 = model_est_SMD[model_est_SMD$case %in% s4_file, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean # or sapply(model_SMD_s4, function(x) x$beta[1])
                             se_beta0 = model_est_SMD[model_est_SMD$case %in% s4_file, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_SMD[model_est_SMD$case %in% s4_file, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_SMD_s4, function(x) x$beta[1]), # beta0_c in equation 1 -  bias corrected overall mean: this is equal to uncorrected overall mean (i.e., beta0)
                             se_beta0_c = sapply(model_SMD_s4, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_SMD_s4, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = 0, # beta1 in Equation 1 - slope of sampling error: no error term
                             se_beta1 = 0, # standard error of beta1
                             pval_beta1 = 0, # p value of beta1
                             beta2 = 0, # beta2 in Equation 1 - slope of year 
                             se_beta2 = 0, # standard error of beta2
                             pval_beta2 = 0 # p value of beta2
                            )
```


# 2.2 scaled data

## (i) estimates of beta0
fit intercept-only multilevel model to each dataset
```{r}

#*************************************************************************#
#                        meta-analytic overall mean                    
#*************************************************************************#


## SMD
model_SMD_scaled <- NA
for (i in 1:length(SMD)) {
  model_SMD_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), method = "REML", test = "t", data = SMD[[i]], sparse = TRUE, control = list(optimizer = "optim")) %>% list()
}

```

## (ii) detect publication bias

we aim for detecting two forms of publication bias: small-study effect  and decline effect.

we use a full model with sampling error (sei) and publication year (year_pub.l) as moderators to detect publication bias.

of relevance, sei's slope (beta1) and year_pub.l's slope (beta2) can be used to indicate the occurrence of small-study effect and decline effect, respectively.

note that for SMD we need to model 'effective sample size' based sampling error analogue (ees.sei), where possible

### (a) fit full models with samping error and year as predictors

note that in the subsequent models, we keep still year as its original scale.

this is to ease interpretation, where the beta2 can be interpreted as 'one year increase leads to xx unit decrease of SMD'
```{r}

#*************************************************************************#
#       Full model with error and latest year as predictors
#*************************************************************************#

## use sampling error (sei) and latest year (year_pub.l) as predictors for those with calculated effect sizes and sampling variance

model_SMD_sei.year_scaled <- NA
for (i in 1:length(SMD[SMD_filenames])) {
  model_SMD_sei.year_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ sei_zscore + year_pub.l, method = "REML", test = "t", data = SMD[SMD_filenames][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
} # note that here only fit the subset of SMD[SMD_filenames] - SMD with calculated effect sizes and sampling variance

## use effective-sample-size based sampling error (ess.sei) where possible (SMD with descriptive statistics)

model_SMD_ess.sei.year_scaled <- NA
for (i in 1:length(SMD[SMD_des_filenames])) {
  model_SMD_ess.sei.year_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ ess.sei_zscore + year_pub.l, method = "REML", test = "t", data = SMD[SMD_des_filenames][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
} 
```



### (b) identify the presence of publication bias

we next aim for identify the presence of the small-study effect and decline effect for each meta-analysis. 

our rational is: for an effect that is expected to be positive, a small study effect and decline effect would be expressed in a positive value of beta1 and negative value of beta2, respectively. In this respect, a slope (beta1 or beta2)) with opposing direction (unexpected sign) indicates no detectable publication bias and subsequently does not require correction for such a bias.

we use the product of beta0 and beta1 (i.e., beta×beta1) as the signal, that is, if beta0×beta1 is positive, it indicates the examined meta-analysis has a small-study effect (beta1 is in a correct direction).

similarly, when the product of beta0×beta2 is negative, the examined meta-analysis has a decline effect (beta 2 is in a correct direction)
```{r}
#*************************************************************************#
#             Identify the presence of publication bias 
#*************************************************************************#

model_SMD_pb_scaled <- append(model_SMD_sei.year_scaled, model_SMD_ess.sei.year_scaled)

### extract model model coefficients and their significance test results
model_est_SMD_scaled <- data.frame(case = names(SMD),
                             es_type = rep("SMD", length(SMD)),
                             beta0 = sapply(model_SMD_scaled, function(x) x$beta), # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = sapply(model_SMD_scaled, function(x) x$se), # standard error of beta0
                             pval_beta0 = sapply(model_SMD_scaled, function(x) x$pval), # p value of beta0
                             beta0_c = sapply(model_SMD_pb_scaled, function(x) x$beta[1]), # beta0_c in equation 2 -  bias corrected overall mean
                             se_beta0_c = sapply(model_SMD_pb_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_SMD_pb_scaled, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_SMD_pb_scaled, function(x) x$beta[2]), # beta1 in Equation 2 - slope of sampling error 
                             se_beta1 = sapply(model_SMD_pb_scaled, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_SMD_pb_scaled, function(x) x$pval[2]), # p value of beta1
                             beta2 = sapply(model_SMD_pb_scaled, function(x) x$beta[3]), # beta2 in Equation 2 - slope of year 
                             se_beta2 = sapply(model_SMD_pb_scaled, function(x) x$se[3]), # standard error of beta2
                             pval_beta2 = sapply(model_SMD_pb_scaled, function(x) x$pval[3]) # p value of beta2
                            )

## we first to create two new columns to contain the two products:  beta0*beta1 and beta0*beta2

model_est_SMD_scaled[15:16] <- data.frame(beta0Tbeta1 = model_est_SMD_scaled$beta0 * model_est_SMD_scaled$beta1, beta0Tbeta2 = model_est_SMD_scaled$beta0 * model_est_SMD_scaled$beta2) # model_est_SMD has 14 column (ncol(model_est_SMD)), so we add columns 15 and 16

## visual check
model_est_SMD_scaled

## identify the small-study effect - significant beta1 with correct sign
sse_SMD_scaled <- model_est_SMD_scaled %>% subset(model_est_SMD_scaled$pval_beta1 < 0.05 & model_est_SMD_scaled$beta0Tbeta1 > 0)
## check which meta-analyses have small-study effects
sse_SMD_scaled$case 



## identify the decline effect - significant beta2 with correct sign 
de_SMD_scaled <- model_est_SMD_scaled %>% subset(model_est_SMD_scaled$pval_beta2 < 0.05 & model_est_SMD_scaled$beta0Tbeta2 < 0)
## check which meta-analyses have decline effects
de_SMD_scaled$case 

## identify the concurrence of the small-study effect and decline effect
sse_de_SMD_scaled <- model_est_SMD_scaled %>% subset(model_est_SMD_scaled$pval_beta1 < 0.05 & model_est_SMD_scaled$beta0Tbeta1 > 0 & model_est_SMD_scaled$pval_beta2 < 0.05 & model_est_SMD_scaled$beta0Tbeta2 < 0) 
sse_de_SMD_scaled$case 



#*************************************************************************#
#        Multilevel models to estimate bias-corrected effect
#*************************************************************************#

#*****************************scenario 1****************************#
## both beta1 and beta2 has a correct direction
beta1c_beta2c_SMD_scaled <- model_est_SMD_scaled %>% subset(model_est_SMD_scaled$beta0Tbeta1 > 0 & model_est_SMD_scaled$beta0Tbeta2 < 0) 
beta1c_beta2c_SMD_scaled$case  

## if a model slope (beta1 and beta2) has a wrong direction, we need to take out it when fitting model to estimate the bias-corrected mean
## in scenario 1, both of the two slopes have a correct direction, we use can use full model directly, no need to use take out any predictor

## full model based on scenario 1 - both beta1 and beta2 has a correct direction
## make a data list which only contains scenario1's data
s1_file_scaled <- beta1c_beta2c_SMD_scaled$case
## fit ess.sei where possible
## subset of sei
s1_sei_file_scaled <- SMD_filenames[SMD_filenames %in% s1_file_scaled] # this subset should use sei as a predictor and belong to scenario 1
## model fitting - fit a full model
model_SMD_sei_s1_scaled <- NA
for (i in 1:length(s1_sei_file_scaled)) {
  model_SMD_sei_s1_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ sei_zscore + year_pub.l, method = "REML", test = "t", data = SMD[s1_sei_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model model coefficients and their significance test results for 'sei' in scenario 1
model_est_SMD_sei_s1_scaled <- data.frame(case = s1_sei_file_scaled,
                             es_type = rep("SMD", length(s1_sei_file_scaled)),
                             beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s1_sei_file_scaled, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s1_sei_file_scaled, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s1_sei_file_scaled, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_SMD_sei_s1_scaled, function(x) x$beta[1]), # beta0_c in equation 6 -  bias corrected overall mean
                             se_beta0_c = sapply(model_SMD_sei_s1_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_SMD_sei_s1_scaled, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_SMD_sei_s1_scaled, function(x) x$beta[2]), # beta1 in Equation 6 - slope of sampling error 
                             se_beta1 = sapply(model_SMD_sei_s1_scaled, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_SMD_sei_s1_scaled, function(x) x$pval[2]), # p value of beta1
                             beta2 = sapply(model_SMD_sei_s1_scaled, function(x) x$beta[3]), # beta2 in Equation 6 - slope of year 
                             se_beta2 = sapply(model_SMD_sei_s1_scaled, function(x) x$se[3]), # standard error of beta2
                             pval_beta2 = sapply(model_SMD_sei_s1_scaled, function(x) x$pval[3]) # p value of beta2
                            )



## subset of ess.sei
s1_ess.sei_file_scaled <- SMD_des_filenames[SMD_des_filenames %in% s1_file_scaled] # this subset should fit ess.sei as a predictor and belong to scenario 1
## model fitting - full model, which does not need to take out any predictor
model_SMD_ess.sei_s1_scaled <- NA
for (i in 1:length(s1_ess.sei_file_scaled)) {
  model_SMD_ess.sei_s1_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ ess.sei_zscore + year_pub.l, method = "REML", test = "t", data = SMD[s1_ess.sei_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model model coefficients and their significance test results 'see.sei' for scenario 2 
model_est_SMD_ess.sei_s1_scaled <- data.frame(case = s1_ess.sei_file_scaled,
                             es_type = rep("SMD", length(s1_ess.sei_file_scaled)),
                             beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s1_ess.sei_file_scaled, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s1_ess.sei_file_scaled, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s1_ess.sei_file_scaled, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_SMD_ess.sei_s1_scaled, function(x) x$beta[1]), # beta0_c in equation 6 -  bias corrected overall mean
                             se_beta0_c = sapply(model_SMD_ess.sei_s1_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_SMD_ess.sei_s1_scaled, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_SMD_ess.sei_s1_scaled, function(x) x$beta[2]), # beta1 in Equation 6 - slope of sampling error 
                             se_beta1 = sapply(model_SMD_ess.sei_s1_scaled, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_SMD_ess.sei_s1_scaled, function(x) x$pval[2]), # p value of beta1
                             beta2 = sapply(model_SMD_ess.sei_s1_scaled, function(x) x$beta[3]), # beta2 in Equation 6 - slope of year 
                             se_beta2 = sapply(model_SMD_ess.sei_s1_scaled, function(x) x$se[3]), # standard error of beta2
                             pval_beta2 = sapply(model_SMD_ess.sei_s1_scaled, function(x) x$pval[3]) # p value of beta2
                            )


#*****************************scenario 2****************************#
## beta1 has a wrong direction, while beta2 has a correct direction
beta1w_beta2c_SMD_scaled <- model_est_SMD_scaled %>% subset(model_est_SMD_scaled$beta0Tbeta1 < 0 & model_est_SMD_scaled$beta0Tbeta2 < 0) 
beta1w_beta2c_SMD_scaled$case 

## reduced model based on scenario 2 - beta1 has a wrong direction, while beta2 has a correct direction
## make a data list which only contains scenario2's data
s2_file_scaled <-  beta1w_beta2c_SMD_scaled$case
## model fitting -  take out beta1-related predictor (sei or esss.sei) and keep beta2-related predictor (year_pub.l)
## beta1-related predictor is removed, so no need to fit ess.sei where possible
model_SMD_s2_scaled <- NA
for (i in 1:length(s2_file_scaled)) {
  model_SMD_s2_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ year_pub.l, method = "REML", test = "t", data = SMD[s2_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model model coefficients and their significance test results scenario 2
model_est_SMD_s2_scaled <- data.frame(case = s2_file_scaled,
                             es_type = rep("SMD", length(s2_file_scaled)),
                             beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s2_file_scaled, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s2_file_scaled, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s2_file_scaled, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_SMD_s2_scaled, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c = sapply(model_SMD_s2_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_SMD_s2_scaled, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = 0, # beta1 in Equation 5 - slope of sampling error: no error term
                             se_beta1 = 0, # standard error of beta1
                             pval_beta1 = 0, # p value of beta1
                             beta2 = sapply(model_SMD_s2_scaled, function(x) x$beta[2]), # beta2 in Equation 5 - slope of year 
                             se_beta2 = sapply(model_SMD_s2_scaled, function(x) x$se[2]), # standard error of beta2
                             pval_beta2 = sapply(model_SMD_s2_scaled, function(x) x$pval[2]) # p value of beta2
                            )


#*****************************scenario 3****************************#
## beta1 has a correct direction, while beta2 has a wrong direction
beta1c_beta2w_SMD_scaled <- model_est_SMD_scaled %>% subset(model_est_SMD_scaled$beta0Tbeta1 > 0 & model_est_SMD_scaled$beta0Tbeta2 > 0) 
beta1c_beta2w_SMD_scaled$case 

## reduced model based on scenario 3 - beta1 has a correct direction, while beta2 has a wrong direction
## make a data list which only contains scenario3's data
s3_file_scaled <-  beta1c_beta2w_SMD_scaled$case
## fit ess.sei where possible
## subset of sei
s3_sei_file_scaled <- SMD_filenames[SMD_filenames %in% s3_file_scaled] # this subset should fit sei as a predictor and belong to scenario 3
## model fitting - keep beta1-related predictor (sei) and take out beta2-related predictor (year_pub.l)
model_SMD_sei_s3_scaled <- NA
for (i in 1:length(s3_sei_file_scaled)) {
  model_SMD_sei_s3_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ sei_zscore, method = "REML", test = "t", data = SMD[s3_sei_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## subset of ess.sei
s3_ess.sei_file_scaled <- SMD_des_filenames[SMD_des_filenames %in% s3_file_scaled] # this subset should fit ess.sei as a predictor and belong to scenario 3
## model fitting - keep beta1-related predictor (sei) and take out beta2-related predictor (year_pub.l)
model_SMD_ess.sei_s3_scaled <- NA
for (i in 1:length(s3_ess.sei_file_scaled)) {
  model_SMD_ess.sei_s3_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), mods = ~ ess.sei_zscore, method = "REML", test = "t", data = SMD[s3_ess.sei_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model model coefficients and their significance test results for 'sei' in scenario 3
model_est_SMD_sei_s3_scaled <- data.frame(case = s3_sei_file_scaled,
                             es_type = rep("SMD", length(s3_sei_file_scaled)),
                             beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s3_sei_file_scaled, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s3_sei_file_scaled, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s3_sei_file_scaled, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_SMD_sei_s3_scaled, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c = sapply(model_SMD_sei_s3_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_SMD_sei_s3_scaled, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_SMD_sei_s3_scaled, function(x) x$beta[2]), # beta1 in Equation 5 - slope of sampling error 
                             se_beta1 = sapply(model_SMD_sei_s3_scaled, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_SMD_sei_s3_scaled, function(x) x$pval[2]), # p value of beta1
                             beta2 = 0, # beta2 in Equation 5 - slope of year: no year term 
                             se_beta2 = 0, # standard error of beta2
                             pval_beta2 = 0 # p value of beta2
                            )

## extract model model coefficients and their significance test results for 'ess.sei' in scenario 3
model_est_SMD_ess.sei_s3_scaled <- data.frame(case = s3_ess.sei_file_scaled,
                             es_type = rep("SMD", length(s3_ess.sei_file_scaled)),
                             beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s3_ess.sei_file_scaled, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean
                             se_beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s3_ess.sei_file_scaled, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s3_ess.sei_file_scaled, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_SMD_ess.sei_s3_scaled, function(x) x$beta[1]), # beta0_c in equation 5 -  bias corrected overall mean
                             se_beta0_c = sapply(model_SMD_ess.sei_s3_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_SMD_ess.sei_s3_scaled, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = sapply(model_SMD_ess.sei_s3_scaled, function(x) x$beta[2]), # beta1 in Equation 5 - slope of sampling error 
                             se_beta1 = sapply(model_SMD_ess.sei_s3_scaled, function(x) x$se[2]), # standard error of beta1
                             pval_beta1 = sapply(model_SMD_ess.sei_s3_scaled, function(x) x$pval[2]), # p value of beta1
                             beta2 = 0, # beta2 in Equation 5 - slope of year: no year term 
                             se_beta2 = 0, # standard error of beta2
                             pval_beta2 = 0 # p value of beta2
                            )


#*****************************scenario 4****************************#
## both beta1 and beta2 has a wrong direction
beta1w_beta2w_SMD_scaled <- model_est_SMD_scaled %>% subset(model_est_SMD_scaled$beta0Tbeta1 < 0 & model_est_SMD_scaled$beta0Tbeta2 > 0) 
beta1w_beta2w_SMD_scaled$case   

## reduced model based on scenario 4 - beta1 has a wrong direction and beta2 has a wrong direction 
## this reduced model needs to take out both of the two predictors (sei and pub_year.l). This is equivalent to a null model (intercept-only model), which is used to estimate (uncorrected) meta-analytic overall mean

## make a data list which only contains scenario4's data
s4_file_scaled <-  beta1w_beta2w_SMD_scaled$case
## no need to subset sei and ess.sei because scenario4 fits a null model (without any predictor)
## model fitting - take out both beta1-related predictor (sei or ess.sei) and  beta2-related predictor (year_pub.l)
model_SMD_s4_scaled <- NA
for (i in 1:length(s4_file_scaled)) {
  model_SMD_s4_scaled[i] <- rma.mv(yi = es_zscore, V = var, random = list(~1|study_ID/obs_ID), method = "REML", test = "t", data = SMD[s4_file_scaled][[i]], sparse = TRUE, control=list(optimizer = "optim")) %>% list()
}

## extract model model coefficients and their significance test results scenario 4
model_est_SMD_s4_scaled <- data.frame(case = s4_file_scaled,
                             es_type = rep("SMD", length(s4_file_scaled)),
                             beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s4_file_scaled, ]$beta0, # beta0 in Equation 1 - meta-analytic overall mean # or sapply(model_SMD_s4_scaled, function(x) x$beta[1])
                             se_beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s4_file_scaled, ]$se_beta0, # standard error of beta0
                             pval_beta0 = model_est_SMD_scaled[model_est_SMD_scaled$case %in% s4_file_scaled, ]$pval_beta0, # p value of beta0
                             beta0_c = sapply(model_SMD_s4_scaled, function(x) x$beta[1]), # beta0_c in equation 1 -  bias corrected overall mean: this is equal to uncorrected overall mean (i.e., beta0)
                             se_beta0_c = sapply(model_SMD_s4_scaled, function(x) x$se[1]), # standard error of beta0_c
                             pval_beta0_c = sapply(model_SMD_s4_scaled, function(x) x$pval[1]), # p valuer of beta0_c
                             beta1 = 0, # beta1 in Equation 1 - slope of sampling error: no error term
                             se_beta1 = 0, # standard error of beta1
                             pval_beta1 = 0, # p value of beta1
                             beta2 = 0, # beta2 in Equation 1 - slope of year 
                             se_beta2 = 0, # standard error of beta2
                             pval_beta2 = 0 # p value of beta2
                            )

```



# 3. meta-meta-analysis

we use random-effect meta-analytic models to aggregate three coefficients obtained from the above meta-analytic models:

3.1 D - the mean difference between overall mean (beta0) and bias-corrected overall mean (beta0_c); see formula in the main text

3.2 beta1 - the slope of uncertainty (sampling error or effective sample size analogue)

3.3 beta2 - the slope of year

note that for a given meta-analysis, if beta0 is negative, we need to flip beta1 and beta2 prior to model fitting

## 3.1 D
quantify the overall decline in the magniude of effect size after adjusting for publication bias

```{r}
# differences in beta0
model_est_SMD_scaled$D <- abs(model_est_SMD_scaled$beta0 - model_est_SMD_scaled$beta0_c)
# differences in beta0' variance
model_est_SMD_scaled$D_var <- model_est_SMD_scaled$se_beta0^2 + model_est_SMD_scaled$se_beta0_c^2 + 2*model_est_SMD_scaled$se_beta0*model_est_SMD_scaled$se_beta0_c
# differences in beta0' SE
model_est_SMD_scaled$D_sei <- sqrt(model_est_SMD_scaled$D_var)

# get folded mean and variance
model_est_SMD_scaled$D_folded <- folded_es(mean = model_est_SMD_scaled$D, variance = model_est_SMD_scaled$D_var)
model_est_SMD_scaled$D_var_folded <- folded_error(mean = model_est_SMD_scaled$D, variance = model_est_SMD_scaled$D_var)
model_est_SMD_scaled$D_sei_folded <- sqrt(model_est_SMD_scaled$D_var_folded)

# overall decline in effect size magnitude
MMA_D_SMD <- rma(yi = D_folded, sei = D_sei_folded, method = "REML", data = model_est_SMD_scaled)
```


## 3.2 beta1
quantify the systematic pattern of small-study effect in the filed of Eco&Evo

```{r}
# overall evidence of small-study effect
# for the dataset with negative beta0, we need to flip beta1
# get the subset of negative beta0
beta1_flip_SMD <- (model_est_SMD_scaled %>% subset(model_est_SMD_scaled$beta0 < 0))$case

# first use beta as flipped beta1
model_est_SMD_scaled$beta1_flip <- model_est_SMD_scaled$beta1
# then flip negative beta0's beta1
model_est_SMD_scaled[model_est_SMD_scaled$case %in% beta1_flip_SMD, ]$beta1_flip <- model_est_SMD_scaled[model_est_SMD_scaled$case %in% beta1_flip_SMD, ]$beta1*(-1)

# meta-meta-analysis
MMA_beta1_SMD <- rma(yi = beta1_flip, sei = se_beta1, method = "REML", data = model_est_SMD_scaled)
```

## 3.3 beta2
quantify the systematic pattern of decline effect in the filed of Eco&Evo

```{r}
# for the dataset with negative beta0, we need to flip beta1
# get the subset of negative beta0
beta2_flip_SMD <- (model_est_SMD_scaled %>% subset(model_est_SMD_scaled$beta0 < 0))$case
# first use beta as flipped beta2
model_est_SMD_scaled$beta2_flip <- model_est_SMD_scaled$beta2
# then flip negative beta0's beta2
model_est_SMD_scaled[model_est_SMD_scaled$case %in% beta2_flip_SMD, ]$beta2_flip <- model_est_SMD_scaled[model_est_SMD_scaled$case %in% beta2_flip_SMD, ]$beta2*(-1)

# meta-meta-analysis
MMA_beta2_SMD <- rma(yi = beta2_flip, sei = se_beta2, method = "REML", data = model_est_SMD_scaled)
```



## 3.4 interpretation of D

```{r}

# sd for each dataset
sd_SMD <- NA
for (i in 1:length(SMD)) {
  sd_SMD[i] <- sd(SMD[[i]]$es)
}
# average sd
mean(sd_SMD) # 2.06368
# exclude large sd
mean(sd_SMD[sd_SMD<2]) # 1.147149
# convert standardised beta0 into original scale
MMA_D_SMD$beta[1]*mean(sd_SMD) # 0.5721656
MMA_D_SMD$beta[1]*mean(sd_SMD[sd_SMD<2]) # 0.3180529
```



# 4. power, Type M/S
we estimate the power, Type M/S at two levels: 
4.1 meta-analysis level 
4.2 experimental level (effect size level)


## 4.1 meta-analysis level
(a) compute power, Type M/S for each dataset

(b) aggregate these three parameters across meta-analyses

### (a) computation
```{r}
#****************************************************************#
#-------------------------------SMD-----------------------------#
#****************************************************************#

#-----------------------------------------------------------#
#          (1) two-tailed power for meta-analyses
#-----------------------------------------------------------#
model_est_SMD$MA.power <- power.ma_Shinichi(mu=model_est_SMD$beta0,SE=model_est_SMD$se_beta0)

#-----------------------------------------------------------#
#            (2) type S error for meta-analyses
#-----------------------------------------------------------#
MA.power.S <- NA
for (i in 1:length(model_est_SMD$case)) {
  MA.power.S[i] <- error_S(mu=model_est_SMD$beta0[i],se=model_est_SMD$se_beta0[i],alpha=0.05) %>% unlist()
}

model_est_SMD$MA.power.S <- MA.power.S

#-----------------------------------------------------------#
#   (3) type M error (overestimate ratio) for meta-analyses
#-----------------------------------------------------------#
MA.power.M <- NA
for (i in 1:length(model_est_SMD$case)) {
  MA.power.M[i] <- error_M(mu=model_est_SMD$beta0[i],se=model_est_SMD$se_beta0[i],alpha=0.05,N=10000) %>% unlist()
}

model_est_SMD$MA.power.M <- MA.power.M
```

### (b) aggregation
use weighted regression to aggregate meta-analysis level power

```{r}
#***************************************************************#
#      estimate overall power for meta-analysis level power     #
#***************************************************************#

# add N and k
N_SMD <- NA
for (i in 1:length(SMD)) {
  N_SMD[i] <- SMD[[i]]$study_ID %>% unique() %>% length()
}
model_est_SMD$N <- N_SMD

k_SMD <- NA
for (i in 1:length(SMD)) {
  k_SMD[i] <- SMD[[i]]$obs_ID %>% length()
}
model_est_SMD$k <- k_SMD

#--------------------- (1) two tailed power ---------------------#
# log transform
MMA_MA.power_SMD <- lm(log(MA.power) ~ 1, weights = k, data = model_est_SMD)
# original scale
MMA_MA.power_SMD2 <- lm(MA.power ~ 1, weights = k, data = model_est_SMD)
MMA_MA.power_SMD2$coefficients

# median
MMA_MA.power_SMD$coefficients  %>% exp() 
# mean
(MMA_MA.power_SMD$coefficients + 0.5*var(log(model_est_SMD$MA.power))) %>% exp() 
# confidence interval of median
confint(MMA_MA.power_SMD) %>% exp()

# compare residuals
par(mfrow = c(1, 2))
residuals(MMA_MA.power_SMD) %>% hist(main = paste("log power"), xlab = "Residual")
residuals(MMA_MA.power_SMD2) %>% hist(main = paste("original power"), xlab = "Residual") 

#---------------------- (2) type S error -----------------------#
# log
MMA_MA.power.S_SMD <- lm(log(MA.power.S+0.025) ~ 1, weights = k, data = model_est_SMD) # add an offset of 0.025(25%) to avoid ln(0) = infinity 
# median
MMA_MA.power.S_SMD$coefficients %>% exp() - 0.025
# mean
(MMA_MA.power.S_SMD$coefficients + 0.5*var(log(model_est_SMD$MA.power.S + 0.025))) %>% exp() - 0.025
# confidence interval of median
confint(MMA_MA.power.S_SMD) %>% exp() - 0.025 # if the lower boundary was negative (probably caused by the negative variance), we used 0 to replace it

#-------------- (3) type M error (overestimate ratio) -------------#
# log
MMA_MA.power.M_SMD <- lm(log(MA.power.M) ~ 1, weights = k, data = model_est_SMD)
# median
MMA_MA.power.M_SMD$coefficients %>% exp() 
# mean
(MMA_MA.power.M_SMD$coefficients + 0.5*var(log(model_est_SMD$MA.power.M))) %>% exp() 

# confidence interval of median
confint(MMA_MA.power.M_SMD) %>% exp()
```




##  4.2 experiment level
(a) compute power, Type M/S for each dataset

(b) aggregate these three parameters across meta-analyses

### (a) computation
```{r}
#***************************************************************#
#        power for single experiments within meta-analysis       #
#***************************************************************#


#****************************************************************#
#------------------------------SMD-----------------------------#
#****************************************************************#

#--------------------- (1) two tailed power ---------------------#

# SMD
power_SMD <- NA
for (i in 1:length(SMD)) {
  power_SMD[i] <- power.individual_Shinichi(mu=rep(model_SMD[[i]]$beta, length(SMD[[i]]$sei)), se=SMD[[i]]$sei) %>% list()}

# allocate each set of power into corresponding dataset
for (i in 1:length(power_SMD)) {
  SMD[[i]]$power_SMD <- power_SMD[[i]]
}


#---------------------- (2) type S error -----------------------#
power.S_SMD <- NA
for (i in 1:length(SMD)) {
  power.S_SMD[i] <- mapply(error_S,mu=rep(model_SMD[[i]]$beta, length(SMD[[i]]$sei)), se=SMD[[i]]$sei) %>% list()}

# allocate each set of type S error into corresponding dataset
for (i in 1:length(power.S_SMD)) {
  SMD[[i]]$power.S_SMD <- power.S_SMD[[i]]
}


#---------------- (3) type M error (overestimate ratio) --------------#
power.M_SMD <- NA
for (i in 1:length(SMD)) {
  power.M_SMD[i] <- mapply(error_M,mu=rep(model_SMD[[i]]$beta, length(SMD[[i]]$sei)), se=SMD[[i]]$sei) %>% list()}

# allocate each set of type S error into corresponding dataset
for (i in 1:length(power.M_SMD)) {
  SMD[[i]]$power.M_SMD <- power.M_SMD[[i]]
}

#*********************************************************************#
#------------------- summary of experimental power --------------------#
#*********************************************************************#

#****************************************************************#
#-----------------------------SMD------------------------------#
#****************************************************************#

#--------------------- (1) two tailed power ---------------------#
power_summary_SMD <- data.frame(case=names(SMD),
                   Minimum=mapply(summary, sapply(SMD, function(x) x$power_SMD))[1,],      
                   `First quarter`=mapply(summary, sapply(SMD, function(x) x$power_SMD))[2,],
                   Median=mapply(summary, sapply(SMD, function(x) x$power_SMD))[3,],
                   Mean=mapply(summary, sapply(SMD, function(x) x$power_SMD))[4,], 
                   `Third quarter`=mapply(summary, sapply(SMD, function(x) x$power_SMD))[5,], 
                   Maximum=mapply(summary, sapply(SMD, function(x) x$power_SMD))[6,]) 


#---------------------- (2) type S error -----------------------#
power.S_summary_SMD <-  data.frame(case=names(SMD),
                   Minimum=mapply(summary, sapply(SMD, function(x) x$power.S_SMD))[1,],      
                   `First quarter`=mapply(summary, sapply(SMD, function(x) x$power.S_SMD))[2,],
                   Median=mapply(summary, sapply(SMD, function(x) x$power.S_SMD))[3,],
                   Mean=mapply(summary, sapply(SMD, function(x) x$power.S_SMD))[4,], 
                   `Third quarter`=mapply(summary, sapply(SMD, function(x) x$power.S_SMD))[5,], 
                   Maximum=mapply(summary, sapply(SMD, function(x) x$power.S_SMD))[6,]) 



#-------------- (3) type M error (overestimate ratio) -------------#
power.M_summary_SMD <-  data.frame(case=names(SMD),
                   Minimum=mapply(summary, sapply(SMD, function(x) x$power.M_SMD))[1,],      
                   `First quarter`=mapply(summary, sapply(SMD, function(x) x$power.M_SMD))[2,],
                   Median=mapply(summary, sapply(SMD, function(x) x$power.M_SMD))[3,],
                   Mean=mapply(summary, sapply(SMD, function(x) x$power.M_SMD))[4,], 
                   `Third quarter`=mapply(summary, sapply(SMD, function(x) x$power.M_SMD))[5,], 
                   Maximum=mapply(summary, sapply(SMD, function(x) x$power.M_SMD))[6,]) 


#*********************************************************************#
#--------- calculate standard error for each type of power -----------#
#*********************************************************************#

#****************************************************************#
#-----------------------------SMD------------------------------#
#****************************************************************#

#--------------------- (1) two tailed power ---------------------#
# standard error of variable power_SMD
se_power <- NA
for (i in 1:length(SMD)) {
  se_power[i] <- sd(SMD[[i]]$power_SMD, na.rm=TRUE) / sqrt(length(SMD[[i]]$power_SMD[!is.na(SMD[[i]]$power_SMD)]))
}

power_summary_SMD$se_power <- se_power


#---------------------- (2) type S error -----------------------#
# standard error of variable power.S_SMD
se_power.S <- NA
for (i in 1:length(SMD)) {
  se_power.S[i] <- sd(SMD[[i]]$power.S_SMD, na.rm=TRUE) / sqrt(length(SMD[[i]]$power.S_SMD[!is.na(SMD[[i]]$power.S_SMD)]))
}

power.S_summary_SMD$se_power.S <- se_power.S


#-------------- (3) type M error (overestimate ratio) -------------#
## standard error of variable power.M_SMD
se_power.M <- NA
for (i in 1:length(SMD)) {
  se_power.M[i] <- sd(SMD[[i]]$power.M_SMD, na.rm=TRUE) / sqrt(length(SMD[[i]]$power.M_SMD[!is.na(SMD[[i]]$power.M_SMD)]))
}

power.M_summary_SMD$se_power.M <- se_power.M

```



### (b) aggregation
use linear mixed model to aggregate experimental level power

```{r}
#***************************************************************#
#      estimate overall power for experimental level power     #
#***************************************************************#

#****************************************************************#
#-----------------------------SMD------------------------------#
#****************************************************************#

# construct a dataframe to contain necessay elements 
study_ID_SMD <- sapply(SMD, function(x) x$study_ID) %>% unlist()
power_SMD <- sapply(SMD, function(x) x$power_SMD) %>% unlist()
power.S_SMD <- sapply(SMD, function(x) x$power.S_SMD) %>% unlist()
power.M_SMD <- sapply(SMD, function(x) x$power.M_SMD) %>% unlist()

individual_est_SMD <- data.frame("study_ID_SMD" = study_ID_SMD,
                                 "power_SMD" = power_SMD,
                                 "power.S_SMD" = power.S_SMD,
                                 "power.M_SMD" = power.M_SMD
                                  )

#--------------------- (1) two tailed power ---------------------#
# log
MMA_EXP.power_SMD <- lmer(log(power_SMD) ~ 1 + (1 | study_ID_SMD), data = individual_est_SMD)
# original scale
MMA_EXP.power_SMD2 <- lmer(power_SMD ~ 1 + (1 | study_ID_SMD), data = individual_est_SMD)
summary(MMA_EXP.power_SMD2)$coefficients[1]

# median 
summary(MMA_EXP.power_SMD)$coefficients[1] %>% exp()
# mean
(summary(MMA_EXP.power_SMD)$coefficients[1] + 0.5*var(log(individual_est_SMD$power_SMD))) %>% exp()
# confidence interval of median
confint(MMA_EXP.power_SMD) %>% exp()

# compare residual
par(mfrow = c(1, 2))
residuals(MMA_EXP.power_SMD) %>% hist(main = paste("log power"), xlab = "Residual")
residuals(MMA_EXP.power_SMD2) %>% hist(main = paste("orignal power"), xlab = "Residual") 

#---------------------- (2) type S error -----------------------#
# log
MMA_EXP.power.S_SMD <- lmer( log(power.S_SMD + 0.025) ~ 1 + (1 | study_ID_SMD), data = individual_est_SMD) # add an offset of 0.025 to avoid log(0) = inf
# median 
summary(MMA_EXP.power.S_SMD)$coefficients[1] %>% exp() - 0.025 # - 0.025 is important
# this is mean
(summary(MMA_EXP.power.S_SMD)$coefficients[1] + 
                0.5*(summary(MMA_EXP.power.S_SMD)$varcor[[1]][[1]] + # sigma^2 for study level
                     summary(MMA_EXP.power.S_SMD)$sigma^2) # residual level - we cannot do like what we did above as we added 0.025
        ) %>% exp() - 0.025

# confidence interval of median
confint(MMA_EXP.power.S_SMD) %>% exp() - 0.025 # if the lower boundary was negative (probably caused by the negative variance), we used 0 to replace it

#--------------------- (3) type M error ---------------------#
# log
MMA_EXP.power.M_SMD <- lmer(log(power.M_SMD) ~ 1 + (1 | study_ID_SMD), data = individual_est_SMD)

# median 
summary(MMA_EXP.power.M_SMD)$coefficients[1] %>% exp()
# mean
(summary(MMA_EXP.power.M_SMD)$coefficients[1] + 0.5*var(log(individual_est_SMD$power.M_SMD))) %>% exp()
# confidence interval of median
confint(MMA_EXP.power.M_SMD) %>% exp()
```

